{"meta":{"title":"holdfire","subtitle":"学无止境，不忘初心！","description":"一只想上天的菜鸟！","author":"holdfire","url":"http://holdfire.github.io","root":"/"},"pages":[{"title":"关于作者","date":"2019-11-01T23:41:44.000Z","updated":"2019-11-02T01:53:41.861Z","comments":true,"path":"about/index.html","permalink":"http://holdfire.github.io/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-11-01T23:17:21.000Z","updated":"2019-11-01T23:46:29.026Z","comments":true,"path":"categories/index.html","permalink":"http://holdfire.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-01T23:17:00.000Z","updated":"2019-11-01T23:45:42.167Z","comments":true,"path":"tags/index.html","permalink":"http://holdfire.github.io/tags/index.html","excerpt":"","text":""},{"title":"schedule","date":"2019-11-02T01:47:41.000Z","updated":"2019-11-02T01:47:41.333Z","comments":true,"path":"schedule/index.html","permalink":"http://holdfire.github.io/schedule/index.html","excerpt":"","text":""}],"posts":[{"title":"文字识别技术简介","slug":"computer-vision-OCR","date":"2019-11-05T13:53:30.000Z","updated":"2019-11-05T14:00:57.379Z","comments":true,"path":"2019/11/05/computer-vision-OCR/","link":"","permalink":"http://holdfire.github.io/2019/11/05/computer-vision-OCR/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[]},{"title":"最近邻搜索NN——哈希散列方法","slug":"computer-vision-nn-local-sensitive-hash","date":"2019-11-05T09:36:20.000Z","updated":"2019-11-05T11:15:22.399Z","comments":true,"path":"2019/11/05/computer-vision-nn-local-sensitive-hash/","link":"","permalink":"http://holdfire.github.io/2019/11/05/computer-vision-nn-local-sensitive-hash/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"},{"name":"最近邻搜索","slug":"最近邻搜索","permalink":"http://holdfire.github.io/tags/%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2/"}]},{"title":"最近邻搜索NN——向量量化","slug":"computer-vision-nn-vector-quantization","date":"2019-11-05T09:20:06.000Z","updated":"2019-11-05T14:24:13.089Z","comments":true,"path":"2019/11/05/computer-vision-nn-vector-quantization/","link":"","permalink":"http://holdfire.github.io/2019/11/05/computer-vision-nn-vector-quantization/","excerpt":"","text":"1. 简介 向量量化是通过聚类把向量聚集成若干类，每类里面的向量用对应的类中心来近似。 这样每个向量只需要用其对应的聚类中心的索引ID来表示，其余查询向量间的距离用其对应的聚类中心与查询向量间的距离来近似。向量量化的优点为： （1）向量需要的存储空间变少了，只需保留对应的聚类中心的ID； （2）计算时间减少了，只需要通过聚类中心的索引ID来查询预先计算好的聚类中心与查询向量的距离表格； 然而，直接用k-means算法并不能带来明显的效果：如果聚类中心数目太少，向量近似效果不佳；而如果聚类中心数目太多，距离表格计算时间会太长。用Cartesian量化来解决此问题，Cartesian量化的聚类中心C是建立在几个小的聚类中心集合{C1,C1,...CP}\\{C_1,C_1,...C_P\\}{C1​,C1​,...CP​}的基础上：C=C1×C2×...×CPC = C_1 \\times C_2 \\times ...\\times C_PC=C1​×C2​×...×CP​。其好处在于通过几个小的聚类中心集合，可以得到非常多的聚类中心，甚至多于搜索的向量集合大小，从而向量近似效果可以得到保证。进一步通过引进一些限制或者策略来保证距离计算可以通过快速查找距离表格来实现，以降低计算时间。 2. 乘积量化PQ 乘积量化(Productive Quantization ,PQ)把向量分成若干个子向量然后对每个子向量分别进行聚类。具体来讲，向量xxx分成子向量{x1,x2,...xP}:x=[x1T x2T ... xPT]T\\{x_1,x_2,...x_P\\}:x = [x_1^T\\ x_2^T\\ ...\\ x_P^T]^T{x1​,x2​,...xP​}:x=[x1T​ x2T​ ... xPT​]T。然后对每个子向量的集合{x1,x2,...xP}\\{ x_1,x_2,...x_P\\}{x1​,x2​,...xP​}进行聚类，得到K（为了方便，每个集合都生成相同数量的聚类中心）个聚类中心：CP={cp1,cp2,...cpk}C_P=\\{ c_p1,c_p2,...c_pk\\}CP​={cp​1,cp​2,...cp​k}。最终，乘积量化得到KPK^PKP聚类中心{[c(1k1)T,c(2k2)T,...,c(PkP)T]T;k1∈{0,1,...,K},k2∈{0,1,...,K},kP∈{0,1,...,K}}\\{ [c_{(1k_1)}^T, c_{(2k_2)}^T, ..., c_{(Pk_P)}^T ]^T;k_1\\in\\{0,1,...,K\\},k_2\\in\\{ 0,1,...,K\\},k_P\\in\\{ 0,1,...,K\\} \\}{[c(1k1​)T​,c(2k2​)T​,...,c(PkP​)T​]T;k1​∈{0,1,...,K},k2​∈{0,1,...,K},kP​∈{0,1,...,K}}。在最近邻搜索中，乘积量化每个向量xxx由其PPP个子向量对应的聚类中心的ID来表达：(k1,k2,...,kP)(k_1, k_2,...,k_P)(k1​,k2​,...,kP​)。查询向量qqq与参考向量xxx距离有对称和非对称两种近似方法。 对称的方法： 线下计算P个大小K×KK \\times KK×K的距离表格，每个表格DpD_pDp​存储对应的聚类中心两两之间的距离Dp(i,j)=dist(cpi,cpj)D_p(i,j) = dist(c_pi,c_pj)Dp​(i,j)=dist(cp​i,cp​j)。查询向量分成PPP个子向量，每个子向量找到对应的聚类中心，查询向量由聚类中心的ID表示：((k1)′′,(k2)′′,...,(kP)′′)((k_1)&#x27;&#x27;,(k_2)&#x27;&#x27;,...,(k_P)&#x27;&#x27;)((k1​)′′,(k2​)′′,...,(kP​)′′)。这样子，距离近似为dist(p,x)≈∑p=1PDp((kp)′′,kp)dist(p,x) \\approx \\sum_{p=1}^P D_p((k_p)&#x27;&#x27;, k_p)dist(p,x)≈∑p=1P​Dp​((kp​)′′,kp​)。 非对称的方法： 线上预先计算一个大小为P×KP \\times KP×K的距离表格，把查询向量q分成P个子向量q1,q2,...,qPq_1, q_2,...,q_Pq1​,q2​,...,qP​，然后计算这P个子向量与P组聚类中心的距离，D(p,j)=dist(qp,cpj)D(p,j)=dist(q_p, c_pj)D(p,j)=dist(qp​,cp​j)。这样，距离近似为dist(p,x)≈∑p=1PD(p,kp)dist(p,x) \\approx \\sum_{p=1}^P D(p, k_p)dist(p,x)≈∑p=1P​D(p,kp​)。很显然，后者距离近似得更为准确，但是需要额外距离表格的计算，在搜索数据库非常大的情况下，额外距离表格计算的时间可以忽略不计。 乘积量化需要把向量分成PPP个子向量，由于各个子向量的分布不一样，每个子向量的量化性能不平衡，会导致距离近似不够理想。由Mhammad Norouzi等人提出的Cartesian k-means方法以及由Tiezheng Ge等人提出的优化的的乘积向量方法首先旋转向量，然后在旋转后的空间里进行乘积量化。这里面旋转的目的是旋转后的每个子向量的量化性能尽量平衡。旋转具有不改变欧氏距离的性质，这是可以把旋转引进来的原因。 3. 合成量化CQ 4. 加和量化AQ 参考资料： Hervé Jégou, Douze M , Schmid C . Product Quantization for Nearest Neighbor Search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 33(1):117-128. 微软研究院AI头条——最近邻搜索综述:https://blog.csdn.net/Y2c8YpZC15p/article/details/86326313 实例理解乘积向量算法：http://fabwrite.com/productquantization 理解product quantization算法 Product Quantizers for k-NN Tutorial Part 1 Optimized Product Quantization","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"},{"name":"最近邻搜索","slug":"最近邻搜索","permalink":"http://holdfire.github.io/tags/%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2/"}]},{"title":"最近邻搜索NN——综述","slug":"computer-vision-nn","date":"2019-11-05T07:39:19.000Z","updated":"2019-11-05T11:15:30.154Z","comments":true,"path":"2019/11/05/computer-vision-nn/","link":"","permalink":"http://holdfire.github.io/2019/11/05/computer-vision-nn/","excerpt":"","text":"1. 简介 最近邻搜索的数学描述为：给定d维欧式空间ℜD\\Re^DℜD中的一个查询向量qqq，从包含N个向量的数据库XXX中找到一个向量NNqNN_qNNq​,NNqNN_qNNq​和qqq的距离最小。 NN(q)=arg min⁡x∈Xdist(q,x)NN(q) = arg \\ \\min_{x \\in X} dist(q,x) NN(q)=arg x∈Xmin​dist(q,x) 通过线性查找，计算查询向量和数据库中各个向量的距离。但如果是海量的高维数据集，采用线性查找会非常耗时。 为了解决这个问题，我们需要采用一些类似索引的技术来加速查找的过程，这类方法包括：最近邻检索(Nearest Neighbor Search, NN)和近似最近邻检索(Approximate Nearest Neighbor Search, ANN)。主要有以下两种思路：一种是基于提升检索结构性能的方法，大多基于树形结构。另一种是基于对数据本身的处理，包括哈希算法，向量量化等方法。 近似最近邻搜索方法ANN通过对数据分析聚类的方法，对数据库中的数据进行分类或编码。对于目标数据，根据其数据特诊预测其所属的数据类别，返回类别中的部分或全部作为检索结果。主要有两类方案，第一类是缩短距离计算时间，例如将维度d由1000将到100，这类方法主要包括哈希散列和矢量量化。第二类方法是通过减少距离的计算次数来实现的。例如将计算次数由1,000,000次减少到1,000次。本文暂不关注。 1.1 基于提升检索结构性能的方法 随机k-d树算法：对数据的处理，减少距离计算的次数 优先搜索k-means树算法 层次聚类树算法 1.2 对数据的处理，缩短距离计算的时间——&gt;哈希散列 哈希散列是通过哈希函数把向量q变化为二值码，然后用Hamming距离来近似表示原来两个向量的距离。 核心思想是：在高维空间相邻的数据，经过哈希函数的映射投影转化到低维空间后，它们落入同一个吊桶的概率很大，而不相邻的数据映射到同一个吊桶的概率很小。在检索时将欧式空间的距离计算转化到汉明（Hamming）空间，并将全局检索转化为对映射到同一个吊桶中的数据进行检索，从而提高了检索速度。这种方法的主要难点在于如何寻找适合的哈希函数。 1.3 对数据的处理，缩短距离计算的时间——&gt;向量量化 向量量化是通过聚类把向量集聚成若干类，每类里面的向量用对应的类中心来近似。这样子，每个向量只需要用其对应的聚类中心的索引ID来表示，其与查询向量间的距离用其对应的聚类中心与查询向量间的距离来近似。向量量化带来了两项优势：向量需要的存储空间变少了，只需保存对应的聚类中心的ID；计算时间减少了，只需要通过聚类中心的索引ID来查询预先计算好的聚类中心与查询向量的距离表格。 2. 基于提升检索结构性能的方法 1977年，Friedman et al.提出了k-d树，这种结构后来被用于加速精确查找。 random multiple k-d trees和priority search 相比较普通k-d树，提升了搜索的准确率和搜索效率。相关文章：Silpa-Anan &amp; Hartley. Optimised kd-trees for fast image descriptor matching. In CVPR, 2008. FLANN方法 该方法在random k-d trees和hierarchial k-means trees之间进行很好地配置 。相关文章：Muja &amp; Lowe. Fast approximate nearest neighbors with automatic algorithm configuration. In VISS-APP(1),.pp.331-340,2009. over tree结构 相关文章：Beygelzimer et al., Cover trees for nearest neighbor. In ICML, pp.97-104,2006. trinary tree结构 相关文章:Jia et al., 2010. Optimised kd-trees for scable visual descriptor indexing. In CVPR,PP 3392-3399. Wang et al.,2014. Trinary-projection trees for approximate nearest neighbor search. IEEE Trans.Pattern Ananl.Mach.Intell. 基于近邻图的最近邻搜索算法 相关文献：Arya &amp; Mount, 1993. Approximate nearest neighbor queries in fixed dimension. In SODA. Wang &amp; Li, 2012. Query-driven iterated neighborhood graph search for large scale indexing. Wang et al., 2013. Fast neighborhood graph search using cartesian concatenation. In ICCV. 3. 基于哈希散列的方法 这类方法将数据库中的向量转换为更短的编码，从而占用的存储空间更小，距离计算的时间也更短。 局部敏感哈希方法的发展 1999年，Gionis提出局部敏感哈希方法LSH 相关文章：Gionis. 1999. Similarity search in high dimensions via hashing. LSH的基础上Mahalanobis distance， 相关文章：Jain et al. 2008. Fast image search for learned metrics. In CVPR. LSH基础上kernalization， 相关文章：Kulis &amp; Grauman. 2009. Kernalized locality-sensitive hashing for scalable image search. In NIPS. comlementary hashing， 相关文章：Xu et al. 2011. Comlementary hashing for approximate nearest neighbor search. In ICCV 设计哈希函数 语义哈希(semantic hashing) 相关文章：Salakhutdinov &amp; Hinton. 2009. Semantic hashing. Int.J.Approx.Reasoning. shift kernel hashing， 相关文章：Raginsky &amp; Lazebnik. 2009. Local sensitive binary codes from shift-invariant kernels. In NIPS. isotropic hashing， 相关文章：[Kong &amp; Li. 2012.Isotropic hashing. In NIPS. 设计保相似度的哈希函数 谱哈希(Spectral hashing) 这种方法的出发点是希望Hamming距离大的两个数据点在原空间的相似度要小，其目标函数为最小化Hamming距离和原空间相似度的乘积，最后转化为解特征值或特征函数问题。相关文章：Weiss et al. 2008. Spectral hashing. In NIPS. 二值化重建嵌入(binary reconstructive embedding) 其目标函数是最小化距离重建误差，即希望Hamming距离和原空间里的欧氏距离尽量接近。相关文章：Kulis &amp; Darrells. 2009. Learning to hash with binary reconstructive embeddings. In NIPS. 基于图的哈希(graph-based hashing) 锚点图哈希(Anchor Graph Hashing, AGH)。相关文章：Liu Wei et al. 2012.Hashing with graphs. In ICML. 半监督哈希(Semi-Supervised Hashing) 相关文章：Wang Jun et al., 2012. Semi-supervised hashing for large scale search. 设计保序的目标函数 三元组损失函数是一种最简单的保序函数 设计保序的目标函数，使二值空间的序跟原空间的序尽量一致。将搜索问题看成排序问题，找到距离查询点近的向量。如果一个点q与一个点p1p_1p1​的距离比q到点q2q_2q2​的距离小，那么在二值空间里，点q与点p1p_1p1​的Hamming距离比q到点q2q_2q2​的Hamming距离小也要。相关文章：Mohanmmad Norouzi, Fleet. 2011. Minimal loss hashing for compact binary codes.In ICML. 更高阶的保序目标函数 相关文章：Wang et al.,2013. Order preserving hashing for approximate nearest neighbor search.In ACM Multimedia. 迭代量化ITQ 迭代量化(Iterative Quntization, ITQ)的方法 其出发点不同于保相似度、保距离或者保序，而是把二值编码当成原向量的近似，利用欧氏距离旋转不变性的性质，建立了最小化二值编码重建旋转原向量误差的目标函数，寻找最优的旋转变换和二值编码。尽管直观看上去重建向量的方法比保相似、保距离或者保序的方法简单，近似得更强，但ITQ实际上效果还是很不错的,原因是保相似、保距离或者保序需要建立二元或多元关系，计算复杂度很大，因而需要各种近似，使得最后的效果不如预期。相关文章：Gong &amp; Lazebnik. Iterative quantization: A procrustean approach to learning bianry codes. In CVPR,PP.817-824,2011. 4. 向量量化的方法 乘积量化(Productive Quantization, PQ) 乘积量化是信号处理上用到的一种数据压缩技术。相关文章：Hervé Jégou, Douze M , Schmid C . Product Quantization for Nearest Neighbor Search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 33(1):117-128. 合成量化(Compositive Quantization,CQ)的方法 相关文章：Zhang Ting, Du Chao, Wang Jingdong.Composite quantization for approximative nearest neighbor search.ICML 2014: 838-846. 以及：Wang Jingdong, Zhang Ting. Composition Quantization. IEEE Transactions on Pattern Analysis and Machine Intelligence.2018 加和量化(Additive Quantization, AQ) 参考资料： 微软研究院AI头条——最近邻搜索综述:https://blog.csdn.net/Y2c8YpZC15p/article/details/86326313","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"},{"name":"最近邻搜索","slug":"最近邻搜索","permalink":"http://holdfire.github.io/tags/%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2/"}]},{"title":"学习方法——如何阅读论文","slug":"personal-ideas-reading-papers","date":"2019-11-03T19:17:54.000Z","updated":"2019-11-03T19:46:39.419Z","comments":true,"path":"2019/11/04/personal-ideas-reading-papers/","link":"","permalink":"http://holdfire.github.io/2019/11/04/personal-ideas-reading-papers/","excerpt":"","text":"1.论文的阅读顺序 第一阶段：判断论文是否值得读 阅读顺序为：读标题和关键词–&gt;读摘要–&gt;读结论 掌握论文的大概内容。 第二阶段：读论文 阅读顺序为：读图标和数据–&gt;读引言introduction 如果想要深挖细节：读结果和讨论–&gt;读实验部分 第三阶段：做笔记 好笔记的标准：下次看笔记内容即可，无需再看这篇论文 2.阅读时应该做的思考和笔记 一篇论文的核心往往就一两句话，而论文的其他部分都是在用不同的方式支持它的核心。 摘要Abstract 摘要是对全篇内容的概括，阅读后需要做的笔记： 作者想要解决什么问题？question 作者通过什么方法/模型解决了这个问题？method 作者给出的答案是什么？result 引言introduction 引言介绍了文章的研究背景和研究意义，一般在最后会给出文章的结构。需要做的笔记： 作者为什么要研究这个问题？ 这个问题现在的研究进展到了哪个地步？ 作者使用的理论是基于哪些假设？ 结论conclusion 结论部分会把整篇文章的主要内容复述一遍，帮助读者回顾+理清思路，然后在此基础上加深自己的研究。需要做的笔记： 这篇文章存在哪些缺陷？ 作者关于这个课题的构思有哪几点？ 参考资料： 明尼苏达大学Peter W. Carr教授传授的阅读顺序：https://video.zhihu.com/video/1172537839240863744 知乎大佬的回答：https://www.zhihu.com/question/345516318/answer/863530375","categories":[{"name":"个人杂记","slug":"个人杂记","permalink":"http://holdfire.github.io/categories/%E4%B8%AA%E4%BA%BA%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"学习方法","slug":"学习方法","permalink":"http://holdfire.github.io/tags/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]},{"title":"机器学习——正则化","slug":"machine-leanrning-regularization","date":"2019-11-03T18:41:48.000Z","updated":"2019-11-05T13:55:41.140Z","comments":true,"path":"2019/11/04/machine-leanrning-regularization/","link":"","permalink":"http://holdfire.github.io/2019/11/04/machine-leanrning-regularization/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"机器学习——标准化和归一化","slug":"machine-leanrning-standardization-normalization","date":"2019-11-03T18:28:16.000Z","updated":"2019-11-03T18:42:38.818Z","comments":true,"path":"2019/11/04/machine-leanrning-standardization-normalization/","link":"","permalink":"http://holdfire.github.io/2019/11/04/machine-leanrning-standardization-normalization/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——GBDT","slug":"machine-leanrning-GBDT","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-03T18:17:57.062Z","comments":true,"path":"2019/11/04/machine-leanrning-GBDT/","link":"","permalink":"http://holdfire.github.io/2019/11/04/machine-leanrning-GBDT/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——XgBoost","slug":"machine-leanrning-XgBoost","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-03T18:18:16.599Z","comments":true,"path":"2019/11/04/machine-leanrning-XgBoost/","link":"","permalink":"http://holdfire.github.io/2019/11/04/machine-leanrning-XgBoost/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——AdaBoost","slug":"machine-leanrning-adaBoost","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-03T18:21:41.336Z","comments":true,"path":"2019/11/04/machine-leanrning-adaBoost/","link":"","permalink":"http://holdfire.github.io/2019/11/04/machine-leanrning-adaBoost/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——简介","slug":"machine-leanrning-ensemble-learning","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-03T18:21:02.547Z","comments":true,"path":"2019/11/04/machine-leanrning-ensemble-learning/","link":"","permalink":"http://holdfire.github.io/2019/11/04/machine-leanrning-ensemble-learning/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算机视觉——介绍","slug":"computer-vision-introduction","date":"2019-11-03T13:40:07.000Z","updated":"2019-11-05T13:51:35.852Z","comments":true,"path":"2019/11/03/computer-vision-introduction/","link":"","permalink":"http://holdfire.github.io/2019/11/03/computer-vision-introduction/","excerpt":"","text":"1. 简介 计算机视觉包含图像处理、图像分析和图像理解三个部分。 2. 计算机视觉基础任务 2.1 图像处理image process 图像处理是对图像进行去噪、增强、复原、分割、特征提取、识别、等操作的理论、方法和技术。 2.2 图像分析image analysis 2.3 图像理解image understanding 图像理解可以认为是一种动态的目标检测，由全局信息生成图像摘要。 图像分类(image classification) 目标检测(object detection) 图像语义分割(image semantic segmentation) 图像描述(image caption) 图像生成(image generation):如变分自编码器VAE，生成对抗网络GAN 视频分类(video classification) 度量学习(metric learning) 3. 计算机视觉方向 3.1 图像相关 图像检索：基于内容的图像检索CBIR，实例图像检索(instance image retrieval) 人脸识别：人脸活体检测算法，人脸表情识别，人脸颜值打分 行人属性识别：Re ID 人体骨骼关键点检测：人体姿态估计(Pose Estimation)，行为识别(open pose开源框架) 3.2 视频相关 视频目标跟踪：单目标跟踪，多目标跟踪，行人再追踪 视频对象提取： 3.3 文字、语言相关 文字检测与识别：文档文字检测与识别，场景文字检测与识别，手写体文本检测与识别 基于深度学习的表格提取： 唇语识别： 3.4 专门领域 自动驾驶：slam技术 车辆结构化：车牌识别，车型识别 医学图像分析: 3.5 三维视觉技术 基于RGB-D的三维深度学习：三维目标分类，三维目标检测，三维语义分割 立体匹配技术：阵列相机立体全景拼接 三维重建：从全景图恢复三维结构，深度相机室内实时稠密三维重建，基于单目视觉的三维重建 构造深度图：单目微运动生成深度图，深度图补全 深度学习自动构图：","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}]},{"title":"目标检测——基于深度学习的目标检测综述（1）","slug":"computer-vision-object-detection","date":"2019-11-03T13:27:36.000Z","updated":"2019-11-03T13:40:24.610Z","comments":true,"path":"2019/11/03/computer-vision-object-detection/","link":"","permalink":"http://holdfire.github.io/2019/11/03/computer-vision-object-detection/","excerpt":"","text":"目标检测介绍","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://holdfire.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}]},{"title":"非监督学习——高斯混合模型GMM","slug":"machine-learning-GMM","date":"2019-11-03T10:22:42.000Z","updated":"2019-11-03T12:55:52.873Z","comments":true,"path":"2019/11/03/machine-learning-GMM/","link":"","permalink":"http://holdfire.github.io/2019/11/03/machine-learning-GMM/","excerpt":"","text":"高斯混合模型的数学形式： 高斯混合模型(Gaussian Mixture Model, GMM)是指具有如下形式概率分布的模型： P(y∣θ)=∑k=1Kαkϕ(y∣θk)(1)\\tag{1}P(y|\\theta) = \\sum_{k=1}^K \\alpha_{k} \\phi(y|\\theta_{k}) P(y∣θ)=k=1∑K​αk​ϕ(y∣θk​)(1) 其中，αk\\alpha_{k}αk​是系数，αk≥0\\alpha_{k}\\geq0αk​≥0，∑k=1Kαk=1\\sum_{k=1}^K \\alpha_{k}=1∑k=1K​αk​=1。θk=(μk,δk2)\\theta_{k} = (\\mu_{k}, \\delta_{k}^2)θk​=(μk​,δk2​)，ϕ(y∣θk)\\phi(y|\\theta_{k})ϕ(y∣θk​)是高斯分布密度，称为高斯混合模型的第k个分模型，模型形式为： ϕ(y∣θk)=12πδkexp(−(y−μk)22δk2)(2)\\tag{2} \\phi(y|\\theta_{k}) = \\frac{1}{\\sqrt {2\\pi} \\delta_{k}} exp(-\\frac {(y-\\mu_{k})^2}{2\\delta_{k}^2}) ϕ(y∣θk​)=2π​δk​1​exp(−2δk2​(y−μk​)2​)(2) 如果公式（1）中的分模型不是高斯分布密度函数，而是任意概率密度分布，式（1）成为一般混合模型。 高斯混合模型参数估计的EM算法 假设观测数据y1,y2,...yNy_1,y_2,...y_Ny1​,y2​,...yN​由高斯混合模型(公式1)生成，其中θ=(α1,α2,...,αK;θ1,θ2,...,θK)\\theta = (\\alpha_1,\\alpha_2,...,\\alpha_K;\\theta_1,\\theta_2,...,\\theta_K)θ=(α1​,α2​,...,αK​;θ1​,θ2​,...,θK​)，利用EM算法估计高斯混合模型参数θ\\thetaθ的步骤为： 明确隐变量，写出完全数据的对述似然函数 可以设想观测数据yj,j=1,2,...,Ny_j, j=1,2,...,Nyj​,j=1,2,...,N是这样产生的：首先依概率αk\\alpha_kαk​选择第k个高斯分布分模型，然后依第k个分模型的概率分布ϕ(y∣θk)\\phi(y|\\theta_{k})ϕ(y∣θk​)生成观测数据yjy_jyj​，这时观测数据yj,j=1,2,...,Ny_j, j=1,2,...,Nyj​,j=1,2,...,N是已知的；反映观测数据yjy_jyj​来自第k个分模型的数据来自第k个分模型的数据是未知的，k=1,2,...,Kk=1,2,...,Kk=1,2,...,K，以隐变量γjk\\gamma_{jk}γjk​表示，其定义如下： γjk={1第j个观测来自第k个分模型的数据0elsej=1,2,...,N；k=1,2,..,K(3)\\tag{3} \\gamma_{jk} = \\begin{cases} 1 &amp; 第j个观测来自第k个分模型的数据 \\\\ 0 &amp; else \\end{cases} \\newline j = 1,2,...,N；k = 1,2,..,K γjk​={10​第j个观测来自第k个分模型的数据else​j=1,2,...,N；k=1,2,..,K(3) 其中，γjk\\gamma_{jk}γjk​是0-1随机变量。 完全数据包含：观测数据yjy_jyj​和未观测数据γjk\\gamma_{jk}γjk​，即： (yj;γj1,γj2,...γjk),j=1,2,...,N(y_j; \\gamma_{j1},\\gamma_{j2},...\\gamma_{jk}), j=1,2,...,N (yj​;γj1​,γj2​,...γjk​),j=1,2,...,N 因而，完全数据的似然函数为： P(y,γ∣θ)=P(y,\\gamma | \\theta) = P(y,γ∣θ)= 上述算法流程 输入：观测数据y1,y2,...yNy_1,y_2,...y_Ny1​,y2​,...yN​，高斯混合模型； 输出：高斯混合模型的参数。 （1）取参数的初始值开始迭代 高斯混合模型的应用","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"非监督学习","slug":"非监督学习","permalink":"http://holdfire.github.io/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"非监督学习——EM算法","slug":"machine-learning-EM-algorithm","date":"2019-11-03T10:00:40.000Z","updated":"2019-11-03T12:55:44.982Z","comments":true,"path":"2019/11/03/machine-learning-EM-algorithm/","link":"","permalink":"http://holdfire.github.io/2019/11/03/machine-learning-EM-algorithm/","excerpt":"","text":"硬币问题中的EM算法 三硬币模型 EM算法的推导 EM算法的收敛性 EM算法的应用——非监督学习 EM算法的应用——高斯混合模型 EM算法的推广 参考资料： 从投硬币问题理解EM算法：https://www.jianshu.com/p/1121509ac1dc 李航《统计学习方法》P155：EM算法及其推广","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"非监督学习","slug":"非监督学习","permalink":"http://holdfire.github.io/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"图像处理——上采样与下采样","slug":"computer-vision-upsampling-downsampling","date":"2019-11-01T22:14:41.000Z","updated":"2019-11-03T12:52:36.013Z","comments":true,"path":"2019/11/02/computer-vision-upsampling-downsampling/","link":"","permalink":"http://holdfire.github.io/2019/11/02/computer-vision-upsampling-downsampling/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"图像处理","slug":"图像处理","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"神经网络——参数初始化","slug":"deep-learning-parameter-initialization","date":"2019-11-01T21:57:50.000Z","updated":"2019-11-03T18:19:00.490Z","comments":true,"path":"2019/11/02/deep-learning-parameter-initialization/","link":"","permalink":"http://holdfire.github.io/2019/11/02/deep-learning-parameter-initialization/","excerpt":"","text":"1.如何分析参数初始化结果的好坏？ 查看初始化后各层的激活值分布，如果在某个固定区间内分布则较好，如果集中在某个值上则初始化不好。 2. 把w初始化为0 对于单层网络可行；对于多层网络，由于链式法则会导致梯度消失。 3. 随机初始化 使用均值为0，方差为0.02的正态分布去初始化。w初始值较小是因为：如果x很大的话，w又相对较大，会导致Z非常大，这样如果激活函数是sigmoid，就会导致sigmoid的输出值1或者0。 4. Xavier initialization Xavier initialization方法是Glorot等人为了解决随机初始化的问题提出来的另一种初始化方法。他们的思想倒也简单，就是尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。Xavier initialization能够很好的tanh激活函数。 5. He initialization 参考资料： 斯坦福大学——李飞飞计算机视觉课程cs231n：","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"神经网络——批归一化Batch Normalization","slug":"deep-learning-batch-normalization","date":"2019-11-01T21:13:56.000Z","updated":"2019-11-04T18:18:26.062Z","comments":true,"path":"2019/11/02/deep-learning-batch-normalization/","link":"","permalink":"http://holdfire.github.io/2019/11/02/deep-learning-batch-normalization/","excerpt":"","text":"1. 简介 什么是批归一化？ 批归一化(Batch Normalization)是在训练模型时进行的。模型训练时，每次输入的数据为[N,H,W,C][N, H, W, C][N,H,W,C]时，在BatchSize这个维度进行归一化处理。而在测试时，批归一化采用的参数为训练集的均值和方差， 批归一化： 批归一化有什么好处？ 参考文献： 2015年谷歌论文：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift CSDN论文解读博客：https://blog.csdn.net/hjimce/article/details/50866313","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://holdfire.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"论文阅读","slug":"论文阅读","permalink":"http://holdfire.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"}]},{"title":"目标检测——交并比IOU,非极大值抑制NMS","slug":"computer-vision-IOU-NMS","date":"2019-11-01T21:07:41.000Z","updated":"2019-11-03T12:52:18.026Z","comments":true,"path":"2019/11/02/computer-vision-IOU-NMS/","link":"","permalink":"http://holdfire.github.io/2019/11/02/computer-vision-IOU-NMS/","excerpt":"","text":"1. 交并比IoU 在目标检测任务中，通常使用矩形框来定位检测对象的边界。假设你的算法给出的边界框为A，实际的边界框为B，那么交并比(Intersection over Union)就可以衡量检测结果的好坏，其计算公式为： IoU=A∩BA∪BIoU = \\frac {A \\cap B} {A \\cup B} IoU=A∪BA∩B​ 通常约定IoU大于0.5，就认为目标检测正确 2. 非极大值抑制NMS 在目标检测任务中，你的算法可能对同一个目标做了多次检测。非极大值抑制(Non-Maximum Suppression, NMS)就是为了确保你的算法对一个目标只检测一次。 3. Anchor Box 参考资料： 吴恩达深度学习课程： 斯坦福大学，李飞飞计算机视觉课程cs231n： 慕课网，深度学习之目标检测课程：","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://holdfire.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"神经网络——二阶优化算法","slug":"deep-learning-optimization-algorithm-2","date":"2019-10-30T18:54:23.000Z","updated":"2019-11-03T18:19:17.737Z","comments":true,"path":"2019/10/31/deep-learning-optimization-algorithm-2/","link":"","permalink":"http://holdfire.github.io/2019/10/31/deep-learning-optimization-algorithm-2/","excerpt":"","text":"1. 简介 1.1 牛顿法 1.2 高斯-牛顿法 1.3 拟牛顿法 1.4 发展历程 2. 不常用的二阶优化算法 2.1 DFP算法 第一个拟牛顿算法是由Argonne国家实验室的物理学家William C.Davidon提出的。 他在1959年开发了第一个拟牛顿算法:DFP（Davidon–Fletcher–Powell formula）更新公式，后来由Fletcher和Powell在1963年推广，但现在很少使用 3. 常用的二阶优化算法 3.1 SR1公式（用于“对称秩一”） 3.2 BHHH方法 3.3 BFGS方法 BFGS算法的全称为Broyden–Fletcher–Goldfarb–Shanno algorithm 3.4 Limited-memory BFGS方法 低内存拓展算法 参考资料： 机器之心——二阶优化算法介绍：https://www.jiqizhixin.com/graph/technologies/75950ad0-edbd-4208-9347-b8c17b8e058c","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"凸优化","slug":"凸优化","permalink":"http://holdfire.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"}]},{"title":"机器学习——损失函数","slug":"deep-learning-loss-function","date":"2019-10-30T18:29:07.000Z","updated":"2019-11-03T12:59:41.128Z","comments":true,"path":"2019/10/31/deep-learning-loss-function/","link":"","permalink":"http://holdfire.github.io/2019/10/31/deep-learning-loss-function/","excerpt":"","text":"1. 简介 1.1 什么是损失函数？ 2.回归问题中的损失函数 3. 分类问题中的损失函数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"神经网络——一阶优化算法","slug":"deep-learning-optimization-algorithm-1","date":"2019-10-30T18:28:46.000Z","updated":"2019-11-03T18:19:26.003Z","comments":true,"path":"2019/10/31/deep-learning-optimization-algorithm-1/","link":"","permalink":"http://holdfire.github.io/2019/10/31/deep-learning-optimization-algorithm-1/","excerpt":"","text":"1. 简介 1.1 什么是优化算法？ 1.2 为什么需要优化算法？ 1.3 优化算法分类 2. 一阶优化算法 2.1 随机梯度下降法 随机梯度下降法(Stotastic Gradient Descend method, SGD)是 2.2 批量梯度下降法 批量梯度下降法(Batch Gradient Descend) 2.3 小批量随机梯度下降法 小批量梯度下降法(mini-Batch Gradient Descend) 2.4 动量梯度下降法 动量梯度下降法(Momentum Gradient Descend) 2.5 Nesterov梯度下降方法 Nesterov梯度下降方法(Nesterov Accelerated Gradient)是对传统momentum方法的一项改进，由Ilya Sutskever(2012 unpublished)在Nesterov工作的启发下提出的。 其基本思路如下图： 2.6 Adagrade 2.7 Adadelta 2.8 RMSprop 2.9 Adam 3 选择技巧 参考资料： 各种优化方法总结比较：https://www.cnblogs.com/qniguoym/p/8058186.html","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"凸优化","slug":"凸优化","permalink":"http://holdfire.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"}]},{"title":"神经网络——激活函数","slug":"deep-learning-activation-function","date":"2019-10-30T18:24:41.000Z","updated":"2019-11-05T14:17:38.472Z","comments":true,"path":"2019/10/31/deep-learning-activation-function/","link":"","permalink":"http://holdfire.github.io/2019/10/31/deep-learning-activation-function/","excerpt":"","text":"简介 在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。激活函数有什么作用呢？ 神经网络中如果不使用激活函数（相当于激活函数是f(x)=xf(x)=xf(x)=x)，那么每一层节点的输出都会是输入的线性函数。其结果是：无论神经网络有多少层，网络的输出层都是输入层的线性组合，神经网络仅相当于一个感知机，网络的拟合能力变得非常有限。 神经网络中加入激活函数后，网络对现实模型的表达能力得到增强，网络几乎能够逼近任意的函数模型。常用的的激活函数主要有：sigmoid函数，relu函数，tanh函数，leaky_relu函数，maxout函数等。 常用的激活函数 sigmoid函数 sigmoid函数是应用最为广泛的激活函数之一,函数形式如下所示。能够把任意实数映射到(0,1)(0,1)(0,1)区间上的实数。当自变量值小于-5时，函数值接近于0；当自变量大于5时，函数值非常接近于1。sigmoid函数主要的优点是求导方便，其导数f′(x)=f(x)⋅[1−f(x)]f^{&#x27;}(x)=f(x)\\cdot[1-f(x)]f′(x)=f(x)⋅[1−f(x)]。当x=0时函数的导数最大，为0.25。 σ(x)=11+e−x(1)\\tag{1} \\sigma(x) = \\frac {1}{1+e^{-x}} σ(x)=1+e−x1​(1) 缺点1：梯度消失和梯度爆炸 如果将神经网络的权值初始化为均值为0，标准差为1，由于sigmoid函数的导数在两端非常接近于0，靠近输入层的权重的更新值，进过一系列累乘后会变得特别小，权重参数得不到更新，这种现象叫做梯度消失。如果将初始权重设得特别大，如1000，同理可能会造成梯度爆炸，即权重参数每次的更新值太大。所以，为了同时解决梯度消失和梯度爆炸两个问题，我们需要设置合理的权重初始值，但这个很难做到，详细解释见附录。 缺点2：输出不是0均值 当上一层的输出结果不是0均值(zero-centered)时，即下一层的输入信号不是0均值，会产生这样的一个结果：对于函数f(x)=wTx+bf(x)=w^{T}x+bf(x)=wTx+b，如果输入x&gt;0x&gt;0x&gt;0,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，权重参数的收敛变得缓慢，称为zig-zag现象。 如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。 缺点3：求幂运算消耗时间较长 计算机求exe^{x}ex会消耗较长的时间。 tanh函数 tanh函数的主要优点是:解决了sigmoid函数的非零均值问题，但梯度消失的问题仍然没有解决。其函数形式为： tanh(x)=ex−e−xex+e−x(2)\\tag{2} tanh(x)=\\frac{e^{x}-e^{-x}} {e^{x}+e^{-x}} tanh(x)=ex+e−xex−e−x​(2) 我们知道Relu函数在卷积神经网络中取得了很好的结果，那为什么在RNN中还是使用tanh作为激活函数呢？ ReLU函数 ReLU函数(Rectified Linear Unit)的主要优点是：一定程度解决了梯度消失的问题，计算速度快，收敛速度远快于sigmoid函数和tanh函数。函数形式为： relu(x)=max(0,x)(3)\\tag{3} relu(x) = max(0,x) relu(x)=max(0,x)(3) ReLU函数的主要缺点是： 输出非零均值；可能会造成Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: 一是参数初始化不当，负的权值太多，这种情况比较少见。二是学习率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 Leaky ReLU函数 为了解决Dead ReLU Problem，Leaky ReLU函数将ReLU的前半段设为αx\\alpha xαx而非0，通常α=0.01\\alpha = 0.01α=0.01。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。其函数形式为： leaky relu(x)=max(αx,x)(4)\\tag{4} leaky\\ relu(x) = max(\\alpha x, x) leaky relu(x)=max(αx,x)(4) Maxout函数 Maxout函数可以看做是relu函数和leaky relu函数的一般化归纳。通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的界，然后每一段取最大。maxout可以看成是ReLU家族的一个推广,不会产生饱和，也不会产生dead ReLU。缺点在于参数量翻倍了。 f(x)=max(w1Tx+b1,w2Tx+b2)(5)\\tag{5} f(x) = max(w_1^Tx+b_1,w_2^Tx+b_2) f(x)=max(w1T​x+b1​,w2T​x+b2​)(5) Softmax函数 Softmax函数作为激活函数，通常用在多分类神经网络的输出层上，目的是让大的值更大。其函数形式为： δ(zj)=ezj∑i=1kezi(6)\\tag{6} \\delta (z_j) = \\frac {e^{z_j}} {\\sum_{i=1}^{k}e^{z_i}} δ(zj​)=∑i=1k​ezi​ezj​​(6) 激活函数的选择 (1)深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。 (2）如果使用 ReLU，那么一定要小心设置 learning rate， 而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout. (3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout，其计算量太大导致收敛很慢。 (4)为了防止饱和，现在主流的做法会在激活函数前进行batch normalization操作，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。 参考资料： 书籍Neural Networks and Deep Learning第5章 Hinton关于Relu函数在RNN中不奏效的论文：A Simple Way to Initialize Recurrent Networks of Rectified Linear Units Glorot和Bengio关于sigmoid函数在深层网络中产生饱和的论文：Understanding the difficulty of training deep feedforward neural networks 知乎专栏——RNN 中为什么要采用 tanh，而不是 ReLU 作为激活函数 ：https://www.zhihu.com/question/61265076/answer/186347780 附录:sigmoid函数梯度消失和梯度爆炸试验 下文来自参考资料第1条： 现象： 在深层网络中，不同隐含层的学习速度(各隐含层权值和偏差偏导组成的向量的范数)相差很大。靠后的隐含层学习速度较大，而靠前的隐含层经常在训练期间卡住，几乎什么都学习不到；也有时候早期的层可能学习很好，但后来的层卡住。 试验： 使用MNIST数据集进行图像分类任务，输入层神经元个数为784，隐含层神经元个数均为30，输出层神经元个数为10；使用sigmoid函数作为激活函数。训练图片为1000张，使用Batch Stostic Gradient Descend算法，500个epoch。使用不同数目的隐含层的分类结果如下： 隐含层数目 1 2 3 4 分类准确率 96.48% 96.90% 96.57% 96.53% 带有4个隐含层的网络，训练过程中各隐含层权重的学习速度变化如下图所示。从图中可以看出：在训练后期，第一个隐含层的学习速率比第四个隐含层慢了约100倍。 分析： 直观上来说，额外的隐含层应该使网络能够学习更为复杂的分类功能，从而进行更好的分类。即使额外的隐含层什么都不做，模型的准确率也不会变得更差。由于权重是随机初始化的，因而第一个隐含层会丢失大量的图片有用信息，所以此时第一层几乎不可能不需要再学习，即还没有收敛到最优值。若假设额外的隐含层确实有用，那么问题应该是我们的学习算法没有找到合适的权重和偏差。 梯度消失的数学推导： 考虑一个每层只有1个神经元的深层神经网络，w1,w2,...w_1,w_2,...w1​,w2​,...为权重，b1,b2,...b_1,b_2,...b1​,b2​,...为偏置。aja_jaj​为第j层的输出值，代价函数C对第一个隐含层的偏置b1b_1b1​的偏导为： 将神经网络的权值初始化为均值为0，标准差为1。则大部分权值满足∣wj∣&lt;1\\left|w_j\\right|&lt;1∣wj​∣&lt;1，且有∣wjσ′(zj)&lt;14∣\\left|w_j \\sigma^{\\prime}(z_j)&lt;\\frac {1} {4}\\right|∣∣​wj​σ′(zj​)&lt;41​∣∣​。由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为(1,+∞)(1,+\\infty)(1,+∞)区间内的值，则会出现梯度爆炸情况。 梯度爆炸的产生： 当权重设置很大时，如w1=w2=w3=w4=1000w_1=w_2=w_3=w_4=1000w1​=w2​=w3​=w4​=1000,而z的值接近0时，会产生梯度爆炸。 思考： 为了避免梯度消失，我们可以通过设置合适wjw_jwj​的初始值，使得∣wjσ′(zj)∣&gt;1\\left|w_j \\sigma^{\\prime}(z_j) \\right|&gt;1∣wj​σ′(zj​)∣&gt;1。事实上，这非常困难，因为σ′(zj)=σ′(wa+b)\\sigma^{\\prime}(z_j)=\\sigma^{\\prime}(wa+b)σ′(zj​)=σ′(wa+b)的值是依赖于w的，当w的值较大时，σ′(zj)\\sigma^{\\prime}(z_j)σ′(zj​)通常会很小。梯度消失很难避免。 总结： 深层神经网络的反向传播过程会发生梯度变化不稳定的状况。通过使用其他的激活函数，我们能否避免这种梯度下降不稳定的现象呢？","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Python——导入操作import","slug":"python-package-module-import","date":"2019-10-30T11:19:53.000Z","updated":"2019-10-30T17:34:35.542Z","comments":true,"path":"2019/10/30/python-package-module-import/","link":"","permalink":"http://holdfire.github.io/2019/10/30/python-package-module-import/","excerpt":"","text":"1. 简介 1.1 导入的内容 Python中导入操作的关键字为import，导入的内容主要是：包(package)、模块(module)、类(class)、函数(类中称作方法, method)、变量(类中称作属性, attributes)等。 在文件夹下面加入__init__.py文件(以下简称init文件)，这个文件夹就成为了一个包。包分为常规包和命名空间包。 模块是包下面的python文件。所有的包都可以被看做模块，但不是所有的模块都是包。 类是文件的基本单元（类中包括变量和函数）。 导入操作中，导入的具体内容主要由init文件、__all__列表(以下简称all列表)控制。 1.2 init文件的作用 如果包中含有init文件，import 包名就会执行init文件，能够批量导入init文件中导入的模块； 当这个包或者该包下的模块被导入时，init文件都会被执行； 1.3 all列表的作用 在init文件或模块中设置all列表时，from 包名/模块名 import *只会导入all列表中的内容； all列表的内容可以是本文件自定义的变量或函数，或者是导入进来的模块； 从模块中all列表导入的内容称为内置变量，内置变量易覆盖本文件中的变量名，因而不建议使用from 模块名 import * 2. import的使用方法 假设你正在一个名为current.py的文件（下面称为当前文件）中写代码，你想导入一些包或模块，那么不同的导入操作会有什么样的结果呢？ 注意：下面说的执行init文件 == init文件中所有的导入操作都会被执行，但这并不意味导入的内容能被当前文件所调用。（这句话主要是针对下面代码块中从包中导入模块这一情况） 1234567891011121314151617181920212223//(1) 从包名导入：包里必须有init文件，且init文件会被执行import 包名 // 当前文件能调用init文件所导入的全部内容。调用方式为包名.模块名from 包名 import * // 若init文件有all列表，则当前文件只能调用all列表的内容；若无则能调用init文件所导入的全部内容。调用方式为模块名//(2) 从包中导入模块：包里如果有init文件则先执行该文件，没有也没关系；然后导入该模块import 包名.模块名 // 当前文件能调用init文件所导入的全部内容和该模块。调用方式为包名.模块名from 包名 import 模块名 // 当前文件只能调用该模块，无法调用init文件所导入的内容。调用方式为模块名//(3) 单独导入模块：当前文件和该模块需在同一目录下import 模块名 // 当前文件能调用该模块的所有对象(类、函数、变量等)，调用方式为模块名.对象名//(4) 从模块中导入对象(类、函数、变量等)from 模块名 import 对象名 // 当前文件只能调用该模块下的指定对象，调用方式为对象名from 模块名 import * // 当前文件只能调用该模块下all列表中的对象，若无all列表则能调用所有的对象。调用方式为对象名//(5) 包的相对导入：相对导入只能用from import 的方式from . import 包名 // .表示当前文件所在目录的路径from .. import 包名 // ..表示当前文件所在目录的上级目录的路径//(6) 相对导入应用：init文件中的导入from 包名 import 模块名 // 正确from . import 模块名 // 正确，使用相对导入，其中.表示当前文件所在的目录import 模块名 // 错误 3. import被解释执行 3.1 importlib包介绍 3.2 import的搜索路径 搜索路径被存储在sys模块中的path变量 在导入搜索期间首先会被检查的地方是 sys.modules。 这个映射起到缓存之前导入的所有模块的作用（包括其中间路径）。 当指定名称的模块在 sys.modules 中找不到时，Python 会接着搜索 sys.meta_path，其中包含元路径查找器对象列表。 3.3 import的注意事项 每个包或模块只会被导入一次。 要避免循环导入。 参考资料： Python语言参考——导入系统：https://docs.python.org/zh-cn/3/reference/import.html 菜鸟教程——Python3模块：https://www.runoob.com/python3/python3-module.html","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://holdfire.github.io/tags/Python/"}]},{"title":"C++学习——内存分配","slug":"cpp-memory-allocation","date":"2019-10-29T19:55:20.000Z","updated":"2019-11-01T19:27:24.030Z","comments":true,"path":"2019/10/30/cpp-memory-allocation/","link":"","permalink":"http://holdfire.github.io/2019/10/30/cpp-memory-allocation/","excerpt":"","text":"1. 栈区 由编译器自动分配和释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 2. 堆区 一般有程序员分配和释放，用new运算符申请；若程序员不释放，程序结束时可由OS回收。 3. 全局区（静态区） 全局变量（main函数前声明的变量）和静态变量的存储放在一起。其中，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在一块区域。 4. 文字常量区 常量字符串存储的地方。 为了节省内存，C/C++把常量字符串放到一个单独的内存区域。当几个指针赋值给相同的常量字符串时，它们实际上会指向相同的内存地址。但用常量内存初始化数组时，情况却有所不同。如下所示：str1和str2的地址不同，str3和str4的地址相同。 1234char str1[] = \"Hello world\"char str2[] = \"Hello world\"char* str3 = \"Hello world\"char* str3 = \"Hello world\" 5. 程序代码区 存放函数体的二进制码。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——左值与右值","slug":"cpp-left-value-right-value","date":"2019-10-29T18:12:27.000Z","updated":"2019-10-30T11:23:24.657Z","comments":true,"path":"2019/10/30/cpp-left-value-right-value/","link":"","permalink":"http://holdfire.github.io/2019/10/30/cpp-left-value-right-value/","excerpt":"","text":"1. 基本定义 可以放到赋值操作符左边的是左值，可以放到赋值操作符右边的是右值; 左值的声明符号为&amp;，右值的声明符号为&amp;&amp;； 左值是可以取地址的对象或变量，可以作为右值使用； 右值通常是临时对象、常量等，不能作为左值使用； 右值分为纯右值(Pure RValue，如非应用返回的函数返回值，表达式等)和将亡值(eXpiring Value); 2. 左值引用和右值引用 2.1 基本定义 左值引用和右值引用都是属于引用类型。无论是声明一个左值引用还是右值引用，都必须立即进行初始化。而其原因可以理解为是引用类型本身自己并不拥有所绑定对象的内存，只是该对象的一个别名。 左值引用是具名变量值的别名，通常不能绑定到右值。非常量左值只能接受非常量左值对其进行初始化。 右值引用是不具名（匿名）变量的别名。 右值值引用通常不能绑定到任何的左值，要想绑定一个左值到右值引用，通常需要std::move()将左值强制转换为右值。 2.2 C++11中各引用类型可以引用的值类型 非常量左值引用： 非常量左值； 常量左值引用： 非常量左值；常量左值；非常量右值；常量右值； 非常量右值引用： 非常量右值； 常量右值引用： 非常量右值；常量右值。 2.3 特点 常量左值引用是个全能的引用类型，可用于拷贝语义。它可以接受非常量左值、常量左值、右值对其进行初始化。 非常量右值引用用于移动语义，完美转发。 2.4 示例 1234567891011int a = 1; //正确，变量a可以是左值int &amp;b = a; //正确，因为a是左值，引用类型变量的赋值表达式里，右边必须是左值int &amp;c = 5; //错误，因为5是一个字面值，是右值不是左值int const &amp;d= 100; //正确，是为了能将临时对象作为引用参数传递给函数，为啥？const int &amp;c = b; //常量左值引用绑定到非常量左值，编译通过const int d = 2; //常量左值const int &amp;e = c; //常量左值引用绑定到常量左值，编译通过const int &amp;b =2; //常量左值引用绑定到右值，编译通过int &amp;&amp;r1 = c; // 编译失败，因为c是一个左值int a;int &amp;&amp;r2 = std::move(a); //编译通过 3. 常见例子 3.1 函数的返回值 如果一个函数的返回值是一个临时对象，就是右值。如果返回值为引用，由于引用是对象的别名，通过引用可以改变对象的值，是左值。 12345int arr[] = &#123;1,2,3,4,5&#125;;int fun1(int i)&#123;return arr[i];&#125;int&amp; fun2(int i)&#123;return arr[i];&#125;fun1(0) = 10; // 错误，fun1()是右值，不能被赋值；fun2(0) = 10; // 正确，fun2()是左值，可以被赋值； 3.2 前置和后置 前置的返回值是被的对象的引用，是一个可寻址的变量。返回值是左值，可以被赋值，如++i=3。 后置的返回值是被对象的原始值，是一个临时对象。返回值是右值，不可以作为左值，代码i++=3是错误的。 下面的代码是C中前置和后置++的实现： 1234// 前置++的实现T&amp; T::operator++() &#123;++*this; return this;&#125;// 后置++的实现T T::operator++(int)&#123;T old(*this); ++(*this); return old;","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——指针","slug":"cpp-pointer","date":"2019-10-29T15:31:44.000Z","updated":"2019-11-01T19:22:56.855Z","comments":true,"path":"2019/10/29/cpp-pointer/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-pointer/","excerpt":"","text":"1. 简介 1.1 指针 1.2 空指针 1.3 智能指针 2. 指针的使用 指针与数组 C/C++中没有记录数组的大小，因此使用指针访问数组元素是，注意不能超过数组的边界。 当数组作为函数的参数进行传递时，数组就自动退化为同类型的指针。 2.1(int*)a 和 int *a的区别？ 3. 指针和引用","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——引用","slug":"cpp-reference","date":"2019-10-29T15:31:23.000Z","updated":"2019-10-30T13:14:08.234Z","comments":true,"path":"2019/10/29/cpp-reference/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-reference/","excerpt":"","text":"1. 简介 引用是一种特殊的变量，可以被认为是一个变量的别名。 例如：int a = 1; int &amp;b = a;变量b即为变量a的引用。 声明一个引用时，必须同时对它进行初始化，使它指向一个已经存在的对象。 一旦一个引用被初始化后，就不能改为指向其他对象。 引用变量和原变量取地址运算&amp;的结果相同。 2. 引用作为函数参数 2.1 引用传递 首先区分一下两个概念：当函数的参数为普通变量时，参数传递的方式称作值传递。当函数的参数为引用时，参数传递的方式称作引用传递。函数在被调用时，若发生的是值传递，形参会被分配内存单元并与实参结合；若发生的是引用传递，由于形参时实参的一个别名，对形参的任何操作都会直接作用于实参。 下方代码中：执行fun1(x1)时参数传递方式为值传递，形参会被分配内存单元，函数调用完后x1的值仍为0。执行fun1(x1)时参数传递方式为引用传递，函数调用栈中不会产生变量x2的副本，而是直接用变量x2的值进行运算，函数调用完后x2的值为1。 12345void fun1(int a) &#123;a++; return;&#125; // 值传递void fun2(int &amp;a)&#123;a++; return;&#125; // 引用传递int x1 = 0, x2 = 0;fun1(x1); // x1的值不会被修改fun2(x2); // x2的值会被修改为1 2.2 引用传递——形参为常引用 如果一个函数的形参被申明为常引用，那么在函数体中，就不能修改该引用变量的值，达到了引用的安全性。下方代码将会报错：“常引用对象a为只读对象，不能进行a++操作。” 123void fun3(const int &amp;a)&#123;a++; return;&#125; // 错误：参数为常引用，只读不允许被修改int x3 = 0; fun3(x3)； 2.3 引用传递——形参为引用时需注意 如果一个函数的形参为引用时，那么需要注意传递的实参不能为const类型。而C++中的临时对象都是const类型的！ 如函数的返回值，未命名的内容都是临时变量，因而下面的代码会运行错误。如果把fun4()的形参设为(int a)或者(const int &amp;a)，代码就可以正常运行了。 所以，引用型参数应该在能被定义为const的情况下，尽量定义为const! 1234void fun4(int &amp;a)&#123;&#125;int helper()&#123;return 0;&#125;fun4(helper())； // 错误：helper()返回的临时对象是const类型，不能作为实参fun4(0); // 错误：未命名的数字0是const类型，不能作为实参 3. 引用作为函数返回值 引用作为函数的返回值时，内存中不会产生返回值的副本。 返回的应该是一个函数外就存在的对象； 返回对象不能是函数体内局部变量的引用，因为函数调用完毕，局部变量就不存在了； 返回对象不能是表达式，不能是new分配的对象。 123456int arr[] = &#123;1,2,3,4,5&#125;;int&amp; fun5(int i)&#123;return arr[i];&#125;int i = 0;cout&lt;&lt;&amp;arr[i]&lt;&lt;endl; // 返回结果：0x100402010cout&lt;&lt;&amp;fun5(i)&lt;&lt;endl; // 返回结果：0x100402010fun5(i) = 10; // 这也可以，牛不，就因为人家函数的返回值为引用，是左值，可以被赋值 返回值为引用时，为左值，返回值可以被赋值、取地址等操作！ 返回值不是引用时，为右值，如下方代码所示。 123456int arr[] = &#123;1,2,3,4,5&#125;;int fun6(int i)&#123;return arr[i];&#125;int i = 0;cout&lt;&lt;&amp;arr[i]&lt;&lt;endl; // 返回结果：0x100402010cout&lt;&lt;&amp;fun6(i)&lt;&lt;endl&gt;&gt;; // 错误，返回值为右值，不能取地址；fun6(i) = 10; // 错误，不可以的兄弟，fun6的返回值是右值 4. 引用总结 引用传递主要应用在函数参数传递上，传送较大数据和对象时，内存中不会产生副本，节约了内存空间； 用const将参数设置为常引用，用以保证引用变量不被随意修改； 跟指针变量相比，引用可以看做变量的别名，代码可读性强，在C++中推荐使用引用而非指针作为函数的参数；","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——多态性","slug":"cpp-polymorphism","date":"2019-10-29T11:20:16.000Z","updated":"2019-10-30T11:23:36.670Z","comments":true,"path":"2019/10/29/cpp-polymorphism/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-polymorphism/","excerpt":"","text":"1. 简介 1.1 C++的4种多态 多态性是指一段程序能够处理多种类型对象的能力。C++中可以通过强制多态、重载多态、类型参数化多态、包含多态4种形式实现。前两种是表面的多态性，后两种是真正的多态性。 强制多态：将一种类型的数据强制转化成另一种类型（隐式转换或显式转换） 重载多态：是指给同一个名字赋予不同的含义 参数化多态：通过模板实现，分为函数模板和类模板两种 包含多态：通过虚函数实现，虚函数是多态性的精华 1.2 静态绑定和动态绑定 多态性还可以分成编译时的多态和运行时的多态：其中，绑定工作在编译连接阶段完成的情况称为静态绑定，在程序运行阶段完成的情况称为动态绑定。绑定时指计算机程序自身彼此关联的过程，用面向对象的术语讲，就是把一条消息和一个对象的方法相结合的过程 2. 强制多态 3. 重载多态 4. 参数化多态 5. 包含多态","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——const限定符","slug":"cpp-const","date":"2019-10-29T10:57:15.000Z","updated":"2019-10-30T11:23:18.538Z","comments":true,"path":"2019/10/29/cpp-const/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-const/","excerpt":"","text":"1. const简介 简介：用const关键字修饰的内容为常量，其值初始化后就不允许修改。 命名：常用的做法是将这个常量的首字母或全部大写，以提醒用户这是个常量。 初始化：在声明一个常量时就应该将其初始化，如const int Name = value;。 替换：C语言中通常使用#define来定义常量，而const能够明确指定类型和限定作用域，优于前者。 优点：在函数的参数传递中，在引用传递前加上const限定符，可以防止输入变量被修改。 2. const的用法 const可以用来修饰整型变量、引用变量、指针变量、函数、对象、数组等。修饰的是其左边相邻的内容，如果左边没有内容，则修饰右边相邻的内容。 12int age = 3; // 我的年龄age是个整型变量，可以修改，我今年3岁const int Age_Mao = 83; // age_Mao是由const修饰的常量，其值初始化后不能修改 2.1 常引用 下面的year就是对变量age的一个常引用，不能通过year修改age的值，但可直接修改age的值。常引用的这一优点可以应用在函数的参数传递上，既能节约内存，也能防止引用对象被修改。 123const int &amp;year = age; // year是一个常引用*b = 4; // 错误，常引用year，你是常引用，你不能改我的age啊age = 4; // 正确，我的年龄age可以通过赋值修改，明年我4岁 2.2 常指针 指针p指向age所在地址，不能指向别的。 1int *const pt1 = &amp;age; 2.3 指向常量的指针 如下所示：pt2和pt3均为指向常量的指针，不能通过指针来修改age或者age_Mao。（但age可直接修改，age_Mao不能修改） 123const int *pt2 = &amp;age; const int *pt3 = &amp;Age_Mao;int *p4 = &amp;Age_Mao; // 错误用法：禁止将const的地址附给非const指针(但const_cast可强制转换) 2.4 复杂情况：将指针指向指针 参考《C++ Primer Plus》P222 12int *pd = &amp;age; // *pd = 4 是有效的const int *pt = pd; // *pt = 4 是无效操作(一级间接关系) 3. 常成员函数 1类名::fun(形参) const; // 常成员函数： 4. 常对象 1类名 const 对象名; // 常对象： 5. 其他 1int const 数组名[]; // 常数组： 6. const_cast","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"Linux——Linux命令介绍","slug":"linux-command","date":"2019-10-26T21:25:15.000Z","updated":"2019-10-30T11:21:53.054Z","comments":true,"path":"2019/10/27/linux-command/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-command/","excerpt":"","text":"1. 简介 1.1 Linux命令分类 Linux命令分为两种，一种是shell内置命令，一种是外部命名。在命令行输入type command可以区分。 1.2 帮助使用Linux命令 对于一些内部命令，可以有： 123man bash # 查看所有的内部命令man help # 得到所有内建命令列表及使用方法help command # 得到该命令的具体信息 对于外部命令，可以有： 1234command --help # 得到该命令的帮助信息command -h # 同上man command # info command # 1.3 Linux执行命令的查找顺序 环境变量$PATH：决定了shell在哪些目录中寻找命令或程序。由左到右依次在这些目录中查找，以先找到的为准。命令行输入echo $PATH可以查看环境变量。 1.4 Linux常用命令的存放位置 在命令行输入完一个命令后，shell会在下面几个目录查找是否有该命令： 12345/bin # 存放普通用户经常使用的命令，如cp,mv,kill/sbin # 存放系统管理员才能使用的系统管理程序，如ping,ifconfig/usr/bin # 存放普通用户使用的应用程序，如python,vi/usr/local/bin # 存放用户下载安装的软件或者自己编写的可执行文件，如pip,virtualenv,MySQL/usr/sbin # 存放系统管理员使用的比较高级的管理程序和系统守护程序","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——系统目录结构","slug":"linux-system-directory","date":"2019-10-26T21:23:19.000Z","updated":"2019-10-30T11:22:10.261Z","comments":true,"path":"2019/10/27/linux-system-directory/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-system-directory/","excerpt":"","text":"1. 简介","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——文件系统与目录篇(1)","slug":"linux-file-directory-1","date":"2019-10-26T19:17:45.000Z","updated":"2019-10-30T11:21:59.352Z","comments":true,"path":"2019/10/27/linux-file-directory-1/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-file-directory-1/","excerpt":"","text":"1. Linux的文件权限","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——简介","slug":"linux-introduction","date":"2019-10-26T16:10:34.000Z","updated":"2019-10-30T11:22:03.332Z","comments":true,"path":"2019/10/27/linux-introduction/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-introduction/","excerpt":"","text":"1. Linux简介 Linux是一个GNU GPL授权模式的开源的操作系统。操作系统由内核和系统调用组成,操作系统是应用程序和计算机硬件沟通的桥梁。即：硬件–&gt;内核–&gt;系统调用–&gt;应用程序(Shell)。 1.1 Linux内核版本与发行版本 Linux内核版本是单线发展的，其版本号由“主版本.次版本.末版本”的格式构成。截至2019年10月27日，最新的Linux稳定内核版本为5.3.7版。 Linux发行版本是由“Linux内核+软件+工具+文件”组成的一个完整的安装程序，不同的厂商提供不同的发行版本，主要分为两大类： 使用rpm/yum包管理方式的系统，包括：RedHat,Fedora,CentOS等； 使用apt-get/dpkg包管理方式的系统，包括：Debian,Ubuntu等； 1.2 Linux的特点 Linux严格区分大小写； Linux中一切内容皆文件（包括硬件）； Linux不依靠拓展名区分文件类型，而是依据文件权限； 通常，压缩包的后缀为.gz .bz2 .tar .bz2 .tgz；二进制软件包为.rmp；脚本文件为 .sh 2. Linux文件系统和目录 3. Shell与Shell Script 4. Linux用户管理 5. Linux系统管理","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——Shell篇(1)","slug":"linux-shell-1","date":"2019-10-26T16:00:27.000Z","updated":"2019-10-30T11:22:06.845Z","comments":true,"path":"2019/10/27/linux-shell-1/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-shell-1/","excerpt":"","text":"1. Shell是什么？ 1.1 正规军的解释 Shell是一个用C语言编写的程序！Shell是一个命令解释器！Shell是一种命令语言！Shell是一种程序设计语言！Shell是系统的用户界面，提供了用户与内核进行交互操作的接口！上面是网上找到的关于Shell的大部分解释，听着是不是让人头大？是的？！那就忘掉它，看看下面两个场景先。 1.2 接待员老白和Shell 场景A：你去同福客栈吃饭，你对接待员老白说：“我要一份酸菜鱼。”老白对着你点头微笑表示ok，然后跑去前台的点菜机上噼里啪啦输入一堆信息，该消息随后传到了后厨李大嘴那里，李大嘴开始鼓捣起他的菜刀、砧板、狼牙棒等厨具。一个时辰过后，你的桌上终于出现了一份既不好看也不好吃的酸菜鱼。 场景B：你在Linux终端工具上写代码，你写下了一行echo &quot;Hello, world&quot;,Shell对这行代码进行了解释，然后系统调用收到了该解释信息，信息接下来被传到了内核那里，内核开始控制CPU、内存、显卡等设备。一眨眼的时间，你的显示器上就出现了&quot;Hello, world&quot;这行字。 补充概念（此处可不看）：计算机的硬件由CPU、内存、显卡、磁盘等组成。操作系统包括内核和系统调用，内核（kernel，包括CPU调度、内存管理、磁盘输入输出等）直接控制上述硬件，系统调用是应用程序同系统之间的接口(API)，eg:Linux操作系统中如内存管理、网络、Socket套接字、进程间通信等接口。 1.3 游击队的解释 场景A中信息的流动方向是：你说的话–&gt;接待员老白–&gt;点菜机—&gt;李大嘴–&gt;厨具，场景B中信息的流动方向是：你的代码–&gt;Shell–&gt;系统调用—&gt;内核–&gt;硬件。仅考虑信息的流动方向，我们可以认为场景A等价于场景B。那么在场景A中，你说的话就是你的代码，接待员老白就是Shell，点菜机就是系统调用，李大嘴就是内核，厨具就是计算机的各种硬件。再回过头来看下1.1中Shell的解释，是不是就很好理解了呢？如果再有人问你Shell是什么，你就告诉他：Shell就是个接待员！ 2. Shell简介 2.1 Shell版本家族 常见的shell有Bourne shell(简称sh),在Sun里默认的C shell(简称csh)等。Linux使用的版本是Bourne Again Shell(简称bash)，它是Bourne shell的增强版本。 2.2 Linux中的Bash shell 使用man bash命令可以查看Bash shell的说明文档。用户在登录linux时，系统会分配一个shell用来工作，这个shell记录在/etc/passwd中。此外，/etc/shells文件中记录了当前可以使用的shell种类。作者在ubuntu16.04.5上查看的结果如下： 1234/bin/sh # 已经被/bin/bash所替换/bin/dash/bin/bash # Linux默认的shell/bin/rbash 3. Shell作为一个C语言程序 以Linux系统中Bash shell为例，主要有下面的优点： 历史命令(history)：历史命令保存在用户家目录~/.bash_history文件中。需要注意该文件记录的是这次登陆之前执行过的命令，这次登录执行过得命令缓存在内存中，注销系统后才会被保存。 命令与文件补全功能(Tab按键) :不仅能少打字，还能确保你的输入是正确的。 命令别名设置(alias) :在命令行输入alias就能查看当前命令的别名，这些别名保存在用户家目录~/.bashrc文件中。 任务管理、前台、后台控制(job control,foreground,background) :可以随时将任务丢到后台中执行，防止不小心使用ctrl+c停止程序。 通配符(Wildcard) :bash支持许多通配符来帮助用户查询与执行命令。 4. Shell作为命令语言和命令解释器 4.1 Bash shell的内置命令 Linux命令分为两种，一种是shell内置命令，一种是外部命名。使用type命令可以查看。 4.2 命令的执行 命令太长时，可以使用反斜杠\\对[Enter]键进行转义，注意两者中间不能有其他字符 4.3 命令的快速编辑 一些方便的命令快速编辑按钮 1234ctrl+u: 从光标处向前删除命令串ctrl+k: 从光标处向后删除命令串ctrl+a: 让光标移动到命令串最前面ctrl+e: 让光标移动到命令串最后面 5. Shell作为程序设计语言 Shell的优点是可以作为一种程序设计语言，通过编写程序化脚本(shell scripts)，可以将需要连续执行的命令写成一个shell脚本文件来执行。shell脚本主要有2中运行方式： 5.1 脚本作为可执行程序 假设有一个test.sh的shell脚本文件，存放在path目录下，则执行方法为： 123cd path # 切换至脚本所在目录pathchmod +x ./test.sh # 使脚本具有执行权限./test.sh # 执行脚本，不能写成test.sh，会被shell当成命令来查找 5.2 脚本作为解释器参数 这种方式是直接运行解释器，文件名作为参数，如/bin/sh test.sh 6. Shell作为用户与内核交互操作的接口 6.1 为什么要有Shell？ Shell的存在是有很多好处的。假设在同福客栈里没有了接待员老白，让你自己去操作点菜机，你不知道如何操作点菜机怎么办？你点了一道不存在的菜怎么办？ 6.2 为什么有多种Shell？ 假设同福客栈偶尔还接待外宾，点菜的时候说的是外语。而接待员老白不会说外语，那么我们是不是就需要另一个服务员（另一种Shell）了。不同的Shell是为了应对应用程序有不同的需求。 6.3 为什么要有系统调用？ 6.4 为什么要有内核？","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Python——虚拟环境","slug":"python-virtual-environment","date":"2019-10-26T09:00:10.000Z","updated":"2019-10-30T11:21:32.804Z","comments":true,"path":"2019/10/26/python-virtual-environment/","link":"","permalink":"http://holdfire.github.io/2019/10/26/python-virtual-environment/","excerpt":"","text":"1. Python虚拟环境简介 Python中所有的第三方包都会被pip安装到site-packages目录下。对于某个特定的Python包，在项目A中可能需要使用1.0版的，而项目B中需要使用1.1版的。如何解决这种冲突呢？ 解决方案是：创建两个不同的虚拟环境，这两个虚拟环境相互独立、互不干扰。不同的项目就可以分别在各自的虚拟环境中独立运行啦，完美！（引申：不同虚拟环境的包是独立的，那它们使用的Python解释器是共享的还是独立的呢？） 那么如何创建虚拟环境呢？接下来就总结一下几种常用的虚拟环境管理工具。 2. 几种虚拟环境管理工具 2.1 virtualenv工具 virtualenv工具在安装Anaconda时有自带，位于安装目录/Lib/site-packages目录下。如果没有安装，可以在终端命令行中输入pip install virtualenv命令安装。 创建虚拟环境： 12345mkdir my_project # 创建项目文件夹cd my_project # 进入项目路径下virtualenv env_name # 使用默认的python解释器创建虚拟环境virtualenv -p path env_name # 使用path路径下的python解释器创建虚拟环境，Linux下path通常为/usr/bin/python,Windows中如e:/anaconda/python.exe # 使用virtualenv -h 可查看创建虚拟环境时的其他参数 操作虚拟环境： 1234567# Linux下操作虚拟环境source env_name/bin/activate # 激活该虚拟环境，之后所有的操作都在该环境中deactivate # 退出该虚拟环境rm -rf env_name # 删除该虚拟环境# Windows下操作虚拟环境，退出和删除操作同上cd env_name/Scripts/ # 进入到activate文件所在的目录activate # 执行activate文件，激活该虚拟环境 2.2 virtualenvwrapper工具 用户可以使用virtualenv工具在系统的任意地方创建虚拟环境。但下次需要激活这个环境时仍需要使用source env_path/activate命令，然而同学你可能早已忘记虚拟环境的路径env_path。 为了让用户更方便使用，我们可以把虚拟环境集中进行管理，virtualenvwrapper工具应运而生。在使用之前，你得先安装了virtualenv，然后再安装virtualenvwrapper。安装命令： pip install virtualenvwrapper # Linux下安装命令 pip install virtualenvwrapper-win # Windows下安装命令","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://holdfire.github.io/tags/Python/"}]},{"title":"分类模型——支持向量机SVM","slug":"machine-learning-SVM","date":"2019-10-25T15:43:05.000Z","updated":"2019-11-03T12:56:12.736Z","comments":true,"path":"2019/10/25/machine-learning-SVM/","link":"","permalink":"http://holdfire.github.io/2019/10/25/machine-learning-SVM/","excerpt":"","text":"1. 支持向量机简介 支持向量机(Support Vector Machine, SVM)是一种二分类模型。其分类决策函数为： f(x)=sign(wT⋅x+b)(1)\\tag{1} f(x)=sign(w^T \\cdot x+b) f(x)=sign(wT⋅x+b)(1) 线性可分支持向量机学习的最优化问题为： min⁡w,b 12∥w∥2(2)\\tag{2}\\min_{w,b}\\ \\ \\ \\frac {1} {2} \\begin{Vmatrix} w \\end{Vmatrix}^2 w,bmin​ 21​∥∥​w​∥∥​2(2) s. t. yi(w⋅xi+b)−1 ≥0 i=1,2,...,N(3)\\tag{3} s.\\ t.\\ \\ \\ y_i(\\bm w\\cdot \\bm x_i+b)-1\\ \\ge 0\\ \\ \\ \\ \\ i=1,2,...,N s. t. yi​(w⋅xi​+b)−1 ≥0 i=1,2,...,N(3) 对每一个不等式约束引入拉格朗日乘子αi≥0, i=1,2,...,N\\alpha _i \\ge 0,\\ \\ i=1,2,...,Nαi​≥0, i=1,2,...,N后构造拉格朗日泛函L(w,b,α)L(\\bm w,b,\\bm \\alpha )L(w,b,α)，则上述优化问题等价为： min⁡w,b max⁡α L(w,b,α)=12∥w∥2−∑i=1Nαi[yi(w⋅xi+b)−1](4)\\tag{4} \\min_{\\bm w,b}\\ \\max_{\\bm \\alpha}\\ L(\\bm w,b,\\bm \\alpha )=\\frac 1 2\\begin{Vmatrix} w \\end{Vmatrix}^2-\\sum_{i=1} ^N \\alpha _i [y_i(\\bm w\\cdot \\bm x_i+b)-1] w,bmin​ αmax​ L(w,b,α)=21​∥∥​w​∥∥​2−i=1∑N​αi​[yi​(w⋅xi​+b)−1](4) 在上式的鞍点处，目标函数L(w,b,α)L(\\bm w,b,\\bm \\alpha )L(w,b,α)对w\\bm ww和bbb的偏导数为零。由此得到，在最优解处满足： w∗=∑i=1Nαiyixi(5)\\tag{5} \\bm w^*= \\sum_{i=1} ^N \\alpha _i y_i \\bm x _i w∗=i=1∑N​αi​yi​xi​(5) ∑i=1nαi∗yi=0(6)\\tag{6} \\sum_{i=1} ^n {\\alpha _i}^* y_i = 0 i=1∑n​αi​∗yi​=0(6) 将上面两个条件代入拉格朗日泛函中，则原问题的解等价于下面对偶问题的解： max⁡α Q(α)=∑i=1Nαi−12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)(7)\\tag{7}\\max_{\\bm \\alpha}\\ \\ Q(\\bm \\alpha)=\\sum_{i=1} ^N \\alpha _i -\\frac 1 2 \\sum_{i=1} ^N \\sum_{j=1} ^N \\alpha _i \\alpha _j y_i y_j(\\bm x_i \\cdot \\bm x_j) αmax​ Q(α)=i=1∑N​αi​−21​i=1∑N​j=1∑N​αi​αj​yi​yj​(xi​⋅xj​)(7) s. t. ∑i=1Nαiyi=0(8)\\tag{8} s.\\ t.\\ \\ \\ \\ \\sum_{i=1} ^N \\alpha _i y_i=0 s. t. i=1∑N​αi​yi​=0(8) αi≥0 i=1,2,...,N(9)\\tag{9} \\alpha _i \\ge 0\\ \\ i=1,2,...,N αi​≥0 i=1,2,...,N(9) 通过对偶问题的解αi∗, i=1,2,...,N\\alpha _i^*,\\ i=1,2,...,Nαi∗​, i=1,2,...,N，代入上式（5）中可以求出原问题的解w∗\\bm w^*w∗，接下看如何求解b。 根据最优化理论中的KKT条件，如果原问题的最优解为对偶问题的最优解，需要满足： αi[yi(w⋅xi+b)−1]=0(10)\\tag{10}\\alpha _i [y_i(\\bm w\\cdot \\bm x_i+b)-1]=0 αi​[yi​(w⋅xi​+b)−1]=0(10) 支持向量对应的αi\\alpha _iαi​是大于0的，因而对于这些点有： yi(w⋅xi+b)−1=0(11)\\tag{11} y_i(\\bm w\\cdot \\bm x_i+b)-1=0 yi​(w⋅xi​+b)−1=0(11) 因为已经求出了w∗\\bm w^*w∗,所以b∗b^*b∗可以用任何支持向量根据上式求得。实际数值计算中，人们通常采用所有非求解αi\\alpha _iαi​非零的样本求解b∗b^*b∗再取平均值。 2. 线性支持向量机与软间隔最大化 3. 非线性支持向量机与核函数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"监督学习","slug":"监督学习","permalink":"http://holdfire.github.io/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"网站出生的第一天","slug":"my_first_blog","date":"2019-10-25T10:51:19.000Z","updated":"2019-10-25T19:00:16.314Z","comments":true,"path":"2019/10/25/my_first_blog/","link":"","permalink":"http://holdfire.github.io/2019/10/25/my_first_blog/","excerpt":"","text":"1. 网站简介 这是holdfire于2019年10月25日创建的个人博客网站。创建步骤： 首先，在个人电脑上安装node.js(一种运行在服务端的 JavaScript，包含环境变量及npm的安装)； 然后，安装个人博客网站框架Hexo(在cmd中使用npm install -g hexo-cli等)； 接下来，创建一个新文件夹，执行hexo init,初始化自己的博客网站。用hexo s命令可以运行该网站。就可以在浏览器中访问啦！ 最后，在站点配置文档_config.yml中，设置你的github仓库地址，将网站内容托管在github上，就可以通过域名访问啦！ 补充，可以在hexo官网中下载各种自己喜欢的网站主题，然后修改主题配置文档_config.yml即可。","categories":[{"name":"软件安装","slug":"软件安装","permalink":"http://holdfire.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"web前端","slug":"web前端","permalink":"http://holdfire.github.io/tags/web%E5%89%8D%E7%AB%AF/"}]}]}