{"meta":{"title":"holdfire","subtitle":"学无止境，不忘初心！","description":"一只想上天的菜鸟！","author":"holdfire","url":"http://holdfire.github.io","root":"/"},"pages":[{"title":"关于作者","date":"2019-11-01T23:41:44.000Z","updated":"2019-11-06T11:48:20.904Z","comments":true,"path":"about/index.html","permalink":"http://holdfire.github.io/about/index.html","excerpt":"","text":"机器学习课程 课程名称 课程网站 学习进度 相关资料 Stanford CS231n: CNN for Visual Recognition 课程网站 leture 03 课程作业 Stanford CS224n: NLP with Deep Learning 课程网站 B站课程，课程作业 Stanford CS224s：Spoken Language Processing 课程网站 Stanford CS229: Machine Learning 课程网站 Stanford CS228: Probabilistic Graphical Models 课程网站 Stanford CS246: Mining Massive Data Sets 课程网站 Stanford EE364A: Convex Optimization 课程网站 EE364B University College London: Reinformation Learning 课程网站 B站课程，学习笔记 University of Minnesota: Recommender Systems 课程网站 吴恩达：Machine Learning 课程网站 吴恩达：Deep Learning 课程网站 台湾大学：李宏毅-机器学习 课程网站 B站课程 台湾大学：林轩田-机器学习基石和技巧 课程网站 计算机/数学课程 课程名称 课程网站 学习进度 相关资料 MIT：算法导论 B站课程 MIT：线性代数 B站课程 UBC：Ravi Ramamoorthi-计算机图形学导论 课程网站 清华大学：邓俊辉-计算几何学 课程网站 清华大学：胡事民-计算机图形学 B站课程 重庆大学：王成良-Web开发技术 课程网站 南京大学：骆斌-计算机操作系统 课程网站 华南大学：袁华-计算机网络 课程网站 哈尔滨工业大学：战德臣-数据库系统（上） 课程网站 书籍阅读 书籍名称 作者 链接 阅读进度 Deep Learning Ian Goodfellow et al. 书籍链接 Chapter1 Computer Vision-Algorithms and Applications Richard Szeliski 书籍链接 Chapter1 Neural Networks and Deep Learning Michael Nielsen 书籍链接 Chapter 4 统计学习方法（第二版） 李航 豆瓣读书"},{"title":"分类","date":"2019-11-01T23:17:21.000Z","updated":"2019-11-01T23:46:29.026Z","comments":true,"path":"categories/index.html","permalink":"http://holdfire.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-01T23:17:00.000Z","updated":"2019-11-01T23:45:42.167Z","comments":true,"path":"tags/index.html","permalink":"http://holdfire.github.io/tags/index.html","excerpt":"","text":""},{"title":"计划日程","date":"2019-11-06T09:36:47.000Z","updated":"2019-11-06T09:38:38.155Z","comments":true,"path":"schedule/index.html","permalink":"http://holdfire.github.io/schedule/index.html","excerpt":"","text":""}],"posts":[{"title":"Linux——文件系统挂载","slug":"linux-file-mount","date":"2019-11-08T10:23:16.000Z","updated":"2019-11-08T10:26:11.658Z","comments":true,"path":"2019/11/08/linux-file-mount/","link":"","permalink":"http://holdfire.github.io/2019/11/08/linux-file-mount/","excerpt":"","text":"1. 简介","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Windows——环境变量path","slug":"windows-environment-path","date":"2019-11-08T09:10:52.000Z","updated":"2019-11-08T09:15:07.395Z","comments":true,"path":"2019/11/08/windows-environment-path/","link":"","permalink":"http://holdfire.github.io/2019/11/08/windows-environment-path/","excerpt":"","text":"1. 简介 windows环境变量","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"http://holdfire.github.io/tags/Windows/"}]},{"title":"Windows——cmd命令","slug":"windows-cmd-commands","date":"2019-11-08T09:04:34.000Z","updated":"2019-11-08T10:25:23.434Z","comments":true,"path":"2019/11/08/windows-cmd-commands/","link":"","permalink":"http://holdfire.github.io/2019/11/08/windows-cmd-commands/","excerpt":"","text":"1.简介 1.1 cmd命令解释器 cmd命令行解释器是一个单独的软件程序，它可在用户和操作系统之间提供直接的通讯。命令行解释器是解释器的一种，用于对命令行进行解释执行。 cmd.exe命令行解释器环境由确定命令行解释器和操作系统行为的变量进行定义。可以使用两种类型的环境变量（系统和局部）来定义命令行解释器环境或整个操作系统环境的行为。系统环境变量定义全局操作系统环境的行为。局部环境变量定义Cmd.exe当前实例环境的行为。 1.2 cmd可执行程序的类型及存放位置 以dos系统而言，可执行程序大约可以细分为五类，依照执行优先级由高到低排列如下。其中，内部系统命令存放在。可执行程序在当前目和环境变量下进行搜索。 （1）DOSKEY宏命令（预先驻留内存） （2）COMMAND.COM中的内部命令（根据内存的环境随时进驻内存） （3）以com为扩展名的可执行程序（由command.com 直接载入内存） （4）以exe位扩展名的可执行程序（由command.com 重定位后载入内存） （5）以bat位扩展名的批处理程序（由command.com 解释分析，根据其内容按优先级顺序调用第2，3，4，5种可执行程序，分析一行，执行一行，文件本身不载入内存） 1.3 执行命令时的搜索顺序 （1）如果该命令不带后缀 首先在无后缀的系统命令中搜索； 然后在当前目录中查找该命令+.exe、.msc、.bat等后缀的可执行文件或批处理文件； 在环境变量那些目录中按上一条的规则搜索； （2）如果该命令带后缀 首先在当前目录中搜索该文件，若存在，如果该文件是一个可执行文件或批处理文件，则执行之，如果是其他一般文件则用与该类型文件关联的默认程序打开它； 若当前目录不存在该文件,则在当前目录中查找是否存在以该文件名+可执行文件或批处理文件后缀（.exe、.bat、.msc等）命名的文件，如果找到了则执行之; 如果在当前目录中上述两种情况都未找到，才在环境变量所设置的那些目录中按上述顺序搜寻。先是按cmd命令所给的准确文件名查找，如果有，是程序或批处理则执行，是其它文件就用默认程序打开； 如果在环境变量目录中未找到该文件，再在环境变量目录中查找是否存在该文件名+可执行文件或批处理文件后缀（.exe、.bat、.msc等）的文件，如果找到了则执行之。 如果还是没有，则只好报错，该命令 is not recognised as an internal or external command, operable program or batch file. （3）如果cmd命令中带路径，很明显只在指定目录中寻找文件，而不会到环境变量中去找，如果文件名不带后缀，则跟第一种情况一样，在指定目录中寻找这个名称的可执行文件或批处理文件执行，找不到报错；如果带后缀，若存在，则执行或用默认程序打开，若不存在，寻找该文件名+可执行文件或批处理文件后缀的文件来执行，找不到报错。 2. 内置命令 2.1 文件和目录相关操作 12cd dir # 切换到该目录下mkdir dir # 创建新目录 2.2 网络相关 12ipcofig # 查看当前IP配置ping 域名/IP地址 # 查看网络连接是否正常 2.3 文件搜索相关 123456find / -name file1 # 从 &apos;/&apos; 开始进入根文件系统搜索文件和目录find / -name \\*.rpm # 搜索以&apos;.rpm&apos;结尾的文件并定义其权限locate \\*.ps # 寻找以 &apos;.ps&apos; 结尾的文件 - 先运行 &apos;updatedb&apos; 命令where command # 查找该命令所在路径whereis halt # 显示一个二进制文件、源码或man的位置which halt # 显示一个二进制文件或可执行文件的完整路径 3. 概念区别 3.1 Shell的定义 操作系统可以分成核心（kernel）和Shell（外壳）两部分，其中，Shell是操作系统与外部的主要接口，位于操作系统的外层，为用户提供与操作系统核心沟通的途径。Shell是一个命令解释器(也是一种应用程序)，处于内核和用户之间，负责把用户的指令传递给内核并且把执行结果回显给用户。同时，shell也可以作为一门强大的编程语言。 Shell分为图形界面shell和命令行shell两大类，如windows的资源管理器explorer.exe和cmd命令窗口，linux系统中的bash。 3.2 cmd和文件资源管理器 在windows系统中见到的桌面即explorer.exe（文件资源管理器）是 图形shell，而cmd就是 命令行shell。 3.3 cmd和dos dos本身是一个系统，这是cmd与dos的最大区别：一个只是接口、一个是操作系统。只是cmd中的某些命令和dos中的命令相似，因此很多人把二者混为一谈。cmd属于windows系统的一部分，dos本身就是一个系统，在dos系统下可以删除，修复windows系统，而在cmd下则不行。 3.4 cmd和bash bash是Linux和Unix下的shell，如果真的想试用，可以在MS windows下安装Cygwin环境，然后再在其下使用。 这时需要注意，Cygwin环境下跟真实的Linux或Unix是有区别的，一些命令会运行不正常。最直接的体验，还是使用Linux来得贴心，几乎可以做任何事情。如果想在MS Windows下使用Shell，建议还是使用微软的PowerShell，它能提供给你操作MS windows的完全功能。 参考资料： cmd命令搜索顺序：https://www.cnblogs.com/idisposable/p/5137808.html shell，dos，cmd和脚本语言：https://www.cnblogs.com/lishanyang/p/9224988.html","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"http://holdfire.github.io/tags/Windows/"}]},{"title":"Python-conda和Anaconda介绍","slug":"python-anaconda","date":"2019-11-08T07:20:56.000Z","updated":"2019-11-08T07:25:44.688Z","comments":true,"path":"2019/11/08/python-anaconda/","link":"","permalink":"http://holdfire.github.io/2019/11/08/python-anaconda/","excerpt":"","text":"1. 简介","categories":[],"tags":[]},{"title":"编译原理——gcc,g++,make和cmake的区别","slug":"complie-make-cmake","date":"2019-11-07T18:32:53.000Z","updated":"2019-11-07T18:53:17.652Z","comments":true,"path":"2019/11/08/complie-make-cmake/","link":"","permalink":"http://holdfire.github.io/2019/11/08/complie-make-cmake/","excerpt":"","text":"1. gcc和g++的区别 二者的区别在于： （1）对于.c的文件，gcc把它当成是c程序，g当做是c程序；对于.cpp的文件，两者都会认为是c程序。 （2）编译可以用gcc或g，链接可以用gcc-lstdc或者g。 （3）如果后缀为.C的文件定义了_cplusplus宏，gcc编译器认为该宏是未定义的，否则就是已定义的。 （4）无论是gcc还是g++，当源文件中用extern &quot;C&quot;时，都是以C的命名方式来为symbol命名的；否则，都以c的方式命名。 （5）在编译阶段，g是调用gcc的。 2. make和cmake的区别 但如果源文件太多，一个个编译会太麻烦。于是人们设计了一个类似批处理的程序来来批量编译源文件，于是就有了make工具，但你需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile。所以编写makefile也是一个程序员所必备的技能。 对于一个大型工程，编写makefile是一件复杂的事。于是人们又设计了一个读入所有源文件后，自动生成makefile的工具，这就是cmake，它能够输出makefile或者project文件。但是程序员还是要编写cmakefile，它是cmake所依据的规则。","categories":[{"name":"编译原理","slug":"编译原理","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"编程语言","slug":"编译原理/编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"编译原理","slug":"编译原理","permalink":"http://holdfire.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"图像特征——基于描述子的图像对齐","slug":"cv-imageFeature-3alignment","date":"2019-11-07T11:41:28.000Z","updated":"2019-11-07T11:54:28.817Z","comments":true,"path":"2019/11/07/cv-imageFeature-3alignment/","link":"","permalink":"http://holdfire.github.io/2019/11/07/cv-imageFeature-3alignment/","excerpt":"","text":"1. 简介 在keypoingts和descriptors均已生成，特征点已经匹配过后，根据matched特征点对可以做图片对齐 2. RANSAC算法 RANSAC算法（Random sample consensus:RANSAC，也叫随机抽样一致算法）用来筛选匹配点，从而计算单应性矩阵。该算法能够有效地去除误差很大的点，并且这些点不计入模型的计算之中。 RANSAC算法经常用于计算机视觉，例如同时求解相关问题与估计立体摄像机的基础矩阵，在图像拼接时求变换矩阵的时候。在SLAM中经常被用于虑除误匹配。 2.1 算法步骤 在数据误差比较小的情况下，可以使用最小二乘法。但针对噪声很大的数据集的时候，容易出问题。算法的框架如下： 在数据中随机的选择几个点设定为内群（也就是用来计算模型的点） 用这些选择的数据计算出一个模型 把其它刚才没选到的点带入刚才建立的模型中，计算是否为内群（也就是看这些点是否符合模型，如果符合模型的话，将新的点也计入内群） 如果此时的内群数量足够多的话，可以认为这个模型还算OK，那么就用现在的内群数据重新计算一个稍微好些的模型。 重复以上步骤，最后，我们保留那个内群数量最多的模型。 在得到了众多的匹配点以后，使用RANSAC算法，每次从中筛选四个随机的点，然后求得H矩阵，不断的迭代，直到求得最优的Homography矩阵为止。 2.2 单应性矩阵Homography 采用RANSAC算法寻找一个最佳单应性矩阵H，矩阵大小为3×3。RANSAC目的是找到最优的参数矩阵使得满足该矩阵的数据点个数最多，通常令h33=1h33=1来归一化矩阵。由于单应性矩阵有8个未知参数，至少需要8个线性方程求解，对应到点位置信息上，一组点对可以列出两个方程，则至少包含4组匹配点对。 2.3 RANSAC算法优缺点： RANSAC的优点是它能鲁棒的估计模型参数。例如，它能从包含大量局外点的数据集中估计出高精度的参数。RANSAC的缺点是它计算参数的迭代次数没有上限；如果设置迭代次数的上限，得到的结果可能不是最优的结果，甚至可能得到错误的结果。 RANSAC只有一定的概率得到可信的模型，概率与迭代次数成正比。RANSAC的另一个缺点是它要求设置跟问题相关的阀值。 RANSAC只能从特定的数据集中估计出一个模型，如果存在两个（或多个）模型，RANSAC不能找到别的模型。 3. GMS算法 GMS（CVPR2017论文：Grid-based Motion Statisticsfor Fast, Ultra-robust Feature Correspondence ）的方法实际上是消除错误匹配的一种方案，比如可以替换ransac。算法执行的大致流程是：先执行任意一种特征点的检测和特征点的描述子计算，论文中采用的是ORB特征。然后执行暴力匹配BF，最后执行GMS以消除错误匹配。 opencv的ransac非常耗时，这个GMS则非常快，比opencv的ransac快好几倍。在同样特征点执行错误消除的时候要比openCV的ransac快。实际上ransac可以优化到非常快，至少可以比openCV的ransac要快10倍以上。在同样特征点个数的情况下，用ORB+BF+GMS 的时间 小于 SIFT + RANSAC的时间。 算法特点： 该论文认为运动的平滑性导致了匹配的特征点周围的区域有较多匹配的点，因此可以通过计数周围区域的匹配点个数来判断一个匹配正确与否。之前的特征匹配的论文多认为匹配的质量受特征不变性和区别是否明显的影响，本文从一个新的角度来分析，认为原始特征的数量也能够影响匹配的质量。而找到更多特征显然比设计全新的匹配器更加简单，GMS就是这样一个简捷的解决方法，综合来看论文的贡献在于：将运动平滑性转化为统计问题以判别错误的匹配，使得困难场景上的匹配更高效。一个有效的基于网格的打分器，能够与实时的特征匹配器结合证明了GMS系统具有更好的效果(SIFT,SURF,LIFT) 参考资料： 使用Python中的SIFT算法进行图像校准 https://www.jianshu.com/p/f1b97dacc501 RANSAC算法理解1： https://blog.csdn.net/robinhjwy/article/details/79174914 RANSAC算法理解2： https://blog.csdn.net/fandq1223/article/details/53175964 GMS论文解读： https://blog.csdn.net/kevin_zhao_zl/article/details/89810718 GMS算法特点介绍：https://zhuanlan.zhihu.com/p/27131143","categories":[],"tags":[{"name":"图像特征","slug":"图像特征","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81/"},{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"图像特征——基于描述子的图像匹配","slug":"cv-imageFeature-2matcht","date":"2019-11-07T11:41:06.000Z","updated":"2019-11-07T11:54:23.439Z","comments":true,"path":"2019/11/07/cv-imageFeature-2matcht/","link":"","permalink":"http://holdfire.github.io/2019/11/07/cv-imageFeature-2matcht/","excerpt":"","text":"1. 简介 在keypoingts和descriptors均已生成的前提下，opencv的匹配步骤： （1）生成匹配器matcher：DescriptorMatcher，BFMatcher，FlannMatcher； （2）选择匹配函数类型：match，knnMatch，radiusMatch； （3）上述匹配函数存在的重载函数，来实现一对一或者一对多 特征描述是实现图像匹配与图像搜索必不可少的步骤，比较有代表性的特征描述子是浮点型特征描述子和二进帽字符串特征描述子。像SIFT与SURF算法用梯度统计直方图来描述的描述子都属于浮点型特征描述子。但它们计算起来，算法复杂，效率较低，所以后来就出现了许多新型的特征描述算法，如BRIEF。后来很多二进制串描述子ORB，BRISK，FREAK等都是在它上面的基础上的改进。 2. 暴力匹配算法Brute Force 以二进制串特征描述子匹配为例，ORB算法最大的特点就是计算速度快 。 这首先得益于使用FAST检测特征点，FAST的检测速度正如它的名字一样是出了名的快。再次是使用BRIEF算法计算描述子，该描述子特有的2进制串的表现形式不仅节约了存储空间，而且大大缩短了匹配的时间。 例如特征点A、B的描述子如下。 A：10101011 B：10101010 我们设定一个阈值，比如80%。当A和B的描述子的相似度大于90%时，我们判断A,B是相同的特征点，即这2个点匹配成功。在这个例子中A,B只有最后一位不同，相似度为87.5%，大于80%。则A和B是匹配的。 我们将A和B进行异或操作就可以轻松计算出A和B的相似度。而异或操作可以借组硬件完成，具有很高的效率，加快了匹配的速度。 3. 聚类算法 4. flann算法","categories":[],"tags":[{"name":"图像特征","slug":"图像特征","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81/"},{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"自动机器学习AutoML和神经架构搜索NAS（1）","slug":"dl-advanced-NASandAutoML1","date":"2019-11-07T02:02:55.000Z","updated":"2019-11-07T03:34:03.422Z","comments":true,"path":"2019/11/07/dl-advanced-NASandAutoML1/","link":"","permalink":"http://holdfire.github.io/2019/11/07/dl-advanced-NASandAutoML1/","excerpt":"","text":"1. 简介 神经架构搜索(Neural Architecture Search, NAS)是一种搜索出最优神经网络架构的算法。它的工作流程通常从定义一组神经网络可能会用到的“建筑模块”开始，比如2018年Google Brain关于NASNet的论文，就为图像识别网络总结了多种常用的卷积池和池化模块。论文：Learning Transferable Architectures for Scalable Image Recognition NAS算法用一个循环神经网络(RNN)作为控制器，从这些模块中挑选，然后将它们放在一起，来创造某种端到端地架构。然后训练这个新网络直至收敛，得到在验证集上的准确率，这个准确率随后会用来通过策略梯度更新控制器，让控制器生成架构的水平越来越高。 2. 架构搜索的进展 渐进式神经架构搜索PNAS 论文：Progressive Neural Architecture Search 高效神经架构搜索ENAS 论文：Efficient Neural Architecture Search via Parameter Sharing 架构搜索的开源项目和商业产品 开源项目 商业产品 Google提供的Cloud AutoML 参考资料： 一文看懂AutoML和NAS：https://zhuanlan.zhihu.com/p/42924585 AutoML和NAS的调研（CVPR2019）:https://blog.csdn.net/soulmeetliang/article/details/93002244","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习前沿","slug":"深度学习前沿","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%89%8D%E6%B2%BF/"}]},{"title":"自动机器学习AutoML和神经架构搜索NAS（2）","slug":"dl-advanced-NASandAutoML2","date":"2019-11-07T02:02:55.000Z","updated":"2019-11-07T03:29:31.705Z","comments":true,"path":"2019/11/07/dl-advanced-NASandAutoML2/","link":"","permalink":"http://holdfire.github.io/2019/11/07/dl-advanced-NASandAutoML2/","excerpt":"","text":"1. 简介 2.NAS算法搜索图像分类backbone 3. NAS算法搜索物体检测backbone 2019 NeurIPS: 旷视研究院提出DetNAS 这是首个用于设计更好的物体检测器Backbone的神经网络搜索方法，由DetNet搜索出的框架在COCO上的性能超越了ResNet-50和ResNet-101，且模型计算量更低。 论文：DetNAS:Backbone Search for Object Detection","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习前沿","slug":"深度学习前沿","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%89%8D%E6%B2%BF/"}]},{"title":"图像检索——特征编码与聚合","slug":"cv-imageRetrieval-2encodeFeatures","date":"2019-11-06T12:23:06.000Z","updated":"2019-11-07T11:36:16.177Z","comments":true,"path":"2019/11/06/cv-imageRetrieval-2encodeFeatures/","link":"","permalink":"http://holdfire.github.io/2019/11/06/cv-imageRetrieval-2encodeFeatures/","excerpt":"","text":"1. 简介 2. BoW词袋模型 BOF算法(Bag of Features)是首先将所有图像的所有特征点进行聚类，然后根据聚类中心对检索库的每张图片做了一个编码。然后将query的图像和图像库中中每幅图的BOF向量求夹角，最小的即为匹配对象。BOF算法解决的是：每张图片的特征点太多，两张图片做起匹配来花费时间太长这一问题。 BOF算法的流程如下所示： （1）所有图像的局部特征提取。 （2）构建视觉词典：使用k-means算法从所有特征点中生成类心。 （3）生成原始的的BOF：判断图像的每个特征点与哪个类心最近，最近则放入该类心，最后将生成一列频数表，即初步的无权BOF。 （4）引入TF-IDF权值：对频数表加上权重，生成最终的bof。（因为每个类心对图像的影响不同。比如超市里条形码中的第一位总是6，它对辨别产品毫无作用，因此权重要减小）。 （5）对query进来的图像也生成带TF-IDF权值的BOF。 （6）将query的Bof向量与图像库中每幅图的Bof向量求夹角，夹角最小的即为匹配对象。 3. VLAD算法 Jégou提出VLAD(vector of locally aggregated descriptors)，其方法是如同BOF先建立出含有k个visual word的codebook，而不同于BOF将一个local descriptor用NN分类到最近的visual word中，VLAD所采用的是计算出local descriptor和每个visual word在每个分量上的差距，将每个分量的差距形成一个新的向量来代表图片。 4. FV算法","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"图像特征——CNN特征","slug":"cv-imageFeature-1cnnFeatures","date":"2019-11-06T12:16:46.000Z","updated":"2019-11-07T11:54:14.904Z","comments":true,"path":"2019/11/06/cv-imageFeature-1cnnFeatures/","link":"","permalink":"http://holdfire.github.io/2019/11/06/cv-imageFeature-1cnnFeatures/","excerpt":"","text":"1. 简介","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像特征","slug":"图像特征","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81/"},{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"图像特征——全局特征描述","slug":"cv-imageFeature-1globalFeatures","date":"2019-11-06T12:05:02.000Z","updated":"2019-11-07T11:54:09.943Z","comments":true,"path":"2019/11/06/cv-imageFeature-1globalFeatures/","link":"","permalink":"http://holdfire.github.io/2019/11/06/cv-imageFeature-1globalFeatures/","excerpt":"","text":"1. 简介 用来描述图像全局特征的指标主要有：颜色特征、形状特征和纹理特征。目前形状特征在图像检索中使用相对较少，本文不予介绍。 2. 颜色特征 颜色特征是统计图片不同颜色的分布来描述整张图片，颜色分布图计算简单，对平移、旋转尺度缩放具有不变性。 灰度分布图损失了颜色信息，RGB分布图和人类对视觉的感知差别较大，也不建议使用。HSV和Lab色彩空间 和人类主观感知更为接近。常用于图像检索的颜色特征包括：直方图、累积直方图、平均灰度级等。其中，基于累积直方图的图像检索性能最优。 3. 纹理特征 全局特征信息又称为Gist信息，为场景的低维签名向量。采用全局特征信息对场景进行识别与分类不需要对图像进行分割和局部特征提取，可以实现快速场景识别与分类。（Gist特征仅指纹理特征吗？） 纹理特征是指物体表面共有的内在特性，其包含了物体表面结构组织排列的重要信息及其与周围物体的联系。当检索在粗细和疏密等方面有较大差别的图像时，利用纹理特征是一种行之有效的方法。有实验结果表明，Gabor小波能够较好地兼顾信号在时域和频域中的分辨能力，是图像检索中的最佳特征之一。 小波变换：2004年，Torralba等采用小波图像分解算法来提取输入图像的全局特征信息。首先将输入图像分解成4×44\\times44×4个小区域子块，然后对每个小区域子块从6个方向和4个尺度采用小波滤波来提取图像的纹理特征信息。每幅图片的Gist信息为对各个小区域模块中滤波后的平均输出，得到384维GIST向量，然后采用PCA算法降维至80维。最后，根据各场景的Gist向量得到训练集Gist向量的最小欧氏距离来确定场景的类别。 Tamura纹理：：Tamura纹理特征包括6个指标：粗糙度(Courseness)、对比度(Contrast)、方向度(Directionality)、线性度(Linelikeness)、规则度(Regularity)、粗略度(Roughness)。一般论文里面只用前三个特征，说前面三个特征是线性无关的，后面三个特征和前面三个特征是线性相关的，因此只采用前三个特征。 Gabor变换：GIST512计算:（1）32个Gabor滤波在4个尺度，8个方向上进行卷积，得到32个个输入图像大小一致的feature map；（2）把每个feature map分成4×44 \\times 44×4共16个区域，计算每个区域内的均值；（3）计算16×3216\\times3216×32个均值的结果，就得到了512维的GIST特征。不同维度的GIST特征在于Gabor滤波器的个数，确切的说是滤波器方向和尺寸的不同。 灰度共生矩阵：灰度共生矩阵(Gray-Leavel Co-currence Matrix, GLCM)是像素灰度在空间位置上的反复出现形成图像的纹理。是描述具有某种空间位置关系两个像素灰度的联合分布，是一种二阶统计量。 纹理谱： 4. 感知哈希算法 基于低频的均值哈希aHash 感知哈希pHash 差异哈希算法dHash 参考资料： HSV色彩空间映射：https://blog.csdn.net/guanjungao/article/details/26617927 三种感知哈希算法：https://blog.csdn.net/weierqiuba/article/details/71305692","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像特征","slug":"图像特征","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81/"},{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"图像特征——局部特征点检测和描述","slug":"cv-imageFeature-1localFeatures","date":"2019-11-06T11:51:49.000Z","updated":"2019-11-07T11:56:10.176Z","comments":true,"path":"2019/11/06/cv-imageFeature-1localFeatures/","link":"","permalink":"http://holdfire.github.io/2019/11/06/cv-imageFeature-1localFeatures/","excerpt":"","text":"1. 简介 局部特征代表了图像的局部特性，一幅图像中旺旺能提取出若干个数量不等的局部特征，这些局部特征组合起来代表了整幅图像的特征分布。 局部特征是从图像局部区域中抽取的特征，包括边缘、角点、线、曲线和特别属性的区域等，常见的局部特征包括角点类和区域类两大类描述方式。 斑点通常是指与周围有着颜色和灰度差别的区域。斑点通常是指与周围有着颜色和灰度差别的区域，在实际地图中如一颗树是一个斑点，一块草地是一个斑点，一栋房子也可以是一个斑点。斑点通常和关键点(key point)，兴趣点(intrest point)以及特征点(feature point)表示同一个概念。 角点可以从两个不同角度定义：角点是两个边缘的交点；角点是领域内具有两个主方向的特征点。角点所在的领域通常也是图像中稳定的，信息丰富的区域，这些领域可能具有某些特性，比如旋转不变性，尺度不变性，仿射不变性和光照亮度不变性。 边缘检测寻找的就是一阶微分的极值点，当然一阶微分的极值点当然是二阶微分的过零点，所以边缘检测有两种常用的方法：一是基于一阶微分的，寻找极值点，如Roberts,Canny,Prewitt等等；另一类是基于二阶微分的，寻找过零点，如LoG等。 openCV中 2. 局部特征点检测keypoints 角点检测： Harris角点： Harris角点检测是一种基于图像灰度的一阶导数矩阵检测方法。检测器的主要思想是局部自相似性/自相关性，即在某个局部窗口内图像块与在各个方向微小移动后的窗口内图像块的相似性。在像素点的邻域内，导数矩阵描述了数据信号的变化情况。假设在像素点邻域内任意方向上移动块区域，若强度发生了剧烈变化，则变化处的像素点为角点。 FAST角点： FAST算法检基于特征点周围的图像灰度值，检测候选特征点周围一圈的像素值，如果候选点周围领域内有足够多的像素点与该候选点的灰度值差别够大，则认为该候选点为一个特征点。候选点周围的圆的选取半径是一个很重要的参数，这里为了简单高效，采用半径为3，共有16个周边像素需要比较。圆周上如果有连续n个像素点的灰度值比P点的灰度值大或者小，则认为P为特征点。一般n设置为12。 斑点检测： DOH斑点： GLOH斑点； LOG斑点： 3. 局部特征描述descriptors 局部纹理描述——针对整张图片： HOG特征： HOG(Histogram of Gradient)计算和统计图像局部区域的梯度直方图来构成描述。（1）将图像灰度化；（2）采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）；目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰；（3）计算图像每个像素的梯度（包括大小和方向）；主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。（4）将图像划分成小cells（例如6×66 \\times 66×6像素/cell）；（5）统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor；（6）将每几个cell组成一个block（例如3*3个cell/block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。（7）将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor了。这个就是最终的可供分类使用的特征向量了。 Haar特征： Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。但矩形特征只对一些简单的图形结构，如边缘、线段较敏感，所以只能描述特定走向（水平、垂直、对角）的结构。 LBP特征： LBP（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。原始的LBP算子定义在一个3×33\\times33×3的窗口内，以窗口中心像素为阈值，与相邻的8个像素的灰度值比较，若周围的像素值大于中心像素值，则该位置被标记为1;，否则标记为0。如此可以得到一个8位二进制数（通常还要转换为10进制，即LBP码，共256种），将这个值作为窗口中心像素点的LBP值，以此来反应这个3×33\\times33×3区域的纹理信息。 局部特征描述子——针对局部特征点： BRIEF算法 ： BRIEF算法（Binary Robust IndependentElementary Features）的主要思想是：在特征点周围邻域内选取若干个像素点对，通过对这些点对的灰度值比较，将比较的结果组合成一个二进制串字符串用来描述特征点。最后，使用汉明距离来计算在特征描述子是否匹配。 4. 局部特征点检测和描述算法 SIFT算法： 尺度不变特征变换算法（Scale-invariant feature transform，SIFT）由Lowe在1999年所发表，2004年完善总结。利用原始图像与高斯核的卷积来建立尺度空间，在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量。其中图像金字塔、计算图像梯度寻找主方向、梯度归一化，分别应对缩放不变、旋转不变、和光照不变，同时局部特征用于模式识别不需要考虑相对平移的影响，对视角变化、仿射变换、噪声也保持一定程度的稳定性。 SURF算法： SURF（Speeded Up Robust Features，加速稳健特征）发表于2006年的ECCV，是对SIFT算法加强版，同时加速的具有鲁棒性的特征。第二、标准的SURF算子比SIFT算子快好几倍，并且在多幅图片下具有更好的鲁棒性。SURF最大的特征在于采用了harr特征以及积分图像integral image的概念，这大大加快了程序的运行速度。 ORB算法： ORB算法的全称是Oriented FAST and Rotated BRIEF，算法分为使用FAST进行特征点检测，然后用BREIF进行特征点的特征描述，但是我们知道BRIEF并没有特征点方向的概念，所以ORB在BRIEF基础上引入了方向的计算方法，并在点对的挑选上使用贪婪搜索算法，挑出了一些区分性强的点对用来描述二进制串。 BRISK算法： BRISK算法主要利用FAST9-16进行特征点检测（为什么是主要？因为用到一次FAST5-8）。BRISK算法在特征点检测部分没有选用FAST特征点检测，而是选用了稳定性更强的AGAST算法。在特征描述子的构建中，BRISK算法通过利用简单的像素灰度值比较，进而得到一个级联的二进制比特串来描述每个特征点，这一点上原理与BRIEF是一致的。BRISK算法里采用了邻域采样模式，即以特征点为圆心，构建多个不同半径的离散化Bresenham同心圆，然后再每一个同心圆上获得具有相同间距的N个采样点。 FREAK算法： FREAK算法的全称是Fast Retina KeyPoint，即快速视网膜关键点。根据视网膜原理进行点对采样，中间密集一些，离中心越远越稀疏。并且由粗到精构建描述子，穷举贪婪搜索找相关性小的。42个感受野，一千对点的组合，找前512个即可。这512个分成4组，前128对相关性更小，可以代表粗的信息，后面越来越精。匹配的时候可以先看前16bytes，即代表精信息的部分，如果距离小于某个阈值，再继续，否则就不用往下看了。 5. 特征描述子的匹配match 参考资料： LBP描述子介绍：https://blog.csdn.net/hongbin_xu/article/details/79924961 SIFT算法简介1： https://www.cnblogs.com/cfantaisie/archive/2011/06/14/2080917.html SIFT算法简介2：https://blog.csdn.net/wishchin/article/details/18319477 SIFT算法详解 https://blog.csdn.net/memray/article/details/39234645 SURF算法详解 https://www.cnblogs.com/gfgwxw/p/9415218.html","categories":[],"tags":[{"name":"图像特征","slug":"图像特征","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81/"},{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"论文阅读笔记——图像检索","slug":"papers-imageRetrieval01","date":"2019-11-06T11:11:07.000Z","updated":"2019-11-06T14:35:05.648Z","comments":true,"path":"2019/11/06/papers-imageRetrieval01/","link":"","permalink":"http://holdfire.github.io/2019/11/06/papers-imageRetrieval01/","excerpt":"","text":"1. 基于SIFT特征的图像检索的改进 作者的highlights在于：（1）使用SIFT特征的关键点匹配两张图片时，将matched keypoints的数目M和距离d结合起来评估相似度得到评分E1E_1E1​；（2）将queried images分成多张sub-image后，也分别进行检索评估相似度得到评分E2E_2E2​，然后将E1E_1E1​和E2E_2E2​加权平均得到最终的相似度评分结果。 论文链接：Li B , Kong X , Wang Z , et al. SIFT-Based Image Retrieval Combining the Distance Measure of Global Image and Sub-Image[C]// Fifth International Conference on Intelligent Information Hiding &amp; Multimedia Signal Processing. IEEE Computer Society, 2009 `","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"},{"name":"论文笔记","slug":"论文笔记","permalink":"http://holdfire.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}]},{"title":"图像检索——综述","slug":"cv-imageRetrieval-0","date":"2019-11-06T08:14:50.000Z","updated":"2019-11-06T12:26:19.890Z","comments":true,"path":"2019/11/06/cv-imageRetrieval-0/","link":"","permalink":"http://holdfire.github.io/2019/11/06/cv-imageRetrieval-0/","excerpt":"","text":"1. 简介 图片库Lib中共有n张图片，现有待检索图片q，我们想在Lib中查找和图片q最相似的图片y。 2. 基于内容的图像检索CBIR 2.1 基于局部特征 可选用的局部特征点包括角点和斑点两类，如：SIFT特征，SURF特征，ORB特征。流程如下： 1234（1）对图片库Lib中图片，逐张提取局部特征点后生成描述子，将所有图片的描述子Desc保存在本地；（2）对待检索图片q提取局部特征并生成描述子desc_q；（3）将desc_q和图片库中图片的描述子进行match操作（判断是matched keypoints的规则见参考文献1），用matched kepoints的数目（设置一个阈值）及其距离来衡量其相似程度；（4）在图片库中所有图片进行查找（暴力查找或最近邻查找，如opencv提供的flann），根据相似度评分找到最优结果。 2.2 基于全局特征 2.3 基于CNN特征 3.检索算法 3.1 树形结构检索 3.2 基于哈希的方法 3.3 基于向量量化的方法 参考资料： SIFT特征做图像检索论文：SIFT-Based Image Retrieval Combining the Distance Measure of Global Image and Sub-Image","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"deep-learning-introduction","slug":"dl-base-introduction","date":"2019-11-06T05:12:36.000Z","updated":"2019-11-07T02:05:10.031Z","comments":true,"path":"2019/11/06/dl-base-introduction/","link":"","permalink":"http://holdfire.github.io/2019/11/06/dl-base-introduction/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"目标检测——YOLO系列算法","slug":"cV-objectDetection-yolov3","date":"2019-11-05T15:45:39.000Z","updated":"2019-11-06T11:57:05.166Z","comments":true,"path":"2019/11/05/cV-objectDetection-yolov3/","link":"","permalink":"http://holdfire.github.io/2019/11/05/cV-objectDetection-yolov3/","excerpt":"","text":"1.简介 5. 对YOLOv3做出的改进 5.1 模型压缩之YOLOv3-Tiny 5.2 剪枝 在oxford hand数据集上对YOLOv3做模型剪枝，github项目地址对 YOLOv3 进行 channel pruning 之后，模型的参数量、模型大小减少 80% ，FLOPs 降低 70%，前向推断的速度可以达到原来的 200%，同时可以保持 mAP 基本不变。 剪枝算法基于论文Learning Efficient Convolutional Networks Through Network Slimming (ICCV 2017) http://openaccess.thecvf.com/content_iccv_2017/html/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.html","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://holdfire.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"文字识别技术","slug":"cv-fields-OCR","date":"2019-11-05T13:53:30.000Z","updated":"2019-11-06T11:53:00.565Z","comments":true,"path":"2019/11/05/cv-fields-OCR/","link":"","permalink":"http://holdfire.github.io/2019/11/05/cv-fields-OCR/","excerpt":"","text":"1. 简介 自然图像文本识别一直被认为是两个连续但独立的问题：文字检测和识别问题。当前最好的文字监测方法都是从目标检测或分割框架拓展而来。基于文字检测的结果，文字识别从被抠出的文本图片中识别一连串字符。一般来说，文字识别被转换为一个序列标记问题，通常用基于卷积神经网络(CNN)特征的递归神经网络(RNN)来解决。然而样两阶段的方法存在一些限制：（1）独立学习两个任务很难利用到文本本身的性质，比如联合的文字检测和识别可以提供丰富的上下文信息，并且两个任务可以实现互补；（2）两阶段的方法通常需要多个连续的步骤，整个系统变复杂。 有些方法尝试开发一个统一的文字检测与识别框架（也叫端到端识别），如通过添加一个RNN的分支到文字检测模型来实现端到端识别。但它们本质上还是属于两阶段的框架。其缺点是：（1）使用RNN序列去识别文字，基于RNN的模型是比较难以与CNN文本监测模型进行联合优化的；（2）这些两阶段框架都需要RoI Pooling，因此很难精确地抠出准确的文本区域，通常会有很多背景被包含在抠出来的特征中，这不可避免的限制了文字识别的性能，尤其是对于多方向或弯曲文本。 2. 自然场景下文字检测与识别 ICCV2019论文：Convolutional Character Networks 码隆科技提出的一种单阶段模型——卷积字符网络CharNet，能够端到端地同时解决文字检测和识别问题。其主要贡献为：（1）利用单个字符作为基本元素，巧妙的避免了需要同时优化CNN文本检测和RNN文本识别模型的困难与限制；（2）针对字符标注难以获取的问题，提出一个可以利用合成数据训练字符检测器的方法。该方法可以更好地将模型在合成数据上学到的知识迁移到真实数据上。 3. 打印文档文字检测与识别 4. 手写体文字检测与识别 参考资料：","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"计算机视觉方向","slug":"计算机视觉方向","permalink":"http://holdfire.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%96%B9%E5%90%91/"}]},{"title":"最近邻搜索NN——哈希散列方法","slug":"cv-imageRetrieval-3NN-lsh","date":"2019-11-04T09:36:20.000Z","updated":"2019-11-06T12:26:29.572Z","comments":true,"path":"2019/11/04/cv-imageRetrieval-3NN-lsh/","link":"","permalink":"http://holdfire.github.io/2019/11/04/cv-imageRetrieval-3NN-lsh/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"最近邻搜索NN——向量量化","slug":"cv-imageRetrieval-3NN-vectorQuantization","date":"2019-11-04T09:20:06.000Z","updated":"2019-11-06T12:26:37.487Z","comments":true,"path":"2019/11/04/cv-imageRetrieval-3NN-vectorQuantization/","link":"","permalink":"http://holdfire.github.io/2019/11/04/cv-imageRetrieval-3NN-vectorQuantization/","excerpt":"","text":"1. 简介 向量量化：把原空间分解为低维子空间的Cartesian乘积后，在每个子空间通过聚类把向量聚集成若干类，每类里面的向量用对应的类中心来近似。 这样每个向量只需要用其对应的聚类中心的索引ID来表示，其余查询向量间的距离用其对应的聚类中心与查询向量间的距离来近似。向量量化的优点为： （1）向量需要的存储空间变少了，只需保留对应的聚类中心的ID； （2）计算时间减少了，只需要通过聚类中心的索引ID来查询预先计算好的聚类中心与查询向量的距离表格； 然而，直接用k-means算法并不能带来明显的效果：如果聚类中心数目太少，向量近似效果不佳；而如果聚类中心数目太多，距离表格计算时间会太长。用Cartesian量化来解决此问题，Cartesian量化的聚类中心C是建立在几个小的聚类中心集合{C1,C1,...CP}\\{C_1,C_1,...C_P\\}{C1​,C1​,...CP​}的基础上：C=C1×C2×...×CPC = C_1 \\times C_2 \\times ...\\times C_PC=C1​×C2​×...×CP​。其好处在于通过几个小的聚类中心集合，可以得到非常多的聚类中心，甚至多于搜索的向量集合大小，从而向量近似效果可以得到保证。进一步通过引进一些限制或者策略来保证距离计算可以通过快速查找距离表格来实现，以降低计算时间。 2. 乘积量化PQ 乘积量化(Productive Quantization ,PQ)把向量分成若干个子向量然后对每个子向量分别进行聚类。具体来讲，向量xxx分成子向量{x1,x2,...xP}:x=[x1T x2T ... xPT]T\\{x_1,x_2,...x_P\\}:x = [x_1^T\\ x_2^T\\ ...\\ x_P^T]^T{x1​,x2​,...xP​}:x=[x1T​ x2T​ ... xPT​]T。然后对每个子向量的集合{x1,x2,...xP}\\{ x_1,x_2,...x_P\\}{x1​,x2​,...xP​}进行聚类，得到K（为了方便，每个集合都生成相同数量的聚类中心）个聚类中心：CP={cp1,cp2,...cpk}C_P=\\{ c_p1,c_p2,...c_pk\\}CP​={cp​1,cp​2,...cp​k}。最终，乘积量化得到KPK^PKP聚类中心{[c(1k1)T,c(2k2)T,...,c(PkP)T]T;k1∈{0,1,...,K},k2∈{0,1,...,K},kP∈{0,1,...,K}}\\{ [c_{(1k_1)}^T, c_{(2k_2)}^T, ..., c_{(Pk_P)}^T ]^T;k_1\\in\\{0,1,...,K\\},k_2\\in\\{ 0,1,...,K\\},k_P\\in\\{ 0,1,...,K\\} \\}{[c(1k1​)T​,c(2k2​)T​,...,c(PkP​)T​]T;k1​∈{0,1,...,K},k2​∈{0,1,...,K},kP​∈{0,1,...,K}}。在最近邻搜索中，乘积量化每个向量xxx由其PPP个子向量对应的聚类中心的ID来表达：(k1,k2,...,kP)(k_1, k_2,...,k_P)(k1​,k2​,...,kP​)。查询向量qqq与参考向量xxx距离有对称和非对称两种近似方法。 对称的方法： 线下计算P个大小K×KK \\times KK×K的距离表格，每个表格DpD_pDp​存储对应的聚类中心两两之间的距离Dp(i,j)=dist(cpi,cpj)D_p(i,j) = dist(c_pi,c_pj)Dp​(i,j)=dist(cp​i,cp​j)。查询向量分成PPP个子向量，每个子向量找到对应的聚类中心，查询向量由聚类中心的ID表示：((k1)′′,(k2)′′,...,(kP)′′)((k_1)&#x27;&#x27;,(k_2)&#x27;&#x27;,...,(k_P)&#x27;&#x27;)((k1​)′′,(k2​)′′,...,(kP​)′′)。这样子，距离近似为dist(p,x)≈∑p=1PDp((kp)′′,kp)dist(p,x) \\approx \\sum_{p=1}^P D_p((k_p)&#x27;&#x27;, k_p)dist(p,x)≈∑p=1P​Dp​((kp​)′′,kp​)。 非对称的方法： 线上预先计算一个大小为P×KP \\times KP×K的距离表格，把查询向量q分成P个子向量q1,q2,...,qPq_1, q_2,...,q_Pq1​,q2​,...,qP​，然后计算这P个子向量与P组聚类中心的距离，D(p,j)=dist(qp,cpj)D(p,j)=dist(q_p, c_pj)D(p,j)=dist(qp​,cp​j)。这样，距离近似为dist(p,x)≈∑p=1PD(p,kp)dist(p,x) \\approx \\sum_{p=1}^P D(p, k_p)dist(p,x)≈∑p=1P​D(p,kp​)。很显然，后者距离近似得更为准确，但是需要额外距离表格的计算，在搜索数据库非常大的情况下，额外距离表格计算的时间可以忽略不计。 乘积量化需要把向量分成PPP个子向量，由于各个子向量的分布不一样，每个子向量的量化性能不平衡，会导致距离近似不够理想。由Mhammad Norouzi等人提出的Cartesian k-means方法以及由Tiezheng Ge等人提出的优化的的乘积向量方法首先旋转向量，然后在旋转后的空间里进行乘积量化。这里面旋转的目的是旋转后的每个子向量的量化性能尽量平衡。旋转具有不改变欧氏距离的性质，这是可以把旋转引进来的原因。 3. 合成量化CQ 4. 加和量化AQ 参考资料： Hervé Jégou, Douze M , Schmid C . Product Quantization for Nearest Neighbor Search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 33(1):117-128. 微软研究院AI头条——最近邻搜索综述:https://blog.csdn.net/Y2c8YpZC15p/article/details/86326313 实例理解乘积向量算法：http://fabwrite.com/productquantization 理解product quantization算法 Product Quantizers for k-NN Tutorial Part 1 Optimized Product Quantization","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"最近邻搜索NN——综述","slug":"cv-imageRetrieval-3NN","date":"2019-11-04T07:39:19.000Z","updated":"2019-11-06T12:26:42.421Z","comments":true,"path":"2019/11/04/cv-imageRetrieval-3NN/","link":"","permalink":"http://holdfire.github.io/2019/11/04/cv-imageRetrieval-3NN/","excerpt":"","text":"1. 简介 最近邻搜索的数学描述为：在DDD维欧式空间RD\\mathbb{R}^DRD中，给定查询向量q∈RDq \\in \\mathbb{R}^Dq∈RD，我们想要从有n个向量的有限集合X⊂RD\\mathcal{X} \\subset \\mathbb{R}^DX⊂RD中,找到和qqq距离最小的元素NN(q)NN(q)NN(q)。 NN(q)=arg min⁡x∈X dist(q,x)NN(q) = arg \\ \\min_{x \\in\\mathcal{X}}\\ dist(q,x) NN(q)=arg x∈Xmin​ dist(q,x) 通过线性查找，计算查询向量和数据库中各个向量的距离。但如果是海量的高维数据集，采用线性查找会非常耗时。 为了解决这个问题，我们需要采用一些类似索引的技术来加速查找的过程，这类方法包括：最近邻检索(Nearest Neighbor Search, NN)和近似最近邻检索(Approximate Nearest Neighbor Search, ANN)。主要有以下两种思路：一种是基于提升检索结构性能的方法，大多基于树形结构。另一种是基于对数据本身的处理，包括哈希算法，向量量化等方法。 近似最近邻搜索方法ANN通过对数据分析聚类的方法，对数据库中的数据进行分类或编码。对于目标数据，根据其数据特诊预测其所属的数据类别，返回类别中的部分或全部作为检索结果。主要有两类方案，第一类是缩短距离计算时间，例如将维度d由1000将到100，这类方法主要包括哈希散列和矢量量化。第二类方法是通过减少距离的计算次数来实现的。例如将计算次数由1,000,000次减少到1,000次。本文暂不关注。 1.1 基于提升检索结构性能的方法 随机k-d树算法：对数据的处理，减少距离计算的次数 优先搜索k-means树算法 层次聚类树算法 1.2 对数据的处理，缩短距离计算的时间——&gt;哈希散列 哈希散列是通过哈希函数把向量q变化为二值码，然后用Hamming距离来近似表示原来两个向量的距离。 核心思想是：在高维空间相邻的数据，经过哈希函数的映射投影转化到低维空间后，它们落入同一个吊桶的概率很大，而不相邻的数据映射到同一个吊桶的概率很小。在检索时将欧式空间的距离计算转化到汉明（Hamming）空间，并将全局检索转化为对映射到同一个吊桶中的数据进行检索，从而提高了检索速度。这种方法的主要难点在于如何寻找适合的哈希函数。 1.3 对数据的处理，缩短距离计算的时间——&gt;向量量化 向量量化是通过聚类把向量集聚成若干类，每类里面的向量用对应的类中心来近似。这样子，每个向量只需要用其对应的聚类中心的索引ID来表示，其与查询向量间的距离用其对应的聚类中心与查询向量间的距离来近似。向量量化带来了两项优势：向量需要的存储空间变少了，只需保存对应的聚类中心的ID；计算时间减少了，只需要通过聚类中心的索引ID来查询预先计算好的聚类中心与查询向量的距离表格。 2. 基于提升检索结构性能的方法 1977年，Friedman et al.提出了k-d树，这种结构后来被用于加速精确查找。 random multiple k-d trees和priority search 相比较普通k-d树，提升了搜索的准确率和搜索效率。相关文章：Silpa-Anan &amp; Hartley. Optimised kd-trees for fast image descriptor matching. In CVPR, 2008. FLANN方法 该方法在random k-d trees和hierarchial k-means trees之间进行很好地配置 。相关文章：Muja &amp; Lowe. Fast approximate nearest neighbors with automatic algorithm configuration. In VISS-APP(1),.pp.331-340,2009. over tree结构 相关文章：Beygelzimer et al., Cover trees for nearest neighbor. In ICML, pp.97-104,2006. trinary tree结构 相关文章:Jia et al., 2010. Optimised kd-trees for scable visual descriptor indexing. In CVPR,PP 3392-3399. Wang et al.,2014. Trinary-projection trees for approximate nearest neighbor search. IEEE Trans.Pattern Ananl.Mach.Intell. 基于近邻图的最近邻搜索算法 相关文献：Arya &amp; Mount, 1993. Approximate nearest neighbor queries in fixed dimension. In SODA. Wang &amp; Li, 2012. Query-driven iterated neighborhood graph search for large scale indexing. Wang et al., 2013. Fast neighborhood graph search using cartesian concatenation. In ICCV. 3. 基于哈希散列的方法 这类方法将数据库中的向量转换为更短的编码，从而占用的存储空间更小，距离计算的时间也更短。 局部敏感哈希方法的发展 1999年，Gionis提出局部敏感哈希方法LSH 相关文章：Gionis. 1999. Similarity search in high dimensions via hashing. LSH的基础上Mahalanobis distance， 相关文章：Jain et al. 2008. Fast image search for learned metrics. In CVPR. LSH基础上kernalization， 相关文章：Kulis &amp; Grauman. 2009. Kernalized locality-sensitive hashing for scalable image search. In NIPS. comlementary hashing， 相关文章：Xu et al. 2011. Comlementary hashing for approximate nearest neighbor search. In ICCV 设计哈希函数 语义哈希(semantic hashing) 相关文章：Salakhutdinov &amp; Hinton. 2009. Semantic hashing. Int.J.Approx.Reasoning. shift kernel hashing， 相关文章：Raginsky &amp; Lazebnik. 2009. Local sensitive binary codes from shift-invariant kernels. In NIPS. isotropic hashing， 相关文章：[Kong &amp; Li. 2012.Isotropic hashing. In NIPS. 设计保相似度的哈希函数 谱哈希(Spectral hashing) 这种方法的出发点是希望Hamming距离大的两个数据点在原空间的相似度要小，其目标函数为最小化Hamming距离和原空间相似度的乘积，最后转化为解特征值或特征函数问题。相关文章：Weiss et al. 2008. Spectral hashing. In NIPS. 二值化重建嵌入(binary reconstructive embedding) 其目标函数是最小化距离重建误差，即希望Hamming距离和原空间里的欧氏距离尽量接近。相关文章：Kulis &amp; Darrells. 2009. Learning to hash with binary reconstructive embeddings. In NIPS. 基于图的哈希(graph-based hashing) 锚点图哈希(Anchor Graph Hashing, AGH)。相关文章：Liu Wei et al. 2012.Hashing with graphs. In ICML. 半监督哈希(Semi-Supervised Hashing) 相关文章：Wang Jun et al., 2012. Semi-supervised hashing for large scale search. 设计保序的目标函数 三元组损失函数是一种最简单的保序函数 设计保序的目标函数，使二值空间的序跟原空间的序尽量一致。将搜索问题看成排序问题，找到距离查询点近的向量。如果一个点q与一个点p1p_1p1​的距离比q到点q2q_2q2​的距离小，那么在二值空间里，点q与点p1p_1p1​的Hamming距离比q到点q2q_2q2​的Hamming距离小也要。相关文章：Mohanmmad Norouzi, Fleet. 2011. Minimal loss hashing for compact binary codes.In ICML. 更高阶的保序目标函数 相关文章：Wang et al.,2013. Order preserving hashing for approximate nearest neighbor search.In ACM Multimedia. 迭代量化ITQ 迭代量化(Iterative Quntization, ITQ)的方法 其出发点不同于保相似度、保距离或者保序，而是把二值编码当成原向量的近似，利用欧氏距离旋转不变性的性质，建立了最小化二值编码重建旋转原向量误差的目标函数，寻找最优的旋转变换和二值编码。尽管直观看上去重建向量的方法比保相似、保距离或者保序的方法简单，近似得更强，但ITQ实际上效果还是很不错的,原因是保相似、保距离或者保序需要建立二元或多元关系，计算复杂度很大，因而需要各种近似，使得最后的效果不如预期。相关文章：Gong &amp; Lazebnik. Iterative quantization: A procrustean approach to learning bianry codes. In CVPR,PP.817-824,2011. 4. 向量量化的方法 乘积量化(Productive Quantization, PQ) 乘积量化是信号处理上用到的一种数据压缩技术。相关文章：Hervé Jégou, Douze M , Schmid C . Product Quantization for Nearest Neighbor Search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 33(1):117-128. 合成量化(Compositive Quantization,CQ)的方法 相关文章：Zhang Ting, Du Chao, Wang Jingdong.Composite quantization for approximative nearest neighbor search.ICML 2014: 838-846. 以及：Wang Jingdong, Zhang Ting. Composition Quantization. IEEE Transactions on Pattern Analysis and Machine Intelligence.2018 加和量化(Additive Quantization, AQ) 参考资料： 微软研究院AI头条——最近邻搜索综述:https://blog.csdn.net/Y2c8YpZC15p/article/details/86326313","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像检索","slug":"图像检索","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"}]},{"title":"学习方法——如何阅读论文","slug":"lx-howToReadPapers","date":"2019-11-03T19:17:54.000Z","updated":"2019-11-06T14:37:18.656Z","comments":true,"path":"2019/11/04/lx-howToReadPapers/","link":"","permalink":"http://holdfire.github.io/2019/11/04/lx-howToReadPapers/","excerpt":"","text":"1.论文的阅读顺序 第一阶段：判断论文是否值得读 阅读顺序为：读标题和关键词–&gt;读摘要–&gt;读结论 掌握论文的大概内容。 第二阶段：读论文 阅读顺序为：读图表和数据–&gt;读引言introduction 如果想要深挖细节：读结果和讨论–&gt;读实验部分 第三阶段：做笔记 好笔记的标准：下次看笔记内容即可，无需再看这篇论文 2.阅读时应该做的思考和笔记 一篇论文的核心往往就一两句话，而论文的其他部分都是在用不同的方式支持它的核心。 摘要Abstract 摘要是对全篇内容的概括，阅读后需要做的笔记： 作者想要解决什么问题？question 作者通过什么方法/模型解决了这个问题？method 作者给出的答案是什么？result 引言introduction 引言介绍了文章的研究背景和研究意义，一般在最后会给出文章的结构。需要做的笔记： 作者为什么要研究这个问题？ 这个问题现在的研究进展到了哪个地步？ 作者使用的理论是基于哪些假设？ 结论conclusion 结论部分会把整篇文章的主要内容复述一遍，帮助读者回顾+理清思路，然后在此基础上加深自己的研究。需要做的笔记： 这篇文章存在哪些缺陷？ 作者关于这个课题的构思有哪几点？ 参考资料： 明尼苏达大学Peter W. Carr教授传授的阅读顺序：https://video.zhihu.com/video/1172537839240863744 知乎大佬的回答：https://www.zhihu.com/question/345516318/answer/863530375","categories":[{"name":"个人杂记","slug":"个人杂记","permalink":"http://holdfire.github.io/categories/%E4%B8%AA%E4%BA%BA%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"学习方法","slug":"学习方法","permalink":"http://holdfire.github.io/tags/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"论文笔记","slug":"论文笔记","permalink":"http://holdfire.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}]},{"title":"机器学习——正则化","slug":"ml-regularization","date":"2019-11-03T18:41:48.000Z","updated":"2019-11-06T12:00:43.453Z","comments":true,"path":"2019/11/04/ml-regularization/","link":"","permalink":"http://holdfire.github.io/2019/11/04/ml-regularization/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"机器学习——标准化和归一化","slug":"ml-standardizationNormalization","date":"2019-11-03T18:28:16.000Z","updated":"2019-11-06T12:00:55.837Z","comments":true,"path":"2019/11/04/ml-standardizationNormalization/","link":"","permalink":"http://holdfire.github.io/2019/11/04/ml-standardizationNormalization/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——GBDT","slug":"ml-GBDT","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-06T12:00:35.263Z","comments":true,"path":"2019/11/04/ml-GBDT/","link":"","permalink":"http://holdfire.github.io/2019/11/04/ml-GBDT/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——AdaBoost","slug":"ml-adaBoost","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-06T12:00:14.436Z","comments":true,"path":"2019/11/04/ml-adaBoost/","link":"","permalink":"http://holdfire.github.io/2019/11/04/ml-adaBoost/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——XgBoost","slug":"ml-XgBoost","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-06T12:01:04.599Z","comments":true,"path":"2019/11/04/ml-XgBoost/","link":"","permalink":"http://holdfire.github.io/2019/11/04/ml-XgBoost/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"集成学习——简介","slug":"ml-ensembleLearning","date":"2019-11-03T18:05:04.000Z","updated":"2019-11-06T12:00:24.989Z","comments":true,"path":"2019/11/04/ml-ensembleLearning/","link":"","permalink":"http://holdfire.github.io/2019/11/04/ml-ensembleLearning/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"集成学习","slug":"集成学习","permalink":"http://holdfire.github.io/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算机视觉方向——介绍","slug":"cv-fields","date":"2019-11-03T13:40:07.000Z","updated":"2019-11-07T02:45:41.258Z","comments":true,"path":"2019/11/03/cv-fields/","link":"","permalink":"http://holdfire.github.io/2019/11/03/cv-fields/","excerpt":"","text":"1. 简介 计算机视觉包含图像处理、图像分析和图像理解三个部分。 2. 计算机视觉基础任务 2.1 图像处理image process 图像处理是对图像进行去噪、增强、复原、分割、特征提取、识别、等操作的理论、方法和技术。 2.2 图像分析image analysis 2.3 图像理解image understanding 图像理解可以认为是一种动态的目标检测，由全局信息生成图像摘要。 特征检测和匹配(feature detection and matching) 基于特征的图像对齐(feature based alignment):姿态估计(pose estimation) 图像分割(segmentation)：图像语义分割(image semantic segmentation) 图像分类(image classification) 目标检测(object detection) 图像描述(image caption) 图像风格转换() 图像生成(image generation):如变分自编码器VAE，生成对抗网络GAN 度量学习(metric learning) 3. 计算机视觉方向 3.1 图像相关 图像检索：基于内容的图像检索CBIR，实例图像检索(instance image retrieval) 人脸识别：人脸检测，人脸识别，人脸活体检测算法，人脸表情识别，人脸颜值打分 行人属性识别：Re ID 人体骨骼关键点检测：人体姿态估计(Pose Estimation)，行为识别(open pose开源框架) 3.2 视频或多幅图像 视频目标跟踪：单目标跟踪，多目标跟踪，行人再追踪 视频显著物体检测：Video Salient Object Detection 视频对象提取： 视频分类：video classification 3.3 文字、语言相关 文字检测与识别：文档文字检测与识别，场景文字检测与识别，手写体文本检测与识别 基于深度学习的表格提取： 唇语识别： 3.4 专门领域 自动驾驶：slam技术 车辆结构化：车牌识别，车型识别 医学图像分析: 3.5 三维视觉技术 三维重建：单视图三维重建，多视图三维重建，从全景图恢复三维结构，深度相机室内实时稠密三维重建， 立体匹配技术：阵列相机立体全景拼接 运动重构SFM：Strucutre From Motion，从一系列包含视觉运动信息的多幅二维图像序列中估计三维结构 基于RGB-D的三维深度学习：三维目标分类，三维目标检测，三维语义分割 构造深度图：单目微运动生成深度图，深度图补全 深度学习自动构图：","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"计算机视觉方向","slug":"计算机视觉方向","permalink":"http://holdfire.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%96%B9%E5%90%91/"}]},{"title":"目标检测——综述","slug":"cV-objectDetection","date":"2019-11-03T13:27:36.000Z","updated":"2019-11-06T11:57:19.233Z","comments":true,"path":"2019/11/03/cV-objectDetection/","link":"","permalink":"http://holdfire.github.io/2019/11/03/cV-objectDetection/","excerpt":"","text":"1. 简介 2.Faster R-CNN系列 3. YOLO系列 4. SSD系列 5. 其他方法","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://holdfire.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"非监督学习——高斯混合模型GMM","slug":"ml-GMM","date":"2019-11-03T10:22:42.000Z","updated":"2019-11-06T12:01:24.094Z","comments":true,"path":"2019/11/03/ml-GMM/","link":"","permalink":"http://holdfire.github.io/2019/11/03/ml-GMM/","excerpt":"","text":"高斯混合模型的数学形式： 高斯混合模型(Gaussian Mixture Model, GMM)是指具有如下形式概率分布的模型： P(y∣θ)=∑k=1Kαkϕ(y∣θk)(1)\\tag{1}P(y|\\theta) = \\sum_{k=1}^K \\alpha_{k} \\phi(y|\\theta_{k}) P(y∣θ)=k=1∑K​αk​ϕ(y∣θk​)(1) 其中，αk\\alpha_{k}αk​是系数，αk≥0\\alpha_{k}\\geq0αk​≥0，∑k=1Kαk=1\\sum_{k=1}^K \\alpha_{k}=1∑k=1K​αk​=1。θk=(μk,δk2)\\theta_{k} = (\\mu_{k}, \\delta_{k}^2)θk​=(μk​,δk2​)，ϕ(y∣θk)\\phi(y|\\theta_{k})ϕ(y∣θk​)是高斯分布密度，称为高斯混合模型的第k个分模型，模型形式为： ϕ(y∣θk)=12πδkexp(−(y−μk)22δk2)(2)\\tag{2} \\phi(y|\\theta_{k}) = \\frac{1}{\\sqrt {2\\pi} \\delta_{k}} exp(-\\frac {(y-\\mu_{k})^2}{2\\delta_{k}^2}) ϕ(y∣θk​)=2π​δk​1​exp(−2δk2​(y−μk​)2​)(2) 如果公式（1）中的分模型不是高斯分布密度函数，而是任意概率密度分布，式（1）成为一般混合模型。 高斯混合模型参数估计的EM算法 假设观测数据y1,y2,...yNy_1,y_2,...y_Ny1​,y2​,...yN​由高斯混合模型(公式1)生成，其中θ=(α1,α2,...,αK;θ1,θ2,...,θK)\\theta = (\\alpha_1,\\alpha_2,...,\\alpha_K;\\theta_1,\\theta_2,...,\\theta_K)θ=(α1​,α2​,...,αK​;θ1​,θ2​,...,θK​)，利用EM算法估计高斯混合模型参数θ\\thetaθ的步骤为： 明确隐变量，写出完全数据的对述似然函数 可以设想观测数据yj,j=1,2,...,Ny_j, j=1,2,...,Nyj​,j=1,2,...,N是这样产生的：首先依概率αk\\alpha_kαk​选择第k个高斯分布分模型，然后依第k个分模型的概率分布ϕ(y∣θk)\\phi(y|\\theta_{k})ϕ(y∣θk​)生成观测数据yjy_jyj​，这时观测数据yj,j=1,2,...,Ny_j, j=1,2,...,Nyj​,j=1,2,...,N是已知的；反映观测数据yjy_jyj​来自第k个分模型的数据来自第k个分模型的数据是未知的，k=1,2,...,Kk=1,2,...,Kk=1,2,...,K，以隐变量γjk\\gamma_{jk}γjk​表示，其定义如下： γjk={1第j个观测来自第k个分模型的数据0elsej=1,2,...,N；k=1,2,..,K(3)\\tag{3} \\gamma_{jk} = \\begin{cases} 1 &amp; 第j个观测来自第k个分模型的数据 \\\\ 0 &amp; else \\end{cases} \\newline j = 1,2,...,N；k = 1,2,..,K γjk​={10​第j个观测来自第k个分模型的数据else​j=1,2,...,N；k=1,2,..,K(3) 其中，γjk\\gamma_{jk}γjk​是0-1随机变量。 完全数据包含：观测数据yjy_jyj​和未观测数据γjk\\gamma_{jk}γjk​，即： (yj;γj1,γj2,...γjk),j=1,2,...,N(y_j; \\gamma_{j1},\\gamma_{j2},...\\gamma_{jk}), j=1,2,...,N (yj​;γj1​,γj2​,...γjk​),j=1,2,...,N 因而，完全数据的似然函数为： P(y,γ∣θ)=P(y,\\gamma | \\theta) = P(y,γ∣θ)= 上述算法流程 输入：观测数据y1,y2,...yNy_1,y_2,...y_Ny1​,y2​,...yN​，高斯混合模型； 输出：高斯混合模型的参数。 （1）取参数的初始值开始迭代 高斯混合模型的应用","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"非监督学习","slug":"非监督学习","permalink":"http://holdfire.github.io/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"非监督学习——EM算法","slug":"ml-EMAlgorithm","date":"2019-11-03T10:00:40.000Z","updated":"2019-11-06T12:01:17.192Z","comments":true,"path":"2019/11/03/ml-EMAlgorithm/","link":"","permalink":"http://holdfire.github.io/2019/11/03/ml-EMAlgorithm/","excerpt":"","text":"硬币问题中的EM算法 三硬币模型 EM算法的推导 EM算法的收敛性 EM算法的应用——非监督学习 EM算法的应用——高斯混合模型 EM算法的推广 参考资料： 从投硬币问题理解EM算法：https://www.jianshu.com/p/1121509ac1dc 李航《统计学习方法》P155：EM算法及其推广","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"非监督学习","slug":"非监督学习","permalink":"http://holdfire.github.io/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"图像处理——上采样与下采样","slug":"cv-imageProcess-upAndDownSampling","date":"2019-11-01T22:14:41.000Z","updated":"2019-11-06T11:58:13.611Z","comments":true,"path":"2019/11/02/cv-imageProcess-upAndDownSampling/","link":"","permalink":"http://holdfire.github.io/2019/11/02/cv-imageProcess-upAndDownSampling/","excerpt":"","text":"","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://holdfire.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"神经网络——参数初始化","slug":"dl-base-parameterInitialization","date":"2019-11-01T21:57:50.000Z","updated":"2019-11-07T02:04:59.683Z","comments":true,"path":"2019/11/02/dl-base-parameterInitialization/","link":"","permalink":"http://holdfire.github.io/2019/11/02/dl-base-parameterInitialization/","excerpt":"","text":"1.如何分析参数初始化结果的好坏？ 查看初始化后各层的激活值分布，如果在某个固定区间内分布则较好，如果集中在某个值上则初始化不好。 2. 把w初始化为0 对于单层网络可行；对于多层网络，由于链式法则会导致梯度消失。 3. 随机初始化 使用均值为0，方差为0.02的正态分布去初始化。w初始值较小是因为：如果x很大的话，w又相对较大，会导致Z非常大，这样如果激活函数是sigmoid，就会导致sigmoid的输出值1或者0。 4. Xavier initialization Xavier initialization方法是Glorot等人为了解决随机初始化的问题提出来的另一种初始化方法。他们的思想倒也简单，就是尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。Xavier initialization能够很好的tanh激活函数。 5. He initialization 参考资料： 斯坦福大学——李飞飞计算机视觉课程cs231n：","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"神经网络——归一化Normalization","slug":"dL-base-normalization","date":"2019-11-01T21:13:56.000Z","updated":"2019-11-07T02:04:39.838Z","comments":true,"path":"2019/11/02/dL-base-normalization/","link":"","permalink":"http://holdfire.github.io/2019/11/02/dL-base-normalization/","excerpt":"","text":"1. 简介 CNN中最广泛使用的归一化形式是局部响应归一化(Divisive Normalization, DN) 2. 批归一化 批归一化(Batch Normalization)是在训练模型时进行的。模型训练时，每次输入的数据为[N,H,W,C][N, H, W, C][N,H,W,C]时，在BatchSize这个维度进行归一化处理。而在测试时，批归一化采用的参数为训练集的均值和方差， 批归一化： 3. Layer Norm, Instance Norm, Group Norm 3.1 层归一化Layer Norm 4. 其他归一化 参考文献： 2015年谷歌论文：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift CSDN论文解读博客：https://blog.csdn.net/hjimce/article/details/50866313","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"http://holdfire.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"论文阅读","slug":"论文阅读","permalink":"http://holdfire.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"}]},{"title":"目标检测——评价指标","slug":"cv-objectDetection-evaluation","date":"2019-11-01T21:07:41.000Z","updated":"2019-11-06T11:56:52.170Z","comments":true,"path":"2019/11/02/cv-objectDetection-evaluation/","link":"","permalink":"http://holdfire.github.io/2019/11/02/cv-objectDetection-evaluation/","excerpt":"","text":"1. 简介 2. 混淆矩阵与相关指标 混淆矩阵的形式如下图 True Positive(TP): 正样本被正确识别为正样本； True Negative(TN)：负样本被正确识别为负样本； False Positive(FP): 假的正样本，即负样本被错误识别为正样本； False Negative(FN)：假的负样本，即正样本被错误识别为负样本； 准确率： 准确率(Accuracy)是混淆矩阵中对角线之和除以样本总数。 Accuracy=(TP+TN)(TP+TN+FP+FN)Accuracy = \\frac {(TP + TN)} {(TP+TN+FP+FN)} Accuracy=(TP+TN+FP+FN)(TP+TN)​ 精确率和召回率： 精确率(precision)也叫查准率，召回率(recall)也叫查全率。 Precision=TPTP+FPPrecision = \\frac {TP} {TP+FP} Precision=TP+FPTP​ Recall=TPTP+FNRecall = \\frac {TP} {TP+FN} Recall=TP+FNTP​ F1度量是基于查全率和查准率的调和平均： F1=11/Precision+1/RecallF1 = \\frac {1} {1 / Precision + 1/Recall} F1=1/Precision+1/Recall1​ 3. PR曲线 对预测结果进行排序，设置不同的阈值依次识别前K张图片为正例，每次计算出当前的查全率和查准率，分别作为横轴和纵轴作图，就得到了PR曲线。曲线上查全率=查准率的点成为平衡点(Break-Even Point, BEP)。 4. ROC曲线和AUC 受试者工作特征曲线(Recievier Operating Characteristic)，其横轴是假正例(False Positive Rate, FPR)，纵轴是真正例率(True Positive Rate, TPR,等价于召回率Recall)，定义公式如下。ROC曲线和PR曲线一样，设置不同的正例阈值来作曲线。曲线和横纵坐标包围的面积成为AUC(Area Under ROC Curve)指标。 FPR=FPTP+FPFPR = \\frac {FP} {TP+FP} FPR=TP+FPFP​ TPR=TPTP+FNTPR = \\frac {TP} {TP+FN} TPR=TP+FNTP​ PR曲线和ROC曲线比较 ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。 参考：https://www.cnblogs.com/eilearn/p/9071440.html 5. 交并比IoU 在目标检测任务中，通常使用矩形框来定位检测对象的边界。假设你的算法给出的边界框为A，实际的边界框为B，那么交并比(Intersection over Union)就可以衡量检测结果的好坏，其计算公式为： IoU=A∩BA∪BIoU = \\frac {A \\cap B} {A \\cup B} IoU=A∪BA∩B​ 通常约定IoU大于0.5，就认为目标检测正确。 6. 平均精度(Average-Precision,AP)与mean Average Precision(mAP) AP就是PR曲线下的面积，通常一个分类器越好，AP值越高。 mAP是多个类别AP的平均值。这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间，越大越好。该指标是目标检测算法中最重要的一个。 7. 非极大值抑制NMS 在目标检测任务中，你的算法可能对同一个目标做了多次检测。非极大值抑制(Non-Maximum Suppression, NMS)就是为了确保你的算法对一个目标只检测一次。 要深究原理！！！！！！ 8. Anchor Box 参考资料： 吴恩达深度学习课程： 斯坦福大学，李飞飞计算机视觉课程cs231n： 慕课网，深度学习之目标检测课程：","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"http://holdfire.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://holdfire.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"神经网络——二阶优化算法","slug":"dl-base-optimizationAlgorithm2","date":"2019-10-30T18:54:23.000Z","updated":"2019-11-07T02:04:52.721Z","comments":true,"path":"2019/10/31/dl-base-optimizationAlgorithm2/","link":"","permalink":"http://holdfire.github.io/2019/10/31/dl-base-optimizationAlgorithm2/","excerpt":"","text":"1. 简介 1.1 牛顿法 1.2 高斯-牛顿法 1.3 拟牛顿法 1.4 发展历程 2. 不常用的二阶优化算法 2.1 DFP算法 第一个拟牛顿算法是由Argonne国家实验室的物理学家William C.Davidon提出的。 他在1959年开发了第一个拟牛顿算法:DFP（Davidon–Fletcher–Powell formula）更新公式，后来由Fletcher和Powell在1963年推广，但现在很少使用 3. 常用的二阶优化算法 3.1 SR1公式（用于“对称秩一”） 3.2 BHHH方法 3.3 BFGS方法 BFGS算法的全称为Broyden–Fletcher–Goldfarb–Shanno algorithm 3.4 Limited-memory BFGS方法 低内存拓展算法 参考资料： 机器之心——二阶优化算法介绍：https://www.jiqizhixin.com/graph/technologies/75950ad0-edbd-4208-9347-b8c17b8e058c","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"凸优化","slug":"凸优化","permalink":"http://holdfire.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"}]},{"title":"机器学习——损失函数","slug":"dl-base-lossFunction","date":"2019-10-30T18:29:07.000Z","updated":"2019-11-07T02:04:30.121Z","comments":true,"path":"2019/10/31/dl-base-lossFunction/","link":"","permalink":"http://holdfire.github.io/2019/10/31/dl-base-lossFunction/","excerpt":"","text":"1. 简介 1.1 什么是损失函数？ 2.回归问题中的损失函数 3. 分类问题中的损失函数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"神经网络——一阶优化算法","slug":"dL-base-optimizationAlgorithm1","date":"2019-10-30T18:28:46.000Z","updated":"2019-11-07T02:04:46.080Z","comments":true,"path":"2019/10/31/dL-base-optimizationAlgorithm1/","link":"","permalink":"http://holdfire.github.io/2019/10/31/dL-base-optimizationAlgorithm1/","excerpt":"","text":"1. 简介 1.1 什么是优化算法？ 1.2 为什么需要优化算法？ 1.3 优化算法分类 2. 一阶优化算法 2.1 随机梯度下降法 随机梯度下降法(Stotastic Gradient Descend method, SGD)是 2.2 批量梯度下降法 批量梯度下降法(Batch Gradient Descend) 2.3 小批量随机梯度下降法 小批量梯度下降法(mini-Batch Gradient Descend) 2.4 动量梯度下降法 动量梯度下降法(Momentum Gradient Descend) 2.5 Nesterov梯度下降方法 Nesterov梯度下降方法(Nesterov Accelerated Gradient)是对传统momentum方法的一项改进，由Ilya Sutskever(2012 unpublished)在Nesterov工作的启发下提出的。 其基本思路如下图： 2.6 Adagrade 2.7 Adadelta 2.8 RMSprop 2.9 Adam 3 选择技巧 参考资料： 各种优化方法总结比较：https://www.cnblogs.com/qniguoym/p/8058186.html","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"凸优化","slug":"凸优化","permalink":"http://holdfire.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"}]},{"title":"神经网络——激活函数","slug":"dl-base-activationFunction","date":"2019-10-30T18:24:41.000Z","updated":"2019-11-07T02:04:10.948Z","comments":true,"path":"2019/10/31/dl-base-activationFunction/","link":"","permalink":"http://holdfire.github.io/2019/10/31/dl-base-activationFunction/","excerpt":"","text":"简介 在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。激活函数有什么作用呢？ 神经网络中如果不使用激活函数（相当于激活函数是f(x)=xf(x)=xf(x)=x)，那么每一层节点的输出都会是输入的线性函数。其结果是：无论神经网络有多少层，网络的输出层都是输入层的线性组合，神经网络仅相当于一个感知机，网络的拟合能力变得非常有限。 神经网络中加入激活函数后，网络对现实模型的表达能力得到增强，网络几乎能够逼近任意的函数模型。常用的的激活函数主要有：sigmoid函数，relu函数，tanh函数，leaky_relu函数，maxout函数等。 常用的激活函数 sigmoid函数 sigmoid函数是应用最为广泛的激活函数之一,函数形式如下所示。能够把任意实数映射到(0,1)(0,1)(0,1)区间上的实数。当自变量值小于-5时，函数值接近于0；当自变量大于5时，函数值非常接近于1。sigmoid函数主要的优点是求导方便，其导数f′(x)=f(x)⋅[1−f(x)]f^{&#x27;}(x)=f(x)\\cdot[1-f(x)]f′(x)=f(x)⋅[1−f(x)]。当x=0时函数的导数最大，为0.25。 σ(x)=11+e−x(1)\\tag{1} \\sigma(x) = \\frac {1}{1+e^{-x}} σ(x)=1+e−x1​(1) 缺点1：梯度消失和梯度爆炸 如果将神经网络的权值初始化为均值为0，标准差为1，由于sigmoid函数的导数在两端非常接近于0，靠近输入层的权重的更新值，进过一系列累乘后会变得特别小，权重参数得不到更新，这种现象叫做梯度消失。如果将初始权重设得特别大，如1000，同理可能会造成梯度爆炸，即权重参数每次的更新值太大。所以，为了同时解决梯度消失和梯度爆炸两个问题，我们需要设置合理的权重初始值，但这个很难做到，详细解释见附录。 缺点2：输出不是0均值 当上一层的输出结果不是0均值(zero-centered)时，即下一层的输入信号不是0均值，会产生这样的一个结果：对于函数f(x)=wTx+bf(x)=w^{T}x+bf(x)=wTx+b，如果输入x&gt;0x&gt;0x&gt;0,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，权重参数的收敛变得缓慢，称为zig-zag现象。 如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。 缺点3：求幂运算消耗时间较长 计算机求exe^{x}ex会消耗较长的时间。 tanh函数 tanh函数的主要优点是:解决了sigmoid函数的非零均值问题，但梯度消失的问题仍然没有解决。其函数形式为： tanh(x)=ex−e−xex+e−x(2)\\tag{2} tanh(x)=\\frac{e^{x}-e^{-x}} {e^{x}+e^{-x}} tanh(x)=ex+e−xex−e−x​(2) 我们知道Relu函数在卷积神经网络中取得了很好的结果，那为什么在RNN中还是使用tanh作为激活函数呢？ ReLU函数 ReLU函数(Rectified Linear Unit)的主要优点是：一定程度解决了梯度消失的问题，计算速度快，收敛速度远快于sigmoid函数和tanh函数。函数形式为： relu(x)=max(0,x)(3)\\tag{3} relu(x) = max(0,x) relu(x)=max(0,x)(3) ReLU函数的主要缺点是： 输出非零均值；可能会造成Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: 一是参数初始化不当，负的权值太多，这种情况比较少见。二是学习率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 Leaky ReLU函数 为了解决Dead ReLU Problem，Leaky ReLU函数将ReLU的前半段设为αx\\alpha xαx而非0，通常α=0.01\\alpha = 0.01α=0.01。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。其函数形式为： leaky relu(x)=max(αx,x)(4)\\tag{4} leaky\\ relu(x) = max(\\alpha x, x) leaky relu(x)=max(αx,x)(4) Maxout函数 Maxout函数可以看做是relu函数和leaky relu函数的一般化归纳。通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的界，然后每一段取最大。maxout可以看成是ReLU家族的一个推广,不会产生饱和，也不会产生dead ReLU。缺点在于参数量翻倍了。 f(x)=max(w1Tx+b1,w2Tx+b2)(5)\\tag{5} f(x) = max(w_1^Tx+b_1,w_2^Tx+b_2) f(x)=max(w1T​x+b1​,w2T​x+b2​)(5) Softmax函数 Softmax函数作为激活函数，通常用在多分类神经网络的输出层上，目的是让大的值更大。其函数形式为： δ(zj)=ezj∑i=1kezi(6)\\tag{6} \\delta (z_j) = \\frac {e^{z_j}} {\\sum_{i=1}^{k}e^{z_i}} δ(zj​)=∑i=1k​ezi​ezj​​(6) 激活函数的选择 (1)深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。 (2）如果使用 ReLU，那么一定要小心设置 learning rate， 而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout. (3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout，其计算量太大导致收敛很慢。 (4)为了防止饱和，现在主流的做法会在激活函数前进行batch normalization操作，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。 参考资料： 书籍Neural Networks and Deep Learning第5章 Hinton关于Relu函数在RNN中不奏效的论文：A Simple Way to Initialize Recurrent Networks of Rectified Linear Units Glorot和Bengio关于sigmoid函数在深层网络中产生饱和的论文：Understanding the difficulty of training deep feedforward neural networks 知乎专栏——RNN 中为什么要采用 tanh，而不是 ReLU 作为激活函数 ：https://www.zhihu.com/question/61265076/answer/186347780 附录:sigmoid函数梯度消失和梯度爆炸试验 下文来自参考资料第1条： 现象： 在深层网络中，不同隐含层的学习速度(各隐含层权值和偏差偏导组成的向量的范数)相差很大。靠后的隐含层学习速度较大，而靠前的隐含层经常在训练期间卡住，几乎什么都学习不到；也有时候早期的层可能学习很好，但后来的层卡住。 试验： 使用MNIST数据集进行图像分类任务，输入层神经元个数为784，隐含层神经元个数均为30，输出层神经元个数为10；使用sigmoid函数作为激活函数。训练图片为1000张，使用Batch Stostic Gradient Descend算法，500个epoch。使用不同数目的隐含层的分类结果如下： 隐含层数目 1 2 3 4 分类准确率 96.48% 96.90% 96.57% 96.53% 带有4个隐含层的网络，训练过程中各隐含层权重的学习速度变化如下图所示。从图中可以看出：在训练后期，第一个隐含层的学习速率比第四个隐含层慢了约100倍。 分析： 直观上来说，额外的隐含层应该使网络能够学习更为复杂的分类功能，从而进行更好的分类。即使额外的隐含层什么都不做，模型的准确率也不会变得更差。由于权重是随机初始化的，因而第一个隐含层会丢失大量的图片有用信息，所以此时第一层几乎不可能不需要再学习，即还没有收敛到最优值。若假设额外的隐含层确实有用，那么问题应该是我们的学习算法没有找到合适的权重和偏差。 梯度消失的数学推导： 考虑一个每层只有1个神经元的深层神经网络，w1,w2,...w_1,w_2,...w1​,w2​,...为权重，b1,b2,...b_1,b_2,...b1​,b2​,...为偏置。aja_jaj​为第j层的输出值，代价函数C对第一个隐含层的偏置b1b_1b1​的偏导为： 将神经网络的权值初始化为均值为0，标准差为1。则大部分权值满足∣wj∣&lt;1\\left|w_j\\right|&lt;1∣wj​∣&lt;1，且有∣wjσ′(zj)&lt;14∣\\left|w_j \\sigma^{\\prime}(z_j)&lt;\\frac {1} {4}\\right|∣∣​wj​σ′(zj​)&lt;41​∣∣​。由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为(1,+∞)(1,+\\infty)(1,+∞)区间内的值，则会出现梯度爆炸情况。 梯度爆炸的产生： 当权重设置很大时，如w1=w2=w3=w4=1000w_1=w_2=w_3=w_4=1000w1​=w2​=w3​=w4​=1000,而z的值接近0时，会产生梯度爆炸。 思考： 为了避免梯度消失，我们可以通过设置合适wjw_jwj​的初始值，使得∣wjσ′(zj)∣&gt;1\\left|w_j \\sigma^{\\prime}(z_j) \\right|&gt;1∣wj​σ′(zj​)∣&gt;1。事实上，这非常困难，因为σ′(zj)=σ′(wa+b)\\sigma^{\\prime}(z_j)=\\sigma^{\\prime}(wa+b)σ′(zj​)=σ′(wa+b)的值是依赖于w的，当w的值较大时，σ′(zj)\\sigma^{\\prime}(z_j)σ′(zj​)通常会很小。梯度消失很难避免。 总结： 深层神经网络的反向传播过程会发生梯度变化不稳定的状况。通过使用其他的激活函数，我们能否避免这种梯度下降不稳定的现象呢？","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://holdfire.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Python——导入操作import","slug":"python-package-module-import","date":"2019-10-30T11:19:53.000Z","updated":"2019-10-30T17:34:35.542Z","comments":true,"path":"2019/10/30/python-package-module-import/","link":"","permalink":"http://holdfire.github.io/2019/10/30/python-package-module-import/","excerpt":"","text":"1. 简介 1.1 导入的内容 Python中导入操作的关键字为import，导入的内容主要是：包(package)、模块(module)、类(class)、函数(类中称作方法, method)、变量(类中称作属性, attributes)等。 在文件夹下面加入__init__.py文件(以下简称init文件)，这个文件夹就成为了一个包。包分为常规包和命名空间包。 模块是包下面的python文件。所有的包都可以被看做模块，但不是所有的模块都是包。 类是文件的基本单元（类中包括变量和函数）。 导入操作中，导入的具体内容主要由init文件、__all__列表(以下简称all列表)控制。 1.2 init文件的作用 如果包中含有init文件，import 包名就会执行init文件，能够批量导入init文件中导入的模块； 当这个包或者该包下的模块被导入时，init文件都会被执行； 1.3 all列表的作用 在init文件或模块中设置all列表时，from 包名/模块名 import *只会导入all列表中的内容； all列表的内容可以是本文件自定义的变量或函数，或者是导入进来的模块； 从模块中all列表导入的内容称为内置变量，内置变量易覆盖本文件中的变量名，因而不建议使用from 模块名 import * 2. import的使用方法 假设你正在一个名为current.py的文件（下面称为当前文件）中写代码，你想导入一些包或模块，那么不同的导入操作会有什么样的结果呢？ 注意：下面说的执行init文件 == init文件中所有的导入操作都会被执行，但这并不意味导入的内容能被当前文件所调用。（这句话主要是针对下面代码块中从包中导入模块这一情况） 1234567891011121314151617181920212223//(1) 从包名导入：包里必须有init文件，且init文件会被执行import 包名 // 当前文件能调用init文件所导入的全部内容。调用方式为包名.模块名from 包名 import * // 若init文件有all列表，则当前文件只能调用all列表的内容；若无则能调用init文件所导入的全部内容。调用方式为模块名//(2) 从包中导入模块：包里如果有init文件则先执行该文件，没有也没关系；然后导入该模块import 包名.模块名 // 当前文件能调用init文件所导入的全部内容和该模块。调用方式为包名.模块名from 包名 import 模块名 // 当前文件只能调用该模块，无法调用init文件所导入的内容。调用方式为模块名//(3) 单独导入模块：当前文件和该模块需在同一目录下import 模块名 // 当前文件能调用该模块的所有对象(类、函数、变量等)，调用方式为模块名.对象名//(4) 从模块中导入对象(类、函数、变量等)from 模块名 import 对象名 // 当前文件只能调用该模块下的指定对象，调用方式为对象名from 模块名 import * // 当前文件只能调用该模块下all列表中的对象，若无all列表则能调用所有的对象。调用方式为对象名//(5) 包的相对导入：相对导入只能用from import 的方式from . import 包名 // .表示当前文件所在目录的路径from .. import 包名 // ..表示当前文件所在目录的上级目录的路径//(6) 相对导入应用：init文件中的导入from 包名 import 模块名 // 正确from . import 模块名 // 正确，使用相对导入，其中.表示当前文件所在的目录import 模块名 // 错误 3. import被解释执行 3.1 importlib包介绍 3.2 import的搜索路径 搜索路径被存储在sys模块中的path变量 在导入搜索期间首先会被检查的地方是 sys.modules。 这个映射起到缓存之前导入的所有模块的作用（包括其中间路径）。 当指定名称的模块在 sys.modules 中找不到时，Python 会接着搜索 sys.meta_path，其中包含元路径查找器对象列表。 3.3 import的注意事项 每个包或模块只会被导入一次。 要避免循环导入。 参考资料： Python语言参考——导入系统：https://docs.python.org/zh-cn/3/reference/import.html 菜鸟教程——Python3模块：https://www.runoob.com/python3/python3-module.html","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://holdfire.github.io/tags/Python/"}]},{"title":"C++学习——内存分配","slug":"cpp-memory-allocation","date":"2019-10-29T19:55:20.000Z","updated":"2019-11-01T19:27:24.030Z","comments":true,"path":"2019/10/30/cpp-memory-allocation/","link":"","permalink":"http://holdfire.github.io/2019/10/30/cpp-memory-allocation/","excerpt":"","text":"1. 栈区 由编译器自动分配和释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 2. 堆区 一般有程序员分配和释放，用new运算符申请；若程序员不释放，程序结束时可由OS回收。 3. 全局区（静态区） 全局变量（main函数前声明的变量）和静态变量的存储放在一起。其中，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在一块区域。 4. 文字常量区 常量字符串存储的地方。 为了节省内存，C/C++把常量字符串放到一个单独的内存区域。当几个指针赋值给相同的常量字符串时，它们实际上会指向相同的内存地址。但用常量内存初始化数组时，情况却有所不同。如下所示：str1和str2的地址不同，str3和str4的地址相同。 1234char str1[] = \"Hello world\"char str2[] = \"Hello world\"char* str3 = \"Hello world\"char* str3 = \"Hello world\" 5. 程序代码区 存放函数体的二进制码。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——左值与右值","slug":"cpp-left-value-right-value","date":"2019-10-29T18:12:27.000Z","updated":"2019-10-30T11:23:24.657Z","comments":true,"path":"2019/10/30/cpp-left-value-right-value/","link":"","permalink":"http://holdfire.github.io/2019/10/30/cpp-left-value-right-value/","excerpt":"","text":"1. 基本定义 可以放到赋值操作符左边的是左值，可以放到赋值操作符右边的是右值; 左值的声明符号为&amp;，右值的声明符号为&amp;&amp;； 左值是可以取地址的对象或变量，可以作为右值使用； 右值通常是临时对象、常量等，不能作为左值使用； 右值分为纯右值(Pure RValue，如非应用返回的函数返回值，表达式等)和将亡值(eXpiring Value); 2. 左值引用和右值引用 2.1 基本定义 左值引用和右值引用都是属于引用类型。无论是声明一个左值引用还是右值引用，都必须立即进行初始化。而其原因可以理解为是引用类型本身自己并不拥有所绑定对象的内存，只是该对象的一个别名。 左值引用是具名变量值的别名，通常不能绑定到右值。非常量左值只能接受非常量左值对其进行初始化。 右值引用是不具名（匿名）变量的别名。 右值值引用通常不能绑定到任何的左值，要想绑定一个左值到右值引用，通常需要std::move()将左值强制转换为右值。 2.2 C++11中各引用类型可以引用的值类型 非常量左值引用： 非常量左值； 常量左值引用： 非常量左值；常量左值；非常量右值；常量右值； 非常量右值引用： 非常量右值； 常量右值引用： 非常量右值；常量右值。 2.3 特点 常量左值引用是个全能的引用类型，可用于拷贝语义。它可以接受非常量左值、常量左值、右值对其进行初始化。 非常量右值引用用于移动语义，完美转发。 2.4 示例 1234567891011int a = 1; //正确，变量a可以是左值int &amp;b = a; //正确，因为a是左值，引用类型变量的赋值表达式里，右边必须是左值int &amp;c = 5; //错误，因为5是一个字面值，是右值不是左值int const &amp;d= 100; //正确，是为了能将临时对象作为引用参数传递给函数，为啥？const int &amp;c = b; //常量左值引用绑定到非常量左值，编译通过const int d = 2; //常量左值const int &amp;e = c; //常量左值引用绑定到常量左值，编译通过const int &amp;b =2; //常量左值引用绑定到右值，编译通过int &amp;&amp;r1 = c; // 编译失败，因为c是一个左值int a;int &amp;&amp;r2 = std::move(a); //编译通过 3. 常见例子 3.1 函数的返回值 如果一个函数的返回值是一个临时对象，就是右值。如果返回值为引用，由于引用是对象的别名，通过引用可以改变对象的值，是左值。 12345int arr[] = &#123;1,2,3,4,5&#125;;int fun1(int i)&#123;return arr[i];&#125;int&amp; fun2(int i)&#123;return arr[i];&#125;fun1(0) = 10; // 错误，fun1()是右值，不能被赋值；fun2(0) = 10; // 正确，fun2()是左值，可以被赋值； 3.2 前置和后置 前置的返回值是被的对象的引用，是一个可寻址的变量。返回值是左值，可以被赋值，如++i=3。 后置的返回值是被对象的原始值，是一个临时对象。返回值是右值，不可以作为左值，代码i++=3是错误的。 下面的代码是C中前置和后置++的实现： 1234// 前置++的实现T&amp; T::operator++() &#123;++*this; return this;&#125;// 后置++的实现T T::operator++(int)&#123;T old(*this); ++(*this); return old;","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——指针","slug":"cpp-pointer","date":"2019-10-29T15:31:44.000Z","updated":"2019-11-01T19:22:56.855Z","comments":true,"path":"2019/10/29/cpp-pointer/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-pointer/","excerpt":"","text":"1. 简介 1.1 指针 1.2 空指针 1.3 智能指针 2. 指针的使用 指针与数组 C/C++中没有记录数组的大小，因此使用指针访问数组元素是，注意不能超过数组的边界。 当数组作为函数的参数进行传递时，数组就自动退化为同类型的指针。 2.1(int*)a 和 int *a的区别？ 3. 指针和引用","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——引用","slug":"cpp-reference","date":"2019-10-29T15:31:23.000Z","updated":"2019-10-30T13:14:08.234Z","comments":true,"path":"2019/10/29/cpp-reference/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-reference/","excerpt":"","text":"1. 简介 引用是一种特殊的变量，可以被认为是一个变量的别名。 例如：int a = 1; int &amp;b = a;变量b即为变量a的引用。 声明一个引用时，必须同时对它进行初始化，使它指向一个已经存在的对象。 一旦一个引用被初始化后，就不能改为指向其他对象。 引用变量和原变量取地址运算&amp;的结果相同。 2. 引用作为函数参数 2.1 引用传递 首先区分一下两个概念：当函数的参数为普通变量时，参数传递的方式称作值传递。当函数的参数为引用时，参数传递的方式称作引用传递。函数在被调用时，若发生的是值传递，形参会被分配内存单元并与实参结合；若发生的是引用传递，由于形参时实参的一个别名，对形参的任何操作都会直接作用于实参。 下方代码中：执行fun1(x1)时参数传递方式为值传递，形参会被分配内存单元，函数调用完后x1的值仍为0。执行fun1(x1)时参数传递方式为引用传递，函数调用栈中不会产生变量x2的副本，而是直接用变量x2的值进行运算，函数调用完后x2的值为1。 12345void fun1(int a) &#123;a++; return;&#125; // 值传递void fun2(int &amp;a)&#123;a++; return;&#125; // 引用传递int x1 = 0, x2 = 0;fun1(x1); // x1的值不会被修改fun2(x2); // x2的值会被修改为1 2.2 引用传递——形参为常引用 如果一个函数的形参被申明为常引用，那么在函数体中，就不能修改该引用变量的值，达到了引用的安全性。下方代码将会报错：“常引用对象a为只读对象，不能进行a++操作。” 123void fun3(const int &amp;a)&#123;a++; return;&#125; // 错误：参数为常引用，只读不允许被修改int x3 = 0; fun3(x3)； 2.3 引用传递——形参为引用时需注意 如果一个函数的形参为引用时，那么需要注意传递的实参不能为const类型。而C++中的临时对象都是const类型的！ 如函数的返回值，未命名的内容都是临时变量，因而下面的代码会运行错误。如果把fun4()的形参设为(int a)或者(const int &amp;a)，代码就可以正常运行了。 所以，引用型参数应该在能被定义为const的情况下，尽量定义为const! 1234void fun4(int &amp;a)&#123;&#125;int helper()&#123;return 0;&#125;fun4(helper())； // 错误：helper()返回的临时对象是const类型，不能作为实参fun4(0); // 错误：未命名的数字0是const类型，不能作为实参 3. 引用作为函数返回值 引用作为函数的返回值时，内存中不会产生返回值的副本。 返回的应该是一个函数外就存在的对象； 返回对象不能是函数体内局部变量的引用，因为函数调用完毕，局部变量就不存在了； 返回对象不能是表达式，不能是new分配的对象。 123456int arr[] = &#123;1,2,3,4,5&#125;;int&amp; fun5(int i)&#123;return arr[i];&#125;int i = 0;cout&lt;&lt;&amp;arr[i]&lt;&lt;endl; // 返回结果：0x100402010cout&lt;&lt;&amp;fun5(i)&lt;&lt;endl; // 返回结果：0x100402010fun5(i) = 10; // 这也可以，牛不，就因为人家函数的返回值为引用，是左值，可以被赋值 返回值为引用时，为左值，返回值可以被赋值、取地址等操作！ 返回值不是引用时，为右值，如下方代码所示。 123456int arr[] = &#123;1,2,3,4,5&#125;;int fun6(int i)&#123;return arr[i];&#125;int i = 0;cout&lt;&lt;&amp;arr[i]&lt;&lt;endl; // 返回结果：0x100402010cout&lt;&lt;&amp;fun6(i)&lt;&lt;endl&gt;&gt;; // 错误，返回值为右值，不能取地址；fun6(i) = 10; // 错误，不可以的兄弟，fun6的返回值是右值 4. 引用总结 引用传递主要应用在函数参数传递上，传送较大数据和对象时，内存中不会产生副本，节约了内存空间； 用const将参数设置为常引用，用以保证引用变量不被随意修改； 跟指针变量相比，引用可以看做变量的别名，代码可读性强，在C++中推荐使用引用而非指针作为函数的参数；","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——多态性","slug":"cpp-polymorphism","date":"2019-10-29T11:20:16.000Z","updated":"2019-10-30T11:23:36.670Z","comments":true,"path":"2019/10/29/cpp-polymorphism/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-polymorphism/","excerpt":"","text":"1. 简介 1.1 C++的4种多态 多态性是指一段程序能够处理多种类型对象的能力。C++中可以通过强制多态、重载多态、类型参数化多态、包含多态4种形式实现。前两种是表面的多态性，后两种是真正的多态性。 强制多态：将一种类型的数据强制转化成另一种类型（隐式转换或显式转换） 重载多态：是指给同一个名字赋予不同的含义 参数化多态：通过模板实现，分为函数模板和类模板两种 包含多态：通过虚函数实现，虚函数是多态性的精华 1.2 静态绑定和动态绑定 多态性还可以分成编译时的多态和运行时的多态：其中，绑定工作在编译连接阶段完成的情况称为静态绑定，在程序运行阶段完成的情况称为动态绑定。绑定时指计算机程序自身彼此关联的过程，用面向对象的术语讲，就是把一条消息和一个对象的方法相结合的过程 2. 强制多态 3. 重载多态 4. 参数化多态 5. 包含多态","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"C++学习——const限定符","slug":"cpp-const","date":"2019-10-29T10:57:15.000Z","updated":"2019-10-30T11:23:18.538Z","comments":true,"path":"2019/10/29/cpp-const/","link":"","permalink":"http://holdfire.github.io/2019/10/29/cpp-const/","excerpt":"","text":"1. const简介 简介：用const关键字修饰的内容为常量，其值初始化后就不允许修改。 命名：常用的做法是将这个常量的首字母或全部大写，以提醒用户这是个常量。 初始化：在声明一个常量时就应该将其初始化，如const int Name = value;。 替换：C语言中通常使用#define来定义常量，而const能够明确指定类型和限定作用域，优于前者。 优点：在函数的参数传递中，在引用传递前加上const限定符，可以防止输入变量被修改。 2. const的用法 const可以用来修饰整型变量、引用变量、指针变量、函数、对象、数组等。修饰的是其左边相邻的内容，如果左边没有内容，则修饰右边相邻的内容。 12int age = 3; // 我的年龄age是个整型变量，可以修改，我今年3岁const int Age_Mao = 83; // age_Mao是由const修饰的常量，其值初始化后不能修改 2.1 常引用 下面的year就是对变量age的一个常引用，不能通过year修改age的值，但可直接修改age的值。常引用的这一优点可以应用在函数的参数传递上，既能节约内存，也能防止引用对象被修改。 123const int &amp;year = age; // year是一个常引用*b = 4; // 错误，常引用year，你是常引用，你不能改我的age啊age = 4; // 正确，我的年龄age可以通过赋值修改，明年我4岁 2.2 常指针 指针p指向age所在地址，不能指向别的。 1int *const pt1 = &amp;age; 2.3 指向常量的指针 如下所示：pt2和pt3均为指向常量的指针，不能通过指针来修改age或者age_Mao。（但age可直接修改，age_Mao不能修改） 123const int *pt2 = &amp;age; const int *pt3 = &amp;Age_Mao;int *p4 = &amp;Age_Mao; // 错误用法：禁止将const的地址附给非const指针(但const_cast可强制转换) 2.4 复杂情况：将指针指向指针 参考《C++ Primer Plus》P222 12int *pd = &amp;age; // *pd = 4 是有效的const int *pt = pd; // *pt = 4 是无效操作(一级间接关系) 3. 常成员函数 1类名::fun(形参) const; // 常成员函数： 4. 常对象 1类名 const 对象名; // 常对象： 5. 其他 1int const 数组名[]; // 常数组： 6. const_cast","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://holdfire.github.io/tags/C/"}]},{"title":"Linux——Linux命令介绍","slug":"linux-command","date":"2019-10-26T21:25:15.000Z","updated":"2019-10-30T11:21:53.054Z","comments":true,"path":"2019/10/27/linux-command/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-command/","excerpt":"","text":"1. 简介 1.1 Linux命令分类 Linux命令分为两种，一种是shell内置命令，一种是外部命名。在命令行输入type command可以区分。 1.2 帮助使用Linux命令 对于一些内部命令，可以有： 123man bash # 查看所有的内部命令man help # 得到所有内建命令列表及使用方法help command # 得到该命令的具体信息 对于外部命令，可以有： 1234command --help # 得到该命令的帮助信息command -h # 同上man command # info command # 1.3 Linux执行命令的查找顺序 环境变量$PATH：决定了shell在哪些目录中寻找命令或程序。由左到右依次在这些目录中查找，以先找到的为准。命令行输入echo $PATH可以查看环境变量。 1.4 Linux常用命令的存放位置 在命令行输入完一个命令后，shell会在下面几个目录查找是否有该命令： 12345/bin # 存放普通用户经常使用的命令，如cp,mv,kill/sbin # 存放系统管理员才能使用的系统管理程序，如ping,ifconfig/usr/bin # 存放普通用户使用的应用程序，如python,vi/usr/local/bin # 存放用户下载安装的软件或者自己编写的可执行文件，如pip,virtualenv,MySQL/usr/sbin # 存放系统管理员使用的比较高级的管理程序和系统守护程序","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——系统目录结构","slug":"linux-system-directory","date":"2019-10-26T21:23:19.000Z","updated":"2019-10-30T11:22:10.261Z","comments":true,"path":"2019/10/27/linux-system-directory/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-system-directory/","excerpt":"","text":"1. 简介","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——文件系统与目录篇(1)","slug":"linux-file-directory-1","date":"2019-10-26T19:17:45.000Z","updated":"2019-10-30T11:21:59.352Z","comments":true,"path":"2019/10/27/linux-file-directory-1/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-file-directory-1/","excerpt":"","text":"1. Linux的文件权限","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——简介","slug":"linux-introduction","date":"2019-10-26T16:10:34.000Z","updated":"2019-10-30T11:22:03.332Z","comments":true,"path":"2019/10/27/linux-introduction/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-introduction/","excerpt":"","text":"1. Linux简介 Linux是一个GNU GPL授权模式的开源的操作系统。操作系统由内核和系统调用组成,操作系统是应用程序和计算机硬件沟通的桥梁。即：硬件–&gt;内核–&gt;系统调用–&gt;应用程序(Shell)。 1.1 Linux内核版本与发行版本 Linux内核版本是单线发展的，其版本号由“主版本.次版本.末版本”的格式构成。截至2019年10月27日，最新的Linux稳定内核版本为5.3.7版。 Linux发行版本是由“Linux内核+软件+工具+文件”组成的一个完整的安装程序，不同的厂商提供不同的发行版本，主要分为两大类： 使用rpm/yum包管理方式的系统，包括：RedHat,Fedora,CentOS等； 使用apt-get/dpkg包管理方式的系统，包括：Debian,Ubuntu等； 1.2 Linux的特点 Linux严格区分大小写； Linux中一切内容皆文件（包括硬件）； Linux不依靠拓展名区分文件类型，而是依据文件权限； 通常，压缩包的后缀为.gz .bz2 .tar .bz2 .tgz；二进制软件包为.rmp；脚本文件为 .sh 2. Linux文件系统和目录 3. Shell与Shell Script 4. Linux用户管理 5. Linux系统管理","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Linux——Shell篇(1)","slug":"linux-shell-1","date":"2019-10-26T16:00:27.000Z","updated":"2019-10-30T11:22:06.845Z","comments":true,"path":"2019/10/27/linux-shell-1/","link":"","permalink":"http://holdfire.github.io/2019/10/27/linux-shell-1/","excerpt":"","text":"1. Shell是什么？ 1.1 正规军的解释 Shell是一个用C语言编写的程序！Shell是一个命令解释器！Shell是一种命令语言！Shell是一种程序设计语言！Shell是系统的用户界面，提供了用户与内核进行交互操作的接口！上面是网上找到的关于Shell的大部分解释，听着是不是让人头大？是的？！那就忘掉它，看看下面两个场景先。 1.2 接待员老白和Shell 场景A：你去同福客栈吃饭，你对接待员老白说：“我要一份酸菜鱼。”老白对着你点头微笑表示ok，然后跑去前台的点菜机上噼里啪啦输入一堆信息，该消息随后传到了后厨李大嘴那里，李大嘴开始鼓捣起他的菜刀、砧板、狼牙棒等厨具。一个时辰过后，你的桌上终于出现了一份既不好看也不好吃的酸菜鱼。 场景B：你在Linux终端工具上写代码，你写下了一行echo &quot;Hello, world&quot;,Shell对这行代码进行了解释，然后系统调用收到了该解释信息，信息接下来被传到了内核那里，内核开始控制CPU、内存、显卡等设备。一眨眼的时间，你的显示器上就出现了&quot;Hello, world&quot;这行字。 补充概念（此处可不看）：计算机的硬件由CPU、内存、显卡、磁盘等组成。操作系统包括内核和系统调用，内核（kernel，包括CPU调度、内存管理、磁盘输入输出等）直接控制上述硬件，系统调用是应用程序同系统之间的接口(API)，eg:Linux操作系统中如内存管理、网络、Socket套接字、进程间通信等接口。 1.3 游击队的解释 场景A中信息的流动方向是：你说的话–&gt;接待员老白–&gt;点菜机—&gt;李大嘴–&gt;厨具，场景B中信息的流动方向是：你的代码–&gt;Shell–&gt;系统调用—&gt;内核–&gt;硬件。仅考虑信息的流动方向，我们可以认为场景A等价于场景B。那么在场景A中，你说的话就是你的代码，接待员老白就是Shell，点菜机就是系统调用，李大嘴就是内核，厨具就是计算机的各种硬件。再回过头来看下1.1中Shell的解释，是不是就很好理解了呢？如果再有人问你Shell是什么，你就告诉他：Shell就是个接待员！ 2. Shell简介 2.1 Shell版本家族 常见的shell有Bourne shell(简称sh),在Sun里默认的C shell(简称csh)等。Linux使用的版本是Bourne Again Shell(简称bash)，它是Bourne shell的增强版本。 2.2 Linux中的Bash shell 使用man bash命令可以查看Bash shell的说明文档。用户在登录linux时，系统会分配一个shell用来工作，这个shell记录在/etc/passwd中。此外，/etc/shells文件中记录了当前可以使用的shell种类。作者在ubuntu16.04.5上查看的结果如下： 1234/bin/sh # 已经被/bin/bash所替换/bin/dash/bin/bash # Linux默认的shell/bin/rbash 3. Shell作为一个C语言程序 以Linux系统中Bash shell为例，主要有下面的优点： 历史命令(history)：历史命令保存在用户家目录~/.bash_history文件中。需要注意该文件记录的是这次登陆之前执行过的命令，这次登录执行过得命令缓存在内存中，注销系统后才会被保存。 命令与文件补全功能(Tab按键) :不仅能少打字，还能确保你的输入是正确的。 命令别名设置(alias) :在命令行输入alias就能查看当前命令的别名，这些别名保存在用户家目录~/.bashrc文件中。 任务管理、前台、后台控制(job control,foreground,background) :可以随时将任务丢到后台中执行，防止不小心使用ctrl+c停止程序。 通配符(Wildcard) :bash支持许多通配符来帮助用户查询与执行命令。 4. Shell作为命令语言和命令解释器 4.1 Bash shell的内置命令 Linux命令分为两种，一种是shell内置命令，一种是外部命名。使用type命令可以查看。 4.2 命令的执行 命令太长时，可以使用反斜杠\\对[Enter]键进行转义，注意两者中间不能有其他字符 4.3 命令的快速编辑 一些方便的命令快速编辑按钮 1234ctrl+u: 从光标处向前删除命令串ctrl+k: 从光标处向后删除命令串ctrl+a: 让光标移动到命令串最前面ctrl+e: 让光标移动到命令串最后面 5. Shell作为程序设计语言 Shell的优点是可以作为一种程序设计语言，通过编写程序化脚本(shell scripts)，可以将需要连续执行的命令写成一个shell脚本文件来执行。shell脚本主要有2中运行方式： 5.1 脚本作为可执行程序 假设有一个test.sh的shell脚本文件，存放在path目录下，则执行方法为： 123cd path # 切换至脚本所在目录pathchmod +x ./test.sh # 使脚本具有执行权限./test.sh # 执行脚本，不能写成test.sh，会被shell当成命令来查找 5.2 脚本作为解释器参数 这种方式是直接运行解释器，文件名作为参数，如/bin/sh test.sh 6. Shell作为用户与内核交互操作的接口 6.1 为什么要有Shell？ Shell的存在是有很多好处的。假设在同福客栈里没有了接待员老白，让你自己去操作点菜机，你不知道如何操作点菜机怎么办？你点了一道不存在的菜怎么办？ 6.2 为什么有多种Shell？ 假设同福客栈偶尔还接待外宾，点菜的时候说的是外语。而接待员老白不会说外语，那么我们是不是就需要另一个服务员（另一种Shell）了。不同的Shell是为了应对应用程序有不同的需求。 6.3 为什么要有系统调用？ 6.4 为什么要有内核？","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://holdfire.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://holdfire.github.io/tags/Linux/"}]},{"title":"Python——虚拟环境","slug":"python-virtual-environment","date":"2019-10-26T09:00:10.000Z","updated":"2019-11-08T08:33:40.158Z","comments":true,"path":"2019/10/26/python-virtual-environment/","link":"","permalink":"http://holdfire.github.io/2019/10/26/python-virtual-environment/","excerpt":"","text":"1. Python虚拟环境简介 Python中所有的第三方包都会被pip安装到site-packages目录下。对于某个特定的Python包，在项目A中可能需要使用1.0版的，而项目B中需要使用1.1版的。如何解决这种冲突呢？ 解决方案是：创建两个不同的虚拟环境，这两个虚拟环境相互独立、互不干扰。不同的项目就可以分别在各自的虚拟环境中独立运行啦，完美！（引申：不同虚拟环境的包是独立的，那它们使用的Python解释器是共享的还是独立的呢？） 那么如何创建虚拟环境呢？接下来就总结一下几种常用的虚拟环境管理工具。 2. 几种虚拟环境管理工具 2.1 virtualenv工具 virtualenv工具在安装Anaconda时有自带，位于安装目录/Lib/site-packages目录下。如果没有安装，可以在终端命令行中输入pip install virtualenv命令安装。 创建虚拟环境： 12345mkdir my_project # 创建项目文件夹cd my_project # 进入项目路径下virtualenv env_name # 使用默认的python解释器创建虚拟环境virtualenv -p path env_name # 使用path路径下的python解释器创建虚拟环境，Linux下path通常为/usr/bin/python,Windows中如e:/anaconda/python.exe # 使用virtualenv -h 可查看创建虚拟环境时的其他参数 操作虚拟环境： 1234567# Linux下操作虚拟环境source env_name/bin/activate # 激活该虚拟环境，之后所有的操作都在该环境中deactivate # 退出该虚拟环境rm -rf env_name # 删除该虚拟环境# Windows下操作虚拟环境，退出和删除操作同上cd env_name/Scripts/ # 进入到activate文件所在的目录activate # 执行activate文件，激活该虚拟环境 2.2 virtualenvwrapper工具 用户可以使用virtualenv工具在系统的任意地方创建虚拟环境。但下次需要激活这个环境时仍需要使用source env_path/activate命令，然而同学你可能早已忘记虚拟环境的路径env_path。 为了让用户更方便使用，我们可以把虚拟环境集中进行管理，virtualenvwrapper工具应运而生。在使用之前，你得先安装了virtualenv，然后再安装virtualenvwrapper。安装命令： 123456789101112131415161718pip install virtualenvwrapper # Linux下安装命令pip install virtualenvwrapper-win # Windows下安装命令``` &amp;emsp;&amp;emsp;不想写了。。。还要自己一遍遍测试代码好麻烦。。。反正我也不用这个工具。。。先放着。。。后面用到了再写！！！##### 2.3 使用pipenv&amp;emsp;&amp;emsp;pipenv能够有效地管理虚拟环境和包，相当于virtualenv和pip的合体。使用pipenv成功安装虚拟环境后，会生成两个文件Pipfile和Pipfile.lock。前者是TOML格式（这是啥？）的，不用管子依赖包，子依赖包在pipenv install package的时候自动安装或更新。后者是json格式的，它包含了所有子依赖包的确定版本```shellpip install pipenv # 安装pipenvmkdir my_project # 创建项目文件夹cd my_project # 进入文件夹下# 创键虚拟环境，下面两种方式二选一pipenv install # 使用本地默认的python创建虚拟环境pipenv --python 3.6 # 指定Python版本，我没试过pipenv shell # 激活虚拟环境pipenv install flask # 然后就可以愉快地安装各种包啦pip list # 这个命令可以查看当前虚拟环境安装的所有包pipenv graph # 可以查看安装包的依赖关系 2.4 使用conda conda是一个开源的软件包和环境管理系统，结合了pip和virtualenv两者的功能。conda是一个通用包管理器，不但可以安装python包，还可以安装其他语言的包。 123456789101112131415# 创建虚拟环境，指定python版本，指不指定pip好像都可以conda create -n venv_name python=3.7 conda create -n venv_name pip python=3.7 conda env list # 可查看当前存在的虚拟环境# linux下激活和退出虚拟环境source activate venv_name source deactivate# windows下激活和退出虚拟环境conda activate venv_name conda deactivate# 安装和管理包，使用conda或者pip都可以conda install numpy # 没试过pip install numpy# 删除虚拟环境conda remove -n venv_name --all","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://holdfire.github.io/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://holdfire.github.io/tags/Python/"}]},{"title":"分类模型——支持向量机SVM","slug":"ml-SVM","date":"2019-10-25T15:43:05.000Z","updated":"2019-11-06T12:01:31.439Z","comments":true,"path":"2019/10/25/ml-SVM/","link":"","permalink":"http://holdfire.github.io/2019/10/25/ml-SVM/","excerpt":"","text":"1. 支持向量机简介 支持向量机(Support Vector Machine, SVM)是一种二分类模型。其分类决策函数为： f(x)=sign(wT⋅x+b)(1)\\tag{1} f(x)=sign(w^T \\cdot x+b) f(x)=sign(wT⋅x+b)(1) 线性可分支持向量机学习的最优化问题为： min⁡w,b 12∥w∥2(2)\\tag{2}\\min_{w,b}\\ \\ \\ \\frac {1} {2} \\begin{Vmatrix} w \\end{Vmatrix}^2 w,bmin​ 21​∥∥​w​∥∥​2(2) s. t. yi(w⋅xi+b)−1 ≥0 i=1,2,...,N(3)\\tag{3} s.\\ t.\\ \\ \\ y_i(\\bm w\\cdot \\bm x_i+b)-1\\ \\ge 0\\ \\ \\ \\ \\ i=1,2,...,N s. t. yi​(w⋅xi​+b)−1 ≥0 i=1,2,...,N(3) 对每一个不等式约束引入拉格朗日乘子αi≥0, i=1,2,...,N\\alpha _i \\ge 0,\\ \\ i=1,2,...,Nαi​≥0, i=1,2,...,N后构造拉格朗日泛函L(w,b,α)L(\\bm w,b,\\bm \\alpha )L(w,b,α)，则上述优化问题等价为： min⁡w,b max⁡α L(w,b,α)=12∥w∥2−∑i=1Nαi[yi(w⋅xi+b)−1](4)\\tag{4} \\min_{\\bm w,b}\\ \\max_{\\bm \\alpha}\\ L(\\bm w,b,\\bm \\alpha )=\\frac 1 2\\begin{Vmatrix} w \\end{Vmatrix}^2-\\sum_{i=1} ^N \\alpha _i [y_i(\\bm w\\cdot \\bm x_i+b)-1] w,bmin​ αmax​ L(w,b,α)=21​∥∥​w​∥∥​2−i=1∑N​αi​[yi​(w⋅xi​+b)−1](4) 在上式的鞍点处，目标函数L(w,b,α)L(\\bm w,b,\\bm \\alpha )L(w,b,α)对w\\bm ww和bbb的偏导数为零。由此得到，在最优解处满足： w∗=∑i=1Nαiyixi(5)\\tag{5} \\bm w^*= \\sum_{i=1} ^N \\alpha _i y_i \\bm x _i w∗=i=1∑N​αi​yi​xi​(5) ∑i=1nαi∗yi=0(6)\\tag{6} \\sum_{i=1} ^n {\\alpha _i}^* y_i = 0 i=1∑n​αi​∗yi​=0(6) 将上面两个条件代入拉格朗日泛函中，则原问题的解等价于下面对偶问题的解： max⁡α Q(α)=∑i=1Nαi−12∑i=1N∑j=1Nαiαjyiyj(xi⋅xj)(7)\\tag{7}\\max_{\\bm \\alpha}\\ \\ Q(\\bm \\alpha)=\\sum_{i=1} ^N \\alpha _i -\\frac 1 2 \\sum_{i=1} ^N \\sum_{j=1} ^N \\alpha _i \\alpha _j y_i y_j(\\bm x_i \\cdot \\bm x_j) αmax​ Q(α)=i=1∑N​αi​−21​i=1∑N​j=1∑N​αi​αj​yi​yj​(xi​⋅xj​)(7) s. t. ∑i=1Nαiyi=0(8)\\tag{8} s.\\ t.\\ \\ \\ \\ \\sum_{i=1} ^N \\alpha _i y_i=0 s. t. i=1∑N​αi​yi​=0(8) αi≥0 i=1,2,...,N(9)\\tag{9} \\alpha _i \\ge 0\\ \\ i=1,2,...,N αi​≥0 i=1,2,...,N(9) 通过对偶问题的解αi∗, i=1,2,...,N\\alpha _i^*,\\ i=1,2,...,Nαi∗​, i=1,2,...,N，代入上式（5）中可以求出原问题的解w∗\\bm w^*w∗，接下看如何求解b。 根据最优化理论中的KKT条件，如果原问题的最优解为对偶问题的最优解，需要满足： αi[yi(w⋅xi+b)−1]=0(10)\\tag{10}\\alpha _i [y_i(\\bm w\\cdot \\bm x_i+b)-1]=0 αi​[yi​(w⋅xi​+b)−1]=0(10) 支持向量对应的αi\\alpha _iαi​是大于0的，因而对于这些点有： yi(w⋅xi+b)−1=0(11)\\tag{11} y_i(\\bm w\\cdot \\bm x_i+b)-1=0 yi​(w⋅xi​+b)−1=0(11) 因为已经求出了w∗\\bm w^*w∗,所以b∗b^*b∗可以用任何支持向量根据上式求得。实际数值计算中，人们通常采用所有非求解αi\\alpha _iαi​非零的样本求解b∗b^*b∗再取平均值。 2. 线性支持向量机与软间隔最大化 3. 非线性支持向量机与核函数","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://holdfire.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"监督学习","slug":"监督学习","permalink":"http://holdfire.github.io/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"网站出生的第一天","slug":"my_first_blog","date":"2019-10-25T10:51:19.000Z","updated":"2019-10-25T19:00:16.314Z","comments":true,"path":"2019/10/25/my_first_blog/","link":"","permalink":"http://holdfire.github.io/2019/10/25/my_first_blog/","excerpt":"","text":"1. 网站简介 这是holdfire于2019年10月25日创建的个人博客网站。创建步骤： 首先，在个人电脑上安装node.js(一种运行在服务端的 JavaScript，包含环境变量及npm的安装)； 然后，安装个人博客网站框架Hexo(在cmd中使用npm install -g hexo-cli等)； 接下来，创建一个新文件夹，执行hexo init,初始化自己的博客网站。用hexo s命令可以运行该网站。就可以在浏览器中访问啦！ 最后，在站点配置文档_config.yml中，设置你的github仓库地址，将网站内容托管在github上，就可以通过域名访问啦！ 补充，可以在hexo官网中下载各种自己喜欢的网站主题，然后修改主题配置文档_config.yml即可。","categories":[{"name":"软件安装","slug":"软件安装","permalink":"http://holdfire.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"web前端","slug":"web前端","permalink":"http://holdfire.github.io/tags/web%E5%89%8D%E7%AB%AF/"}]}]}