<!DOCTYPE html>
<html>
<head>
    

    

    



    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    
    
    
    <title>机器学习——激活函数 | 学无止境 | 不忘初心</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="机器学习">
    <meta name="description" content="1.简介  1.1激活函数是什么   在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。如下图所示：  1.2激活函数的作用   如果不使用激活函数（相当于激活函数是f(x)=x)，每一层节点的输出都是输入的线性函数。无论神经网络有多少层，神经网络输出层都是输入层的线性组合，神经网络相当于一个感知机(perceptron)，网络的拟合能力">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习——激活函数">
<meta property="og:url" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;index.html">
<meta property="og:site_name" content="学无止境">
<meta property="og:description" content="1.简介  1.1激活函数是什么   在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。如下图所示：  1.2激活函数的作用   如果不使用激活函数（相当于激活函数是f(x)=x)，每一层节点的输出都是输入的线性函数。无论神经网络有多少层，神经网络输出层都是输入层的线性组合，神经网络相当于一个感知机(perceptron)，网络的拟合能力">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;8B6637324C144A04B452F206E47ED1A9">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;B5829748B50D4CD395A164702E26CA17">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;D0CEA84C47E0407CAC66F19606F16C60">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;56488FB0850042D29A9D1E05ABD04407">
<meta property="og:updated_time" content="2019-10-30T19:15:30.783Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;8B6637324C144A04B452F206E47ED1A9">
    
        <link rel="alternate" type="application/atom+xml" title="学无止境" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu"  >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">holdfire</h5>
          <a href="mailto:liuxing16@tsinghua.org.cn" target="_blank" rel="noopener" title="liuxing16@tsinghua.org.cn" class="mail">liuxing16@tsinghua.org.cn</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                动态
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/holdfire" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="http://www.weibo.com/holdfireLX" target="_blank" >
                <i class="icon icon-lg icon-weibo"></i>
                微博
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/custom"  >
                <i class="icon icon-lg icon-link"></i>
                关于
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">机器学习——激活函数</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">机器学习——激活函数</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-10-30T18:24:41.000Z" itemprop="datePublished" class="page-time">
  2019-10-31
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1简介"><span class="post-toc-number">1.</span> <span class="post-toc-text"> 1.简介</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#11激活函数是什么"><span class="post-toc-number">1.1.</span> <span class="post-toc-text"> 1.1激活函数是什么</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#12激活函数的作用"><span class="post-toc-number">1.2.</span> <span class="post-toc-text"> 1.2激活函数的作用</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#13常用的激活函数"><span class="post-toc-number">1.3.</span> <span class="post-toc-text"> 1.3常用的激活函数：</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-各激活函数的特点"><span class="post-toc-number">2.</span> <span class="post-toc-text"> 2. 各激活函数的特点</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#21-sigmoid函数"><span class="post-toc-number">2.1.</span> <span class="post-toc-text"> 2.1 sigmoid函数</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#特点"><span class="post-toc-number">2.2.</span> <span class="post-toc-text"> 特点：</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#缺点1反向传播过程产生梯度消失或梯度爆炸"><span class="post-toc-number">2.3.</span> <span class="post-toc-text"> 缺点1：反向传播过程产生梯度消失或梯度爆炸</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#缺点2sigmoid函数的输出不是0均值zero-centered"><span class="post-toc-number">2.4.</span> <span class="post-toc-text"> 缺点2：sigmoid函数的输出不是0均值(zero-centered)</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#22-tanh函数"><span class="post-toc-number">2.5.</span> <span class="post-toc-text"> 2.2 tanh函数</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#23-relu函数"><span class="post-toc-number">2.6.</span> <span class="post-toc-text"> 2.3 ReLU函数</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#24-leaky-relu函数"><span class="post-toc-number">2.7.</span> <span class="post-toc-text"> 2.4 Leaky ReLU函数</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#25-其他激活函数"><span class="post-toc-number">3.</span> <span class="post-toc-text"> 2.5 其他激活函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3激活函数的选择"><span class="post-toc-number">4.</span> <span class="post-toc-text"> 3.激活函数的选择</span></a></li></ol>
        </nav>
    </aside>


<article id="post-machine-learning-activation-function"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">机器学习——激活函数</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-10-31 02:24:41" datetime="2019-10-30T18:24:41.000Z"  itemprop="datePublished">2019-10-31</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h4 id="1简介"><a class="markdownIt-Anchor" href="#1简介"></a> 1.简介</h4>
<h5 id="11激活函数是什么"><a class="markdownIt-Anchor" href="#11激活函数是什么"></a> 1.1激活函数是什么</h5>
<p>  在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。如下图所示：</p>
<h5 id="12激活函数的作用"><a class="markdownIt-Anchor" href="#12激活函数的作用"></a> 1.2激活函数的作用</h5>
<p>  如果不使用激活函数（相当于激活函数是<code>f(x)=x</code>)，每一层节点的输出都是输入的线性函数。无论神经网络有多少层，神经网络输出层都是输入层的线性组合，神经网络相当于一个感知机(perceptron)，网络的拟合能力就有限，因而，引入非线性的激活函数能够增强神经网络的表达能力，使得神经网络能够逼近任意函数。</p>
<h5 id="13常用的激活函数"><a class="markdownIt-Anchor" href="#13常用的激活函数"></a> 1.3常用的激活函数：</h5>
<p>sigmoid,relu,tanh;<br />
leaky_relu,erelu,maxout;</p>
<h4 id="2-各激活函数的特点"><a class="markdownIt-Anchor" href="#2-各激活函数的特点"></a> 2. 各激活函数的特点</h4>
<h5 id="21-sigmoid函数"><a class="markdownIt-Anchor" href="#21-sigmoid函数"></a> 2.1 sigmoid函数</h5>
<p>  sigmoid是应用最为广泛的激活函数之一，函数形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\sigma(x) = \frac &#123;1&#125;&#123;1+e^&#123;-x&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>函数的图像及其导数的图像如下：</p>
<h5 id="特点"><a class="markdownIt-Anchor" href="#特点"></a> <mark>特点：</mark></h5>
<p>  sigmoid函数能够把任意实数映射为<code>$(0,1)$</code>区间上的实数。当自变量值小于-5时，函数值接近于0；而当自变量大于5时，函数值非常接近于1。  sigmoid函数求导非常方便，其导数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>f</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">f^{&#x27;}(x)=f(x)\cdot[1-f(x)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.19248em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.94248em;"><span style="top:-2.94248em;margin-right:0.05em;"><span class="pstrut" style="height:2.57948em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>，x=0时函数的导数最大，为0.25。</p>
<h5 id="缺点1反向传播过程产生梯度消失或梯度爆炸"><a class="markdownIt-Anchor" href="#缺点1反向传播过程产生梯度消失或梯度爆炸"></a> <mark>缺点1：反向传播过程产生梯度消失或梯度爆炸</mark></h5>
<p>下文参考资料：<html><a href="http://neuralnetworksanddeeplearning.com/chap5.html" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap5.html</a></html><br />
  <strong>现象：</strong> 在深层网络中，不同隐含层的学习速度(各隐含层权值和偏差偏导组成的向量的范数)相差很大。靠后的隐含层学习速度较大，而靠前的隐含层经常在训练期间卡住，几乎什么都学习不到；也有时候早期的层可能学习很好，但后来的层卡住。<br />
  <strong>试验：</strong> 使用MNIST数据集进行图像分类任务，输入层神经元个数为784，隐含层神经元个数均为30，输出层神经元个数为10；使用sigmoid函数作为激活函数。训练图片为1000张，使用Batch Stostic Gradient Descend算法，500个epoch。使用不同数目的隐含层的分类结果如下：</p>
<table>
<thead>
<tr>
<th>隐含层数目</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>分类准确率</td>
<td>96.48%</td>
<td>96.90%</td>
<td>96.57%</td>
<td>96.53%</td>
</tr>
</tbody>
</table>
<p>  带有4个隐含层的网络，训练过程中各隐含层权重的学习速度变化如下图所示。从图中可以看出：在训练后期，第一个隐含层的学习速率比第四个隐含层慢了约100倍。</p>
<p>  <strong>分析：</strong> 直观上来说，额外的隐含层应该使网络能够学习更为复杂的分类功能，从而进行更好的分类。即使额外的隐含层什么都不做，模型的准确率也不会变得更差。由于权重是随机初始化的，因而第一个隐含层会丢失大量的图片有用信息，所以此时第一层几乎不可能不需要再学习，即还没有收敛到最优值。若假设额外的隐含层确实有用，那么问题应该是我们的学习算法没有找到合适的权重和偏差。<br />
  <strong>梯度消失的数学推导：</strong> 考虑一个每层只有1个神经元的深层神经网络，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">w_1,w_2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>为权重，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>b</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">b_1,b_2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>为偏置。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">a_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>为第j层的输出值，代价函数C对第一个隐含层的偏置<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的偏导为：<br />
<img src="8B6637324C144A04B452F206E47ED1A9" alt="tikz38" /><br />
  将神经网络的权值初始化为均值为0，标准差为1。则大部分权值满足<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo fence="true">∣</mo><msub><mi>w</mi><mi>j</mi></msub><mo fence="true">∣</mo></mrow><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left|w_j\right|&lt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，且有$\left|w_j \sigma^{\prime}(z_j)&lt;\frac {1} {4}\right| <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">。</mi><mi mathvariant="normal">由</mi><mi mathvariant="normal">反</mi><mi mathvariant="normal">向</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">播</mi><mi mathvariant="normal">算</mi><mi mathvariant="normal">法</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">数</mi><mi mathvariant="normal">学</mi><mi mathvariant="normal">推</mi><mi mathvariant="normal">导</mi><mi mathvariant="normal">可</mi><mi mathvariant="normal">知</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">从</mi><mi mathvariant="normal">后</mi><mi mathvariant="normal">向</mi><mi mathvariant="normal">前</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">播</mi><mi mathvariant="normal">时</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">每</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">递</mi><mi mathvariant="normal">一</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">都</mi><mi mathvariant="normal">会</mi><mi mathvariant="normal">减</mi><mi mathvariant="normal">小</mi><mi mathvariant="normal">为</mi><mi mathvariant="normal">原</mi><mi mathvariant="normal">来</mi><mi mathvariant="normal">的</mi><mn>0.25</mn><mi mathvariant="normal">倍</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">如</mi><mi mathvariant="normal">果</mi><mi mathvariant="normal">神</mi><mi mathvariant="normal">经</mi><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">隐</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">特</mi><mi mathvariant="normal">别</mi><mi mathvariant="normal">多</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">那</mi><mi mathvariant="normal">么</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">在</mi><mi mathvariant="normal">穿</mi><mi mathvariant="normal">过</mi><mi mathvariant="normal">多</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">后</mi><mi mathvariant="normal">将</mi><mi mathvariant="normal">变</mi><mi mathvariant="normal">得</mi><mi mathvariant="normal">非</mi><mi mathvariant="normal">常</mi><mi mathvariant="normal">小</mi><mi mathvariant="normal">接</mi><mi mathvariant="normal">近</mi><mi mathvariant="normal">于</mi><mn>0</mn><mi mathvariant="normal">，</mi><mi mathvariant="normal">即</mi><mi mathvariant="normal">出</mi><mi mathvariant="normal">现</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">消</mi><mi mathvariant="normal">失</mi><mi mathvariant="normal">现</mi><mi mathvariant="normal">象</mi><mi mathvariant="normal">；</mi><mi mathvariant="normal">当</mi><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">权</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">初</mi><mi mathvariant="normal">始</mi><mi mathvariant="normal">化</mi><mi mathvariant="normal">为</mi></mrow><annotation encoding="application/x-tex">。由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">由</span><span class="mord cjk_fallback">反</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">播</span><span class="mord cjk_fallback">算</span><span class="mord cjk_fallback">法</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">学</span><span class="mord cjk_fallback">推</span><span class="mord cjk_fallback">导</span><span class="mord cjk_fallback">可</span><span class="mord cjk_fallback">知</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">从</span><span class="mord cjk_fallback">后</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">前</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">播</span><span class="mord cjk_fallback">时</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">每</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">递</span><span class="mord cjk_fallback">一</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">都</span><span class="mord cjk_fallback">会</span><span class="mord cjk_fallback">减</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">为</span><span class="mord cjk_fallback">原</span><span class="mord cjk_fallback">来</span><span class="mord cjk_fallback">的</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span><span class="mord cjk_fallback">倍</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">如</span><span class="mord cjk_fallback">果</span><span class="mord cjk_fallback">神</span><span class="mord cjk_fallback">经</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">隐</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">别</span><span class="mord cjk_fallback">多</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">那</span><span class="mord cjk_fallback">么</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">在</span><span class="mord cjk_fallback">穿</span><span class="mord cjk_fallback">过</span><span class="mord cjk_fallback">多</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">后</span><span class="mord cjk_fallback">将</span><span class="mord cjk_fallback">变</span><span class="mord cjk_fallback">得</span><span class="mord cjk_fallback">非</span><span class="mord cjk_fallback">常</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">接</span><span class="mord cjk_fallback">近</span><span class="mord cjk_fallback">于</span><span class="mord">0</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">即</span><span class="mord cjk_fallback">出</span><span class="mord cjk_fallback">现</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">消</span><span class="mord cjk_fallback">失</span><span class="mord cjk_fallback">现</span><span class="mord cjk_fallback">象</span><span class="mord cjk_fallback">；</span><span class="mord cjk_fallback">当</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">权</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">初</span><span class="mord cjk_fallback">始</span><span class="mord cjk_fallback">化</span><span class="mord cjk_fallback">为</span></span></span></span>(1,+\infty)<span class='katex-error' title='ParseError: KaTeX parse error: Expected &#039;EOF&#039;, got &#039;&amp;&#039; at position 21: …，则会出现梯度爆炸情况。  
&amp;̲emsp;&amp;emsp;**梯度…'>区间内的值，则会出现梯度爆炸情况。  
&amp;emsp;&amp;emsp;**梯度爆炸的产生：**   当权重设置很大时，如</span>w_1=w_2=w_3=w_4=1000<span class='katex-error' title='ParseError: KaTeX parse error: Expected &#039;EOF&#039;, got &#039;&amp;&#039; at position 23: …0时，会产生梯度爆炸。   
&amp;̲emsp;&amp;emsp;**思考…'>,而z的值接近0时，会产生梯度爆炸。   
&amp;emsp;&amp;emsp;**思考：** 为了避免梯度消失，我们可以通过设置合适</span>w_j<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">的</mi><mi mathvariant="normal">初</mi><mi mathvariant="normal">始</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">使</mi><mi mathvariant="normal">得</mi></mrow><annotation encoding="application/x-tex">的初始值，使得</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">初</span><span class="mord cjk_fallback">始</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">使</span><span class="mord cjk_fallback">得</span></span></span></span>\left|w_j \sigma^{\prime}(z_j) \right|&gt;1<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">。</mi><mi mathvariant="normal">事</mi><mi mathvariant="normal">实</mi><mi mathvariant="normal">上</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">这</mi><mi mathvariant="normal">非</mi><mi mathvariant="normal">常</mi><mi mathvariant="normal">困</mi><mi mathvariant="normal">难</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">因</mi><mi mathvariant="normal">为</mi></mrow><annotation encoding="application/x-tex">。事实上，这非常困难，因为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">事</span><span class="mord cjk_fallback">实</span><span class="mord cjk_fallback">上</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">这</span><span class="mord cjk_fallback">非</span><span class="mord cjk_fallback">常</span><span class="mord cjk_fallback">困</span><span class="mord cjk_fallback">难</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">因</span><span class="mord cjk_fallback">为</span></span></span></span>\sigma<sup>{\prime}(z_j)=\sigma</sup>{\prime}(wa+b) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">的</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">是</mi><mi mathvariant="normal">依</mi><mi mathvariant="normal">赖</mi><mi mathvariant="normal">于</mi><mi>w</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">当</mi><mi>w</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">较</mi><mi mathvariant="normal">大</mi><mi mathvariant="normal">时</mi><mi mathvariant="normal">，</mi></mrow><annotation encoding="application/x-tex">的值是依赖于w的，当w的值较大时，</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">是</span><span class="mord cjk_fallback">依</span><span class="mord cjk_fallback">赖</span><span class="mord cjk_fallback">于</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">当</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">较</span><span class="mord cjk_fallback">大</span><span class="mord cjk_fallback">时</span><span class="mord cjk_fallback">，</span></span></span></span>\sigma^{\prime}(z_j)$通常会很小。梯度消失很难避免。<br />
  <strong>总结：</strong> 深层神经网络的反向传播过程会发生梯度变化不稳定的状况。通过使用其他的激活函数，我们能否避免这种梯度下降不稳定的现象呢？<br />
  <strong>相关文献：</strong> 2010年Glorot和Bengio发文表明sigmoid函数在深层神经网络中，最后的隐含层在训练中会产生饱和，其值接近0，作者建议使用其他的激活函数。<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a></p>
<h5 id="缺点2sigmoid函数的输出不是0均值zero-centered"><a class="markdownIt-Anchor" href="#缺点2sigmoid函数的输出不是0均值zero-centered"></a> <mark>缺点2：sigmoid函数的输出不是0均值(zero-centered)</mark></h5>
<p>  这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">x&gt;0,f(x)=w^{T}x+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。</p>
<h5 id="22-tanh函数"><a class="markdownIt-Anchor" href="#22-tanh函数"></a> 2.2 tanh函数</h5>
<p>  tanh函数的形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tanh(x)=\frac&#123;e^&#123;x&#125;-e^&#123;-x&#125;&#125; &#123;e^&#123;x&#125;+e^&#123;-x&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>  tanh函数及其导数的图像如下所示：<br />
<img src="B5829748B50D4CD395A164702E26CA17" alt="2018041517590341" /><br />
  <strong>特点：</strong> 解决了sigmoid函数的非零均值问题，但梯度消失的问题仍然没有解决。</p>
<h5 id="23-relu函数"><a class="markdownIt-Anchor" href="#23-relu函数"></a> 2.3 ReLU函数</h5>
<p>  ReLU(Rectified Linear Unit)函数的基本形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Relu(x) = max(0,x)</span><br></pre></td></tr></table></figure>
<p>  ReLU函数及其导数的图像如下所示：<br />
<img src="D0CEA84C47E0407CAC66F19606F16C60" alt="relu" /><br />
  <strong>优点：</strong> ReLU函数解决了梯度消失的问题；计算速度快；收敛速度远快于sigmoid函数和tanh函数。<br />
  <strong>缺点：</strong> ReLU函数的输出不是零均值；Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 参数初始化不当，负的权值太多，这种情况比较少见； (2) 学习率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</p>
<h5 id="24-leaky-relu函数"><a class="markdownIt-Anchor" href="#24-leaky-relu函数"></a> 2.4 Leaky ReLU函数</h5>
<p>  Leaky ReLU函数的形式如下，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>是一个非常小的正数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Leaky ReLU(X) = max(\alpha x, x)</span><br></pre></td></tr></table></figure>
<p>  Leaky ReLU函数及其导数的图像如下所示：<br />
<img src="56488FB0850042D29A9D1E05ABD04407" alt="leakyrelu" /><br />
  <strong>特点：</strong> 为了解决Dead ReLU Problem，将ReLU的前半段设为αx 而非0，通常α=0.01。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h4 id="25-其他激活函数"><a class="markdownIt-Anchor" href="#25-其他激活函数"></a> 2.5 其他激活函数</h4>
<p>  如ELU(Exponential Linear Units)函数，Maxout函数</p>
<h4 id="3激活函数的选择"><a class="markdownIt-Anchor" href="#3激活函数的选择"></a> 3.激活函数的选择</h4>
<p>  (1)深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。<br />
  (2）如果使用 ReLU，那么一定要小心设置 learning rate， 而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.<br />
  (3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2019-10-30T19:15:30.783Z" itemprop="dateUpdated">2019-10-31 03:15:30</time>
</span><br>


        
        本人才疏学浅，内容如有错误，敬请批评指正！内容系原创，转载请邮件联系作者！本文链接：<a href="/2019/10/31/machine-learning-activation-function/" target="_blank" rel="external">http://holdfire.github.io/2019/10/31/machine-learning-activation-function/</a>
        
    </div>
    
    <footer>
        <a href="http://holdfire.github.io">
            <img src="/img/avatar.jpg" alt="holdfire">
            holdfire
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/&title=《机器学习——激活函数》 — 学无止境&pic=http://holdfire.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/&title=《机器学习——激活函数》 — 学无止境&source=一只梦想遨游天际的菜鸟！" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习——激活函数》 — 学无止境&url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/&via=http://holdfire.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/10/31/machine-learning-optimization-algorithm-1/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">机器学习——一阶优化算法</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/10/30/python-package-module-import/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Python——导入操作import</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'false' == 'true',
            verify: 'false' == 'true',
            appId: "y3U6QFDMxRlh8fDNs1qJl7su-gzGzoHsz",
            appKey: "pssOSm45PFn6XJO82i2Nkb20",
            avatar: "mm",
            placeholder: "这是写评论的地方...",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "5"
        })
    </script>
    <!-- Valine Comments end -->










</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>holdfire &copy; 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/&title=《机器学习——激活函数》 — 学无止境&pic=http://holdfire.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/&title=《机器学习——激活函数》 — 学无止境&source=一只梦想遨游天际的菜鸟！" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习——激活函数》 — 学无止境&url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/&via=http://holdfire.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=http://holdfire.github.io/2019/10/31/machine-learning-activation-function/" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '无止尽...';
            clearTimeout(titleTime);
        } else {
            document.title = '真的无止尽!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
