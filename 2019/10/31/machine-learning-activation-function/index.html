<!DOCTYPE html>
<html lang=zh>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>机器学习——激活函数 | holdfire</title>
  <meta name="description" content="1.简介  1.1激活函数是什么   在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。如下图所示：  1.2激活函数的作用   如果不使用激活函数（相当于激活函数是f(x)=x)，每一层节点的输出都是输入的线性函数。无论神经网络有多少层，神经网络输出层都是输入层的线性组合，神经网络相当于一个感知机(perceptron)，网络的拟合能力">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习——激活函数">
<meta property="og:url" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1.简介  1.1激活函数是什么   在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。如下图所示：  1.2激活函数的作用   如果不使用激活函数（相当于激活函数是f(x)=x)，每一层节点的输出都是输入的线性函数。无论神经网络有多少层，神经网络输出层都是输入层的线性组合，神经网络相当于一个感知机(perceptron)，网络的拟合能力">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;8B6637324C144A04B452F206E47ED1A9">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;B5829748B50D4CD395A164702E26CA17">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;D0CEA84C47E0407CAC66F19606F16C60">
<meta property="og:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;56488FB0850042D29A9D1E05ABD04407">
<meta property="og:updated_time" content="2019-10-30T19:15:30.783Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;2019&#x2F;10&#x2F;31&#x2F;machine-learning-activation-function&#x2F;8B6637324C144A04B452F206E47ED1A9">
  <!-- Canonical links -->
  <link rel="canonical" href="http://holdfire.github.io/2019/10/31/machine-learning-activation-function/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
  
  
</head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/holdfire" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">holdfire</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">Computer Vision</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Beijing, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">书单</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">友链</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/holdfire" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://weibo.com/holdfireLX" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>本站开始接客啦！  -2019年10月25日</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/">软件安装</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/" rel="tag">C++</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web%E5%89%8D%E7%AB%AF/" rel="tag">web前端</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">5</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/C/" style="font-size: 14px;">C++</a> <a href="/tags/Linux/" style="font-size: 13.67px;">Linux</a> <a href="/tags/Python/" style="font-size: 13.33px;">Python</a> <a href="/tags/web%E5%89%8D%E7%AB%AF/" style="font-size: 13px;">web前端</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13.67px;">机器学习</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a><span class="archive-list-count">19</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled ">
        
          <li>
            
            <div class="item-thumb">
              <a href="/2019/10/31/machine-learning-optimization-algorithm-2/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
              </p>
              <p class="item-title">
                <a href="/2019/10/31/machine-learning-optimization-algorithm-2/" class="title">机器学习——二阶优化算法</a>
              </p>
              <p class="item-date">
                <time datetime="2019-10-30T18:54:23.000Z" itemprop="datePublished">2019-10-31</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2019/10/31/machine-learning-loss-function/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
              </p>
              <p class="item-title">
                <a href="/2019/10/31/machine-learning-loss-function/" class="title">机器学习——损失函数</a>
              </p>
              <p class="item-date">
                <time datetime="2019-10-30T18:29:07.000Z" itemprop="datePublished">2019-10-31</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2019/10/31/machine-learning-optimization-algorithm-1/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
              </p>
              <p class="item-title">
                <a href="/2019/10/31/machine-learning-optimization-algorithm-1/" class="title">机器学习——一阶优化算法</a>
              </p>
              <p class="item-date">
                <time datetime="2019-10-30T18:28:46.000Z" itemprop="datePublished">2019-10-31</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2019/10/31/machine-learning-activation-function/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
              </p>
              <p class="item-title">
                <a href="/2019/10/31/machine-learning-activation-function/" class="title">机器学习——激活函数</a>
              </p>
              <p class="item-date">
                <time datetime="2019-10-30T18:24:41.000Z" itemprop="datePublished">2019-10-31</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-thumb">
              <a href="/2019/10/30/python-package-module-import/" class="thumb">
    
    
        <span class="thumb-image thumb-none"></span>
    
</a>

            </div>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言</a>
              </p>
              <p class="item-title">
                <a href="/2019/10/30/python-package-module-import/" class="title">Python——导入操作import</a>
              </p>
              <p class="item-date">
                <time datetime="2019-10-30T11:19:53.000Z" itemprop="datePublished">2019-10-30</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-machine-learning-activation-function" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      机器学习——激活函数
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2019/10/31/machine-learning-activation-function/" class="article-date">
	  <time datetime="2019-10-30T18:24:41.000Z" itemprop="datePublished">2019-10-31</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
  </span>


        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill" aria-hidden="true"></i>
	    <span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv">0</span>
		</span>
	</span>


        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2019/10/31/machine-learning-activation-function/#comments" class="article-comment-link">评论</a></span>
        
	
		<span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 2.4k(字)</span>
	
	
		<span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 9(分)</span>
	

      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h4 id="1简介"><a class="markdownIt-Anchor" href="#1简介"></a> 1.简介</h4>
<h5 id="11激活函数是什么"><a class="markdownIt-Anchor" href="#11激活函数是什么"></a> 1.1激活函数是什么</h5>
<p>  在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。如下图所示：</p>
<h5 id="12激活函数的作用"><a class="markdownIt-Anchor" href="#12激活函数的作用"></a> 1.2激活函数的作用</h5>
<p>  如果不使用激活函数（相当于激活函数是<code>f(x)=x</code>)，每一层节点的输出都是输入的线性函数。无论神经网络有多少层，神经网络输出层都是输入层的线性组合，神经网络相当于一个感知机(perceptron)，网络的拟合能力就有限，因而，引入非线性的激活函数能够增强神经网络的表达能力，使得神经网络能够逼近任意函数。</p>
<h5 id="13常用的激活函数"><a class="markdownIt-Anchor" href="#13常用的激活函数"></a> 1.3常用的激活函数：</h5>
<p>sigmoid,relu,tanh;<br />
leaky_relu,erelu,maxout;</p>
<h4 id="2-各激活函数的特点"><a class="markdownIt-Anchor" href="#2-各激活函数的特点"></a> 2. 各激活函数的特点</h4>
<h5 id="21-sigmoid函数"><a class="markdownIt-Anchor" href="#21-sigmoid函数"></a> 2.1 sigmoid函数</h5>
<p>  sigmoid是应用最为广泛的激活函数之一，函数形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\sigma(x) = \frac &#123;1&#125;&#123;1+e^&#123;-x&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>函数的图像及其导数的图像如下：</p>
<h5 id="特点"><a class="markdownIt-Anchor" href="#特点"></a> <mark>特点：</mark></h5>
<p>  sigmoid函数能够把任意实数映射为<code>$(0,1)$</code>区间上的实数。当自变量值小于-5时，函数值接近于0；而当自变量大于5时，函数值非常接近于1。  sigmoid函数求导非常方便，其导数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>f</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">f^{&#x27;}(x)=f(x)\cdot[1-f(x)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.19248em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.94248em;"><span style="top:-2.94248em;margin-right:0.05em;"><span class="pstrut" style="height:2.57948em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>，x=0时函数的导数最大，为0.25。</p>
<h5 id="缺点1反向传播过程产生梯度消失或梯度爆炸"><a class="markdownIt-Anchor" href="#缺点1反向传播过程产生梯度消失或梯度爆炸"></a> <mark>缺点1：反向传播过程产生梯度消失或梯度爆炸</mark></h5>
<p>下文参考资料：<html><a href="http://neuralnetworksanddeeplearning.com/chap5.html" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/chap5.html</a></html><br />
  <strong>现象：</strong> 在深层网络中，不同隐含层的学习速度(各隐含层权值和偏差偏导组成的向量的范数)相差很大。靠后的隐含层学习速度较大，而靠前的隐含层经常在训练期间卡住，几乎什么都学习不到；也有时候早期的层可能学习很好，但后来的层卡住。<br />
  <strong>试验：</strong> 使用MNIST数据集进行图像分类任务，输入层神经元个数为784，隐含层神经元个数均为30，输出层神经元个数为10；使用sigmoid函数作为激活函数。训练图片为1000张，使用Batch Stostic Gradient Descend算法，500个epoch。使用不同数目的隐含层的分类结果如下：</p>
<table>
<thead>
<tr>
<th>隐含层数目</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>分类准确率</td>
<td>96.48%</td>
<td>96.90%</td>
<td>96.57%</td>
<td>96.53%</td>
</tr>
</tbody>
</table>
<p>  带有4个隐含层的网络，训练过程中各隐含层权重的学习速度变化如下图所示。从图中可以看出：在训练后期，第一个隐含层的学习速率比第四个隐含层慢了约100倍。</p>
<p>  <strong>分析：</strong> 直观上来说，额外的隐含层应该使网络能够学习更为复杂的分类功能，从而进行更好的分类。即使额外的隐含层什么都不做，模型的准确率也不会变得更差。由于权重是随机初始化的，因而第一个隐含层会丢失大量的图片有用信息，所以此时第一层几乎不可能不需要再学习，即还没有收敛到最优值。若假设额外的隐含层确实有用，那么问题应该是我们的学习算法没有找到合适的权重和偏差。<br />
  <strong>梯度消失的数学推导：</strong> 考虑一个每层只有1个神经元的深层神经网络，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">w_1,w_2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>为权重，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>b</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">b_1,b_2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>为偏置。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">a_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>为第j层的输出值，代价函数C对第一个隐含层的偏置<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的偏导为：<br />
<img src="8B6637324C144A04B452F206E47ED1A9" alt="tikz38" /><br />
  将神经网络的权值初始化为均值为0，标准差为1。则大部分权值满足<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo fence="true">∣</mo><msub><mi>w</mi><mi>j</mi></msub><mo fence="true">∣</mo></mrow><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left|w_j\right|&lt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，且有$\left|w_j \sigma^{\prime}(z_j)&lt;\frac {1} {4}\right| <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">。</mi><mi mathvariant="normal">由</mi><mi mathvariant="normal">反</mi><mi mathvariant="normal">向</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">播</mi><mi mathvariant="normal">算</mi><mi mathvariant="normal">法</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">数</mi><mi mathvariant="normal">学</mi><mi mathvariant="normal">推</mi><mi mathvariant="normal">导</mi><mi mathvariant="normal">可</mi><mi mathvariant="normal">知</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">从</mi><mi mathvariant="normal">后</mi><mi mathvariant="normal">向</mi><mi mathvariant="normal">前</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">播</mi><mi mathvariant="normal">时</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">每</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">递</mi><mi mathvariant="normal">一</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">都</mi><mi mathvariant="normal">会</mi><mi mathvariant="normal">减</mi><mi mathvariant="normal">小</mi><mi mathvariant="normal">为</mi><mi mathvariant="normal">原</mi><mi mathvariant="normal">来</mi><mi mathvariant="normal">的</mi><mn>0.25</mn><mi mathvariant="normal">倍</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">如</mi><mi mathvariant="normal">果</mi><mi mathvariant="normal">神</mi><mi mathvariant="normal">经</mi><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">隐</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">特</mi><mi mathvariant="normal">别</mi><mi mathvariant="normal">多</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">那</mi><mi mathvariant="normal">么</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">在</mi><mi mathvariant="normal">穿</mi><mi mathvariant="normal">过</mi><mi mathvariant="normal">多</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">后</mi><mi mathvariant="normal">将</mi><mi mathvariant="normal">变</mi><mi mathvariant="normal">得</mi><mi mathvariant="normal">非</mi><mi mathvariant="normal">常</mi><mi mathvariant="normal">小</mi><mi mathvariant="normal">接</mi><mi mathvariant="normal">近</mi><mi mathvariant="normal">于</mi><mn>0</mn><mi mathvariant="normal">，</mi><mi mathvariant="normal">即</mi><mi mathvariant="normal">出</mi><mi mathvariant="normal">现</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">消</mi><mi mathvariant="normal">失</mi><mi mathvariant="normal">现</mi><mi mathvariant="normal">象</mi><mi mathvariant="normal">；</mi><mi mathvariant="normal">当</mi><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">权</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">初</mi><mi mathvariant="normal">始</mi><mi mathvariant="normal">化</mi><mi mathvariant="normal">为</mi></mrow><annotation encoding="application/x-tex">。由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">由</span><span class="mord cjk_fallback">反</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">播</span><span class="mord cjk_fallback">算</span><span class="mord cjk_fallback">法</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">学</span><span class="mord cjk_fallback">推</span><span class="mord cjk_fallback">导</span><span class="mord cjk_fallback">可</span><span class="mord cjk_fallback">知</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">从</span><span class="mord cjk_fallback">后</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">前</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">播</span><span class="mord cjk_fallback">时</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">每</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">递</span><span class="mord cjk_fallback">一</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">都</span><span class="mord cjk_fallback">会</span><span class="mord cjk_fallback">减</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">为</span><span class="mord cjk_fallback">原</span><span class="mord cjk_fallback">来</span><span class="mord cjk_fallback">的</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span><span class="mord cjk_fallback">倍</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">如</span><span class="mord cjk_fallback">果</span><span class="mord cjk_fallback">神</span><span class="mord cjk_fallback">经</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">隐</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">别</span><span class="mord cjk_fallback">多</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">那</span><span class="mord cjk_fallback">么</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">在</span><span class="mord cjk_fallback">穿</span><span class="mord cjk_fallback">过</span><span class="mord cjk_fallback">多</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">后</span><span class="mord cjk_fallback">将</span><span class="mord cjk_fallback">变</span><span class="mord cjk_fallback">得</span><span class="mord cjk_fallback">非</span><span class="mord cjk_fallback">常</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">接</span><span class="mord cjk_fallback">近</span><span class="mord cjk_fallback">于</span><span class="mord">0</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">即</span><span class="mord cjk_fallback">出</span><span class="mord cjk_fallback">现</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">消</span><span class="mord cjk_fallback">失</span><span class="mord cjk_fallback">现</span><span class="mord cjk_fallback">象</span><span class="mord cjk_fallback">；</span><span class="mord cjk_fallback">当</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">权</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">初</span><span class="mord cjk_fallback">始</span><span class="mord cjk_fallback">化</span><span class="mord cjk_fallback">为</span></span></span></span>(1,+\infty)<span class='katex-error' title='ParseError: KaTeX parse error: Expected &#039;EOF&#039;, got &#039;&amp;&#039; at position 21: …，则会出现梯度爆炸情况。  
&amp;̲emsp;&amp;emsp;**梯度…'>区间内的值，则会出现梯度爆炸情况。  
&amp;emsp;&amp;emsp;**梯度爆炸的产生：**   当权重设置很大时，如</span>w_1=w_2=w_3=w_4=1000<span class='katex-error' title='ParseError: KaTeX parse error: Expected &#039;EOF&#039;, got &#039;&amp;&#039; at position 23: …0时，会产生梯度爆炸。   
&amp;̲emsp;&amp;emsp;**思考…'>,而z的值接近0时，会产生梯度爆炸。   
&amp;emsp;&amp;emsp;**思考：** 为了避免梯度消失，我们可以通过设置合适</span>w_j<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">的</mi><mi mathvariant="normal">初</mi><mi mathvariant="normal">始</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">使</mi><mi mathvariant="normal">得</mi></mrow><annotation encoding="application/x-tex">的初始值，使得</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">初</span><span class="mord cjk_fallback">始</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">使</span><span class="mord cjk_fallback">得</span></span></span></span>\left|w_j \sigma^{\prime}(z_j) \right|&gt;1<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">。</mi><mi mathvariant="normal">事</mi><mi mathvariant="normal">实</mi><mi mathvariant="normal">上</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">这</mi><mi mathvariant="normal">非</mi><mi mathvariant="normal">常</mi><mi mathvariant="normal">困</mi><mi mathvariant="normal">难</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">因</mi><mi mathvariant="normal">为</mi></mrow><annotation encoding="application/x-tex">。事实上，这非常困难，因为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">事</span><span class="mord cjk_fallback">实</span><span class="mord cjk_fallback">上</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">这</span><span class="mord cjk_fallback">非</span><span class="mord cjk_fallback">常</span><span class="mord cjk_fallback">困</span><span class="mord cjk_fallback">难</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">因</span><span class="mord cjk_fallback">为</span></span></span></span>\sigma<sup>{\prime}(z_j)=\sigma</sup>{\prime}(wa+b) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">的</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">是</mi><mi mathvariant="normal">依</mi><mi mathvariant="normal">赖</mi><mi mathvariant="normal">于</mi><mi>w</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">当</mi><mi>w</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">较</mi><mi mathvariant="normal">大</mi><mi mathvariant="normal">时</mi><mi mathvariant="normal">，</mi></mrow><annotation encoding="application/x-tex">的值是依赖于w的，当w的值较大时，</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">是</span><span class="mord cjk_fallback">依</span><span class="mord cjk_fallback">赖</span><span class="mord cjk_fallback">于</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">当</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">较</span><span class="mord cjk_fallback">大</span><span class="mord cjk_fallback">时</span><span class="mord cjk_fallback">，</span></span></span></span>\sigma^{\prime}(z_j)$通常会很小。梯度消失很难避免。<br />
  <strong>总结：</strong> 深层神经网络的反向传播过程会发生梯度变化不稳定的状况。通过使用其他的激活函数，我们能否避免这种梯度下降不稳定的现象呢？<br />
  <strong>相关文献：</strong> 2010年Glorot和Bengio发文表明sigmoid函数在深层神经网络中，最后的隐含层在训练中会产生饱和，其值接近0，作者建议使用其他的激活函数。<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a></p>
<h5 id="缺点2sigmoid函数的输出不是0均值zero-centered"><a class="markdownIt-Anchor" href="#缺点2sigmoid函数的输出不是0均值zero-centered"></a> <mark>缺点2：sigmoid函数的输出不是0均值(zero-centered)</mark></h5>
<p>  这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">x&gt;0,f(x)=w^{T}x+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。</p>
<h5 id="22-tanh函数"><a class="markdownIt-Anchor" href="#22-tanh函数"></a> 2.2 tanh函数</h5>
<p>  tanh函数的形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tanh(x)=\frac&#123;e^&#123;x&#125;-e^&#123;-x&#125;&#125; &#123;e^&#123;x&#125;+e^&#123;-x&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>  tanh函数及其导数的图像如下所示：<br />
<img src="B5829748B50D4CD395A164702E26CA17" alt="2018041517590341" /><br />
  <strong>特点：</strong> 解决了sigmoid函数的非零均值问题，但梯度消失的问题仍然没有解决。</p>
<h5 id="23-relu函数"><a class="markdownIt-Anchor" href="#23-relu函数"></a> 2.3 ReLU函数</h5>
<p>  ReLU(Rectified Linear Unit)函数的基本形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Relu(x) = max(0,x)</span><br></pre></td></tr></table></figure>
<p>  ReLU函数及其导数的图像如下所示：<br />
<img src="D0CEA84C47E0407CAC66F19606F16C60" alt="relu" /><br />
  <strong>优点：</strong> ReLU函数解决了梯度消失的问题；计算速度快；收敛速度远快于sigmoid函数和tanh函数。<br />
  <strong>缺点：</strong> ReLU函数的输出不是零均值；Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 参数初始化不当，负的权值太多，这种情况比较少见； (2) 学习率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</p>
<h5 id="24-leaky-relu函数"><a class="markdownIt-Anchor" href="#24-leaky-relu函数"></a> 2.4 Leaky ReLU函数</h5>
<p>  Leaky ReLU函数的形式如下，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>是一个非常小的正数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Leaky ReLU(X) = max(\alpha x, x)</span><br></pre></td></tr></table></figure>
<p>  Leaky ReLU函数及其导数的图像如下所示：<br />
<img src="56488FB0850042D29A9D1E05ABD04407" alt="leakyrelu" /><br />
  <strong>特点：</strong> 为了解决Dead ReLU Problem，将ReLU的前半段设为αx 而非0，通常α=0.01。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h4 id="25-其他激活函数"><a class="markdownIt-Anchor" href="#25-其他激活函数"></a> 2.5 其他激活函数</h4>
<p>  如ELU(Exponential Linear Units)函数，Maxout函数</p>
<h4 id="3激活函数的选择"><a class="markdownIt-Anchor" href="#3激活函数的选择"></a> 3.激活函数的选择</h4>
<p>  (1)深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。<br />
  (2）如果使用 ReLU，那么一定要小心设置 learning rate， 而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.<br />
  (3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout.</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://holdfire.github.io/2019/10/31/machine-learning-activation-function/" title="机器学习——激活函数" target="_blank" rel="external">http://holdfire.github.io/2019/10/31/machine-learning-activation-function/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/holdfire" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/holdfire" target="_blank"><span class="text-dark">holdfire</span><small class="ml-1x">Computer Vision</small></a></h3>
        <div>No pains, no gains. Stay hungry, stay foolish. And be a lifelong learner.</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2019/10/31/machine-learning-optimization-algorithm-1/" title="机器学习——一阶优化算法"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2019/10/30/python-package-module-import/" title="Python——导入操作import"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/holdfire" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="http://weibo.com/holdfireLX" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script src="/js/plugin.min.js"></script>
<script src="/js/application.js"></script>

    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>





   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'y3U6QFDMxRlh8fDNs1qJl7su-gzGzoHsz',
    appKey: 'pssOSm45PFn6XJO82i2Nkb20',
    placeholder: '来踩我呀！',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

     







</body>
</html>