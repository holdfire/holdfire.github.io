<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="一只梦想遨游天际的菜鸟！">
<meta name="keywords" content="计算机，水利工程">
<meta property="og:type" content="website">
<meta property="og:title" content="学无止境">
<meta property="og:url" content="http:&#x2F;&#x2F;holdfire.github.io&#x2F;index.html">
<meta property="og:site_name" content="学无止境">
<meta property="og:description" content="一只梦想遨游天际的菜鸟！">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://holdfire.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>学无止境</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">学无止境</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">不忘初心</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/11/02/computer-vision-upsampling-downsampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/computer-vision-upsampling-downsampling/" class="post-title-link" itemprop="url">图像处理——上采样与下采样</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-11-02 06:14:41 / 修改时间：06:15:41" itemprop="dateCreated datePublished" datetime="2019-11-02T06:14:41+08:00">2019-11-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/11/02/deep-learning-parameter-initialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/deep-learning-parameter-initialization/" class="post-title-link" itemprop="url">神经网络——参数初始化</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-11-02 05:57:50 / 修改时间：06:37:25" itemprop="dateCreated datePublished" datetime="2019-11-02T05:57:50+08:00">2019-11-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1如何分析参数初始化结果的好坏"><a class="markdownIt-Anchor" href="#1如何分析参数初始化结果的好坏"></a> 1.如何分析参数初始化结果的好坏？</h4>
<p>  查看初始化后各层的激活值分布，如果在某个固定区间内分布则较好，如果集中在某个值上则初始化不好。</p>
<h4 id="2-把w初始化为0"><a class="markdownIt-Anchor" href="#2-把w初始化为0"></a> 2. 把w初始化为0</h4>
<p>  对于单层网络可行；对于多层网络，由于链式法则会导致梯度消失。</p>
<h4 id="3-随机初始化"><a class="markdownIt-Anchor" href="#3-随机初始化"></a> 3. 随机初始化</h4>
<p>  使用均值为0，方差为0.02的正态分布去初始化。w初始值较小是因为：如果x很大的话，w又相对较大，会导致Z非常大，这样如果激活函数是sigmoid，就会导致sigmoid的输出值1或者0。</p>
<h4 id="4-xavier-initialization"><a class="markdownIt-Anchor" href="#4-xavier-initialization"></a> 4. Xavier initialization</h4>
<p>  Xavier initialization方法是Glorot等人为了解决随机初始化的问题提出来的另一种初始化方法。他们的思想倒也简单，就是尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。Xavier initialization能够很好的tanh激活函数。</p>
<h4 id="5-he-initialization"><a class="markdownIt-Anchor" href="#5-he-initialization"></a> 5. He initialization</h4>
<p>  </p>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料：</h4>
<ol>
<li>斯坦福大学——李飞飞计算机视觉课程cs231n：</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/11/02/deep-learning-batch-normalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/deep-learning-batch-normalization/" class="post-title-link" itemprop="url">神经网络——Batch Normalization</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-11-02 05:13:56 / 修改时间：06:37:09" itemprop="dateCreated datePublished" datetime="2019-11-02T05:13:56+08:00">2019-11-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-简介"><a class="markdownIt-Anchor" href="#1-简介"></a> 1. 简介</h4>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/11/02/computer-vision-IOU-NMS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/02/computer-vision-IOU-NMS/" class="post-title-link" itemprop="url">目标检测——交并比IOU,非极大值抑制NMS</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-11-02 05:07:41 / 修改时间：06:36:51" itemprop="dateCreated datePublished" datetime="2019-11-02T05:07:41+08:00">2019-11-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-交并比iou"><a class="markdownIt-Anchor" href="#1-交并比iou"></a> 1. 交并比IoU</h4>
<p>  在目标检测任务中，通常使用矩形框来定位检测对象的边界。假设你的算法给出的边界框为A，实际的边界框为B，那么交并比(Intersection over Union)就可以衡量检测结果的好坏，其计算公式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mrow><mi>A</mi><mo>∩</mo><mi>B</mi></mrow><mrow><mi>A</mi><mo>∪</mo><mi>B</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">IoU = \frac {A \cap B} {A \cup B}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.04633em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>  通常约定IoU大于0.5，就认为目标检测正确</p>
<h4 id="2-非极大值抑制nms"><a class="markdownIt-Anchor" href="#2-非极大值抑制nms"></a> 2. 非极大值抑制NMS</h4>
<p>  在目标检测任务中，你的算法可能对同一个目标做了多次检测。非极大值抑制(Non-Maximum Suppression, NMS)就是为了确保你的算法对一个目标只检测一次。</p>
<h4 id="3-anchor-box"><a class="markdownIt-Anchor" href="#3-anchor-box"></a> 3. Anchor Box</h4>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料：</h4>
<ol>
<li>吴恩达深度学习课程：</li>
<li>斯坦福大学，李飞飞计算机视觉课程cs231n：</li>
<li>慕课网，深度学习之目标检测课程：</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/10/31/deep-learning-optimization-algorithm-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/31/deep-learning-optimization-algorithm-2/" class="post-title-link" itemprop="url">神经网络——二阶优化算法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-31 02:54:23" itemprop="dateCreated datePublished" datetime="2019-10-31T02:54:23+08:00">2019-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-02 06:37:21" itemprop="dateModified" datetime="2019-11-02T06:37:21+08:00">2019-11-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-简介"><a class="markdownIt-Anchor" href="#1-简介"></a> 1. 简介</h4>
<h5 id="11-牛顿法"><a class="markdownIt-Anchor" href="#11-牛顿法"></a> 1.1 牛顿法</h5>
<h5 id="12-高斯-牛顿法"><a class="markdownIt-Anchor" href="#12-高斯-牛顿法"></a> 1.2 高斯-牛顿法</h5>
<h5 id="13-拟牛顿法"><a class="markdownIt-Anchor" href="#13-拟牛顿法"></a> 1.3 拟牛顿法</h5>
<h5 id="14-发展历程"><a class="markdownIt-Anchor" href="#14-发展历程"></a> 1.4 发展历程</h5>
<h4 id="2-不常用的二阶优化算法"><a class="markdownIt-Anchor" href="#2-不常用的二阶优化算法"></a> 2. 不常用的二阶优化算法</h4>
<h5 id="21-dfp算法"><a class="markdownIt-Anchor" href="#21-dfp算法"></a> 2.1 DFP算法</h5>
<p>  第一个拟牛顿算法是由Argonne国家实验室的物理学家William C.Davidon提出的。 他在1959年开发了第一个拟牛顿算法:DFP（Davidon–Fletcher–Powell formula）更新公式，后来由Fletcher和Powell在1963年推广，但现在很少使用</p>
<h4 id="3-常用的二阶优化算法"><a class="markdownIt-Anchor" href="#3-常用的二阶优化算法"></a> 3. 常用的二阶优化算法</h4>
<h5 id="31-sr1公式用于对称秩一"><a class="markdownIt-Anchor" href="#31-sr1公式用于对称秩一"></a> 3.1 SR1公式（用于“对称秩一”）</h5>
<p>  </p>
<h5 id="32-bhhh方法"><a class="markdownIt-Anchor" href="#32-bhhh方法"></a> 3.2 BHHH方法</h5>
<p>  </p>
<h5 id="33-bfgs方法"><a class="markdownIt-Anchor" href="#33-bfgs方法"></a> 3.3 BFGS方法</h5>
<p>  BFGS算法的全称为Broyden–Fletcher–Goldfarb–Shanno algorithm</p>
<h5 id="34-limited-memory-bfgs方法"><a class="markdownIt-Anchor" href="#34-limited-memory-bfgs方法"></a> 3.4 Limited-memory BFGS方法</h5>
<p>  低内存拓展算法</p>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料：</h4>
<ol>
<li>机器之心——二阶优化算法介绍：<a href="https://www.jiqizhixin.com/graph/technologies/75950ad0-edbd-4208-9347-b8c17b8e058c" target="_blank" rel="noopener">https://www.jiqizhixin.com/graph/technologies/75950ad0-edbd-4208-9347-b8c17b8e058c</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/10/31/deep-learning-loss-function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/31/deep-learning-loss-function/" class="post-title-link" itemprop="url">机器学习——损失函数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-31 02:29:07" itemprop="dateCreated datePublished" datetime="2019-10-31T02:29:07+08:00">2019-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-02 06:37:13" itemprop="dateModified" datetime="2019-11-02T06:37:13+08:00">2019-11-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-简介"><a class="markdownIt-Anchor" href="#1-简介"></a> 1. 简介</h4>
<h5 id="11-什么是损失函数"><a class="markdownIt-Anchor" href="#11-什么是损失函数"></a> 1.1 什么是损失函数？</h5>
<h4 id="2回归问题中的损失函数"><a class="markdownIt-Anchor" href="#2回归问题中的损失函数"></a> 2.回归问题中的损失函数</h4>
<h4 id="3-分类问题中的损失函数"><a class="markdownIt-Anchor" href="#3-分类问题中的损失函数"></a> 3. 分类问题中的损失函数</h4>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/10/31/deep-learning-optimization-algorithm-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/31/deep-learning-optimization-algorithm-1/" class="post-title-link" itemprop="url">神经网络——一阶优化算法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-31 02:28:46" itemprop="dateCreated datePublished" datetime="2019-10-31T02:28:46+08:00">2019-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-02 06:37:17" itemprop="dateModified" datetime="2019-11-02T06:37:17+08:00">2019-11-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-简介"><a class="markdownIt-Anchor" href="#1-简介"></a> 1. 简介</h4>
<h5 id="11-什么是优化算法"><a class="markdownIt-Anchor" href="#11-什么是优化算法"></a> 1.1 什么是优化算法？</h5>
<h5 id="12-为什么需要优化算法"><a class="markdownIt-Anchor" href="#12-为什么需要优化算法"></a> 1.2 为什么需要优化算法？</h5>
<h5 id="13-优化算法分类"><a class="markdownIt-Anchor" href="#13-优化算法分类"></a> 1.3 优化算法分类</h5>
<h4 id="2-一阶优化算法"><a class="markdownIt-Anchor" href="#2-一阶优化算法"></a> 2. 一阶优化算法</h4>
<h5 id="21-随机梯度下降法"><a class="markdownIt-Anchor" href="#21-随机梯度下降法"></a> 2.1 随机梯度下降法</h5>
<p>  随机梯度下降法(Stotastic Gradient Descend method, SGD)是</p>
<h5 id="22-批量梯度下降法"><a class="markdownIt-Anchor" href="#22-批量梯度下降法"></a> 2.2 批量梯度下降法</h5>
<p>  批量梯度下降法(Batch Gradient Descend)</p>
<h5 id="23-小批量随机梯度下降法"><a class="markdownIt-Anchor" href="#23-小批量随机梯度下降法"></a> 2.3 小批量随机梯度下降法</h5>
<p>  小批量梯度下降法(mini-Batch Gradient Descend)</p>
<h5 id="24-动量梯度下降法"><a class="markdownIt-Anchor" href="#24-动量梯度下降法"></a> 2.4 动量梯度下降法</h5>
<p>  动量梯度下降法(Momentum Gradient Descend)</p>
<h5 id="25-nesterov梯度下降方法"><a class="markdownIt-Anchor" href="#25-nesterov梯度下降方法"></a> 2.5 Nesterov梯度下降方法</h5>
<p>  Nesterov梯度下降方法(Nesterov Accelerated Gradient)是对传统momentum方法的一项改进，由Ilya Sutskever(2012 unpublished)在Nesterov工作的启发下提出的。<br />
其基本思路如下图：</p>
<h5 id="26-adagrade"><a class="markdownIt-Anchor" href="#26-adagrade"></a> 2.6 Adagrade</h5>
<p>  </p>
<h5 id="27-adadelta"><a class="markdownIt-Anchor" href="#27-adadelta"></a> 2.7 Adadelta</h5>
<p>  </p>
<h5 id="28-rmsprop"><a class="markdownIt-Anchor" href="#28-rmsprop"></a> 2.8 RMSprop</h5>
<p>  </p>
<h5 id="29-adam"><a class="markdownIt-Anchor" href="#29-adam"></a> 2.9 Adam</h5>
<p>  </p>
<h5 id="3-选择技巧"><a class="markdownIt-Anchor" href="#3-选择技巧"></a> 3 选择技巧</h5>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料：</h4>
<ol>
<li>各种优化方法总结比较：<a href="https://www.cnblogs.com/qniguoym/p/8058186.html" target="_blank" rel="noopener">https://www.cnblogs.com/qniguoym/p/8058186.html</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/10/31/deep-learning-activation-function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/31/deep-learning-activation-function/" class="post-title-link" itemprop="url">神经网络——激活函数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-31 02:24:41" itemprop="dateCreated datePublished" datetime="2019-10-31T02:24:41+08:00">2019-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-02 06:37:04" itemprop="dateModified" datetime="2019-11-02T06:37:04+08:00">2019-11-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index">
                    <span itemprop="name">人工智能</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1简介"><a class="markdownIt-Anchor" href="#1简介"></a> 1.简介</h4>
<p>  在多层神经网络中，上层神经元输出的线性组合和下层神经元的输出之间具有一个函数关系，这个函数关系称为激活函数。激活函数有什么作用呢？<br />
  神经网络中如果不使用激活函数（相当于激活函数是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x)=x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>)，那么每一层节点的输出都会是输入的线性函数。其结果是：无论神经网络有多少层，网络的输出层都是输入层的线性组合，神经网络仅相当于一个感知机，网络的拟合能力变得非常有限。<br />
  神经网络中加入激活函数后，网络对现实模型的表达能力得到增强，网络几乎能够逼近任意的函数模型。常用的的激活函数主要有：sigmoid函数，relu函数，tanh函数，leaky_relu函数，maxout函数等。</p>
<h4 id="2-常用的激活函数"><a class="markdownIt-Anchor" href="#2-常用的激活函数"></a> 2. 常用的激活函数</h4>
<h5 id="21-sigmoid函数"><a class="markdownIt-Anchor" href="#21-sigmoid函数"></a> 2.1 sigmoid函数</h5>
<p>  sigmoid函数是应用最为广泛的激活函数之一,函数形式如下所示。能够把任意实数映射到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>区间上的实数。当自变量值小于-5时，函数值接近于0；当自变量大于5时，函数值非常接近于1。sigmoid函数主要的优点是求导方便，其导数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>f</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">[</mo><mn>1</mn><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">f^{&#x27;}(x)=f(x)\cdot[1-f(x)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.19248em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.94248em;"><span style="top:-2.94248em;margin-right:0.05em;"><span class="pstrut" style="height:2.57948em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>。当x=0时函数的导数最大，为0.25。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\tag{1} \sigma(x) = \frac {1}{1+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="tag"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p><strong>缺点1：梯度消失和梯度爆炸</strong><br />
  如果将神经网络的权值初始化为均值为0，标准差为1，由于sigmoid函数的导数在两端非常接近于0，靠近输入层的权重的更新值，进过一系列累乘后会变得特别小，权重参数得不到更新，这种现象叫做梯度消失。如果将初始权重设得特别大，如1000，同理可能会造成梯度爆炸，即权重参数每次的更新值太大。所以，为了同时解决梯度消失和梯度爆炸两个问题，我们需要设置合理的权重初始值，但这个很难做到，详细解释见附录。</p>
<p><strong>缺点2：输出不是0均值</strong><br />
  当上一层的输出结果不是0均值(zero-centered)时，即下一层的输入信号不是0均值，会产生这样的一个结果：对于函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">f(x)=w^{T}x+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>，如果输入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x&gt;0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，权重参数的收敛变得缓慢，称为zig-zag现象。<br />
  如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。<br />
<strong>缺点3：求幂运算消耗时间较长</strong><br />
  计算机求<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow><annotation encoding="application/x-tex">e^{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.664392em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span>会消耗较长的时间。</p>
<h5 id="22-tanh函数"><a class="markdownIt-Anchor" href="#22-tanh函数"></a> 2.2 tanh函数</h5>
<p>  tanh函数的主要优点是:解决了sigmoid函数的非零均值问题，但梯度消失的问题仍然没有解决。其函数形式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\tag{2} tanh(x)=\frac{e^{x}-e^{-x}} {e^{x}+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.217661em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="tag"><span class="strut" style="height:2.217661em;vertical-align:-0.7693300000000001em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>  我们知道Relu函数在卷积神经网络中取得了很好的结果，那为什么在RNN中还是使用tanh作为激活函数呢？</p>
<h5 id="23-relu函数"><a class="markdownIt-Anchor" href="#23-relu函数"></a> 2.3 ReLU函数</h5>
<p>  ReLU函数(Rectified Linear Unit)的主要优点是：一定程度解决了梯度消失的问题，计算速度快，收敛速度远快于sigmoid函数和tanh函数。函数形式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\tag{3} relu(x) = max(0,x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">u</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p><strong>ReLU函数的主要缺点是：</strong><br />
  输出非零均值；可能会造成Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生:<br />
   一是参数初始化不当，负的权值太多，这种情况比较少见。二是学习率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用<strong>Xavier初始化</strong>方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</p>
<h5 id="24-leaky-relu函数"><a class="markdownIt-Anchor" href="#24-leaky-relu函数"></a> 2.4 Leaky ReLU函数</h5>
<p>  为了解决Dead ReLU Problem，Leaky ReLU函数将ReLU的前半段设为$ \alpha x<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">而</mi><mi mathvariant="normal">非</mi><mn>0</mn><mi mathvariant="normal">，</mi><mi mathvariant="normal">通</mi><mi mathvariant="normal">常</mi></mrow><annotation encoding="application/x-tex">而非0，通常</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord cjk_fallback">而</span><span class="mord cjk_fallback">非</span><span class="mord">0</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">通</span><span class="mord cjk_fallback">常</span></span></span></span> \alpha = 0.01$。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。其函数形式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>l</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>y</mi><mtext> </mtext><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(4)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\tag{4} leaky\ relu(x) = max(\alpha x, x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace"> </span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">u</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">4</span></span><span class="mord">)</span></span></span></span></span></span></p>
<h5 id="25-maxout函数"><a class="markdownIt-Anchor" href="#25-maxout函数"></a> 2.5 Maxout函数</h5>
<p>  Maxout函数可以看做是relu函数和leaky relu函数的一般化归纳。通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的界，然后每一段取最大。maxout可以看成是ReLU家族的一个推广,不会产生饱和，也不会产生dead ReLU。缺点在于参数量翻倍了。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mi>T</mi></msubsup><mi>x</mi><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo separator="true">,</mo><msubsup><mi>w</mi><mn>2</mn><mi>T</mi></msubsup><mi>x</mi><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(5)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\tag{5} f(x) = max(w_1^Tx+b_1,w_2^Tx+b_2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.138331em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1.1413309999999999em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">5</span></span><span class="mord">)</span></span></span></span></span></span></p>
<h5 id="26-softmax函数"><a class="markdownIt-Anchor" href="#26-softmax函数"></a> 2.6 Softmax函数</h5>
<p>  Softmax函数作为激活函数，通常用在多分类神经网络的输出层上，目的是让大的值更大。其函数形式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>δ</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(6)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\tag{6} \delta (z_j) = \frac {e^{z_j}} {\sum_{i=1}^{k}e^{z_i}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.03785em;">δ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.52011em;vertical-align:-1.178718em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.120992em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9890079999999999em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.178718em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="tag"><span class="strut" style="height:2.52011em;vertical-align:-1.178718em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">6</span></span><span class="mord">)</span></span></span></span></span></span></p>
<h4 id="3激活函数的选择"><a class="markdownIt-Anchor" href="#3激活函数的选择"></a> 3.激活函数的选择</h4>
<p>  (1)深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。<br />
  (2）如果使用 ReLU，那么一定要小心设置 learning rate， 而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.<br />
  (3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout，其计算量太大导致收敛很慢。<br />
  (4)为了防止饱和，现在主流的做法会在激活函数前进行<strong>batch normalization</strong>操作，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。</p>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料：</h4>
<ol>
<li>书籍<a href="http://neuralnetworksanddeeplearning.com/chap5.html" target="_blank" rel="noopener">Neural Networks and Deep Learning</a>第5章</li>
<li>Hinton关于Relu函数在RNN中不奏效的论文：<a href="https://arxiv.org/abs/1504.00941" target="_blank" rel="noopener">A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</a></li>
<li>Glorot和Bengio关于sigmoid函数在深层网络中产生饱和的论文：<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a></li>
<li>知乎专栏——RNN 中为什么要采用 tanh，而不是 ReLU 作为激活函数 ：<a href="https://www.zhihu.com/question/61265076/answer/186347780" target="_blank" rel="noopener">https://www.zhihu.com/question/61265076/answer/186347780</a></li>
</ol>
<h2 id="center附录center"><a class="markdownIt-Anchor" href="#center附录center"></a> <center>附录</center></h2>
<h4 id="sigmoid函数梯度消失和梯度爆炸试验"><a class="markdownIt-Anchor" href="#sigmoid函数梯度消失和梯度爆炸试验"></a> sigmoid函数梯度消失和梯度爆炸试验：</h4>
<p>下文来自参考资料第1条：<br />
  <strong>现象：</strong> 在深层网络中，不同隐含层的学习速度(各隐含层权值和偏差偏导组成的向量的范数)相差很大。靠后的隐含层学习速度较大，而靠前的隐含层经常在训练期间卡住，几乎什么都学习不到；也有时候早期的层可能学习很好，但后来的层卡住。<br />
  <strong>试验：</strong> 使用MNIST数据集进行图像分类任务，输入层神经元个数为784，隐含层神经元个数均为30，输出层神经元个数为10；使用sigmoid函数作为激活函数。训练图片为1000张，使用Batch Stostic Gradient Descend算法，500个epoch。使用不同数目的隐含层的分类结果如下：</p>
<table>
<thead>
<tr>
<th>隐含层数目</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>分类准确率</td>
<td>96.48%</td>
<td>96.90%</td>
<td>96.57%</td>
<td>96.53%</td>
</tr>
</tbody>
</table>
<p>  带有4个隐含层的网络，训练过程中各隐含层权重的学习速度变化如下图所示。从图中可以看出：在训练后期，第一个隐含层的学习速率比第四个隐含层慢了约100倍。</p>
<p>  <strong>分析：</strong> 直观上来说，额外的隐含层应该使网络能够学习更为复杂的分类功能，从而进行更好的分类。即使额外的隐含层什么都不做，模型的准确率也不会变得更差。由于权重是随机初始化的，因而第一个隐含层会丢失大量的图片有用信息，所以此时第一层几乎不可能不需要再学习，即还没有收敛到最优值。若假设额外的隐含层确实有用，那么问题应该是我们的学习算法没有找到合适的权重和偏差。<br />
  <strong>梯度消失的数学推导：</strong> 考虑一个每层只有1个神经元的深层神经网络，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">w_1,w_2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>为权重，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>b</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">b_1,b_2,...</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span></span></span></span>为偏置。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">a_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>为第j层的输出值，代价函数C对第一个隐含层的偏置<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的偏导为：<br />
  将神经网络的权值初始化为均值为0，标准差为1。则大部分权值满足<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo fence="true">∣</mo><msub><mi>w</mi><mi>j</mi></msub><mo fence="true">∣</mo></mrow><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left|w_j\right|&lt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，且有$ \left|w_j \sigma^{\prime}(z_j)&lt;\frac {1} {4}\right| <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">。</mi><mi mathvariant="normal">由</mi><mi mathvariant="normal">反</mi><mi mathvariant="normal">向</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">播</mi><mi mathvariant="normal">算</mi><mi mathvariant="normal">法</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">数</mi><mi mathvariant="normal">学</mi><mi mathvariant="normal">推</mi><mi mathvariant="normal">导</mi><mi mathvariant="normal">可</mi><mi mathvariant="normal">知</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">从</mi><mi mathvariant="normal">后</mi><mi mathvariant="normal">向</mi><mi mathvariant="normal">前</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">播</mi><mi mathvariant="normal">时</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">每</mi><mi mathvariant="normal">传</mi><mi mathvariant="normal">递</mi><mi mathvariant="normal">一</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">都</mi><mi mathvariant="normal">会</mi><mi mathvariant="normal">减</mi><mi mathvariant="normal">小</mi><mi mathvariant="normal">为</mi><mi mathvariant="normal">原</mi><mi mathvariant="normal">来</mi><mi mathvariant="normal">的</mi><mn>0.25</mn><mi mathvariant="normal">倍</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">如</mi><mi mathvariant="normal">果</mi><mi mathvariant="normal">神</mi><mi mathvariant="normal">经</mi><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">隐</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">特</mi><mi mathvariant="normal">别</mi><mi mathvariant="normal">多</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">那</mi><mi mathvariant="normal">么</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">在</mi><mi mathvariant="normal">穿</mi><mi mathvariant="normal">过</mi><mi mathvariant="normal">多</mi><mi mathvariant="normal">层</mi><mi mathvariant="normal">后</mi><mi mathvariant="normal">将</mi><mi mathvariant="normal">变</mi><mi mathvariant="normal">得</mi><mi mathvariant="normal">非</mi><mi mathvariant="normal">常</mi><mi mathvariant="normal">小</mi><mi mathvariant="normal">接</mi><mi mathvariant="normal">近</mi><mi mathvariant="normal">于</mi><mn>0</mn><mi mathvariant="normal">，</mi><mi mathvariant="normal">即</mi><mi mathvariant="normal">出</mi><mi mathvariant="normal">现</mi><mi mathvariant="normal">梯</mi><mi mathvariant="normal">度</mi><mi mathvariant="normal">消</mi><mi mathvariant="normal">失</mi><mi mathvariant="normal">现</mi><mi mathvariant="normal">象</mi><mi mathvariant="normal">；</mi><mi mathvariant="normal">当</mi><mi mathvariant="normal">网</mi><mi mathvariant="normal">络</mi><mi mathvariant="normal">权</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">初</mi><mi mathvariant="normal">始</mi><mi mathvariant="normal">化</mi><mi mathvariant="normal">为</mi></mrow><annotation encoding="application/x-tex">。由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">由</span><span class="mord cjk_fallback">反</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">播</span><span class="mord cjk_fallback">算</span><span class="mord cjk_fallback">法</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">学</span><span class="mord cjk_fallback">推</span><span class="mord cjk_fallback">导</span><span class="mord cjk_fallback">可</span><span class="mord cjk_fallback">知</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">从</span><span class="mord cjk_fallback">后</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">前</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">播</span><span class="mord cjk_fallback">时</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">每</span><span class="mord cjk_fallback">传</span><span class="mord cjk_fallback">递</span><span class="mord cjk_fallback">一</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">都</span><span class="mord cjk_fallback">会</span><span class="mord cjk_fallback">减</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">为</span><span class="mord cjk_fallback">原</span><span class="mord cjk_fallback">来</span><span class="mord cjk_fallback">的</span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span><span class="mord cjk_fallback">倍</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">如</span><span class="mord cjk_fallback">果</span><span class="mord cjk_fallback">神</span><span class="mord cjk_fallback">经</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">隐</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">别</span><span class="mord cjk_fallback">多</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">那</span><span class="mord cjk_fallback">么</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">在</span><span class="mord cjk_fallback">穿</span><span class="mord cjk_fallback">过</span><span class="mord cjk_fallback">多</span><span class="mord cjk_fallback">层</span><span class="mord cjk_fallback">后</span><span class="mord cjk_fallback">将</span><span class="mord cjk_fallback">变</span><span class="mord cjk_fallback">得</span><span class="mord cjk_fallback">非</span><span class="mord cjk_fallback">常</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">接</span><span class="mord cjk_fallback">近</span><span class="mord cjk_fallback">于</span><span class="mord">0</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">即</span><span class="mord cjk_fallback">出</span><span class="mord cjk_fallback">现</span><span class="mord cjk_fallback">梯</span><span class="mord cjk_fallback">度</span><span class="mord cjk_fallback">消</span><span class="mord cjk_fallback">失</span><span class="mord cjk_fallback">现</span><span class="mord cjk_fallback">象</span><span class="mord cjk_fallback">；</span><span class="mord cjk_fallback">当</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">权</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">初</span><span class="mord cjk_fallback">始</span><span class="mord cjk_fallback">化</span><span class="mord cjk_fallback">为</span></span></span></span>(1,+\infty)<span class='katex-error' title='ParseError: KaTeX parse error: Expected &#039;EOF&#039;, got &#039;&amp;&#039; at position 21: …，则会出现梯度爆炸情况。  
&amp;̲emsp;&amp;emsp;**梯度…'>区间内的值，则会出现梯度爆炸情况。  
&amp;emsp;&amp;emsp;**梯度爆炸的产生：**   当权重设置很大时，如</span>w_1=w_2=w_3=w_4=1000<span class='katex-error' title='ParseError: KaTeX parse error: Expected &#039;EOF&#039;, got &#039;&amp;&#039; at position 23: …0时，会产生梯度爆炸。   
&amp;̲emsp;&amp;emsp;**思考…'>,而z的值接近0时，会产生梯度爆炸。   
&amp;emsp;&amp;emsp;**思考：** 为了避免梯度消失，我们可以通过设置合适</span>w_j<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">的</mi><mi mathvariant="normal">初</mi><mi mathvariant="normal">始</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">使</mi><mi mathvariant="normal">得</mi></mrow><annotation encoding="application/x-tex">的初始值，使得</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">初</span><span class="mord cjk_fallback">始</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">使</span><span class="mord cjk_fallback">得</span></span></span></span>\left|w_j \sigma^{\prime}(z_j) \right|&gt;1<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">。</mi><mi mathvariant="normal">事</mi><mi mathvariant="normal">实</mi><mi mathvariant="normal">上</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">这</mi><mi mathvariant="normal">非</mi><mi mathvariant="normal">常</mi><mi mathvariant="normal">困</mi><mi mathvariant="normal">难</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">因</mi><mi mathvariant="normal">为</mi></mrow><annotation encoding="application/x-tex">。事实上，这非常困难，因为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord cjk_fallback">。</span><span class="mord cjk_fallback">事</span><span class="mord cjk_fallback">实</span><span class="mord cjk_fallback">上</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">这</span><span class="mord cjk_fallback">非</span><span class="mord cjk_fallback">常</span><span class="mord cjk_fallback">困</span><span class="mord cjk_fallback">难</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">因</span><span class="mord cjk_fallback">为</span></span></span></span>\sigma<sup>{\prime}(z_j)=\sigma</sup>{\prime}(wa+b) <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">的</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">是</mi><mi mathvariant="normal">依</mi><mi mathvariant="normal">赖</mi><mi mathvariant="normal">于</mi><mi>w</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">，</mi><mi mathvariant="normal">当</mi><mi>w</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">值</mi><mi mathvariant="normal">较</mi><mi mathvariant="normal">大</mi><mi mathvariant="normal">时</mi><mi mathvariant="normal">，</mi></mrow><annotation encoding="application/x-tex">的值是依赖于w的，当w的值较大时，</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">是</span><span class="mord cjk_fallback">依</span><span class="mord cjk_fallback">赖</span><span class="mord cjk_fallback">于</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">当</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">值</span><span class="mord cjk_fallback">较</span><span class="mord cjk_fallback">大</span><span class="mord cjk_fallback">时</span><span class="mord cjk_fallback">，</span></span></span></span>\sigma^{\prime}(z_j)$通常会很小。梯度消失很难避免。<br />
  <strong>总结：</strong> 深层神经网络的反向传播过程会发生梯度变化不稳定的状况。通过使用其他的激活函数，我们能否避免这种梯度下降不稳定的现象呢？</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/10/30/python-package-module-import/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/30/python-package-module-import/" class="post-title-link" itemprop="url">Python——导入操作import</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-30 19:19:53" itemprop="dateCreated datePublished" datetime="2019-10-30T19:19:53+08:00">2019-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-31 01:34:35" itemprop="dateModified" datetime="2019-10-31T01:34:35+08:00">2019-10-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" itemprop="url" rel="index">
                    <span itemprop="name">编程语言</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-简介"><a class="markdownIt-Anchor" href="#1-简介"></a> 1. 简介</h4>
<h5 id="11-导入的内容"><a class="markdownIt-Anchor" href="#11-导入的内容"></a> 1.1 导入的内容</h5>
<p>  Python中导入操作的关键字为import，导入的内容主要是：包(package)、模块(module)、类(class)、函数(类中称作方法, method)、变量(类中称作属性, attributes)等。<br />
  在文件夹下面加入<code>__init__.py</code>文件(以下简称init文件)，这个文件夹就成为了一个包。包分为常规包和命名空间包。<br />
  模块是包下面的python文件。所有的包都可以被看做模块，但不是所有的模块都是包。<br />
  类是文件的基本单元（类中包括变量和函数）。<br />
  导入操作中，导入的具体内容主要由init文件、<code>__all__</code>列表(以下简称all列表)控制。</p>
<h5 id="12-init文件的作用"><a class="markdownIt-Anchor" href="#12-init文件的作用"></a> 1.2 init文件的作用</h5>
<ul>
<li>如果包中含有init文件，<code>import 包名</code>就会执行init文件，能够批量导入init文件中导入的模块；</li>
<li>当这个包或者该包下的模块被导入时，init文件都会被执行；</li>
</ul>
<h5 id="13-all列表的作用"><a class="markdownIt-Anchor" href="#13-all列表的作用"></a> 1.3 all列表的作用</h5>
<ul>
<li>在init文件或模块中设置all列表时，<code>from 包名/模块名 import *</code>只会导入all列表中的内容；</li>
<li>all列表的内容可以是本文件自定义的变量或函数，或者是导入进来的模块；</li>
<li>从模块中all列表导入的内容称为内置变量，内置变量易覆盖本文件中的变量名，因而不建议使用<code>from 模块名 import *</code></li>
</ul>
<h4 id="2-import的使用方法"><a class="markdownIt-Anchor" href="#2-import的使用方法"></a> 2. import的使用方法</h4>
<p>  假设你正在一个名为current.py的文件（下面称为当前文件）中写代码，你想导入一些包或模块，那么不同的导入操作会有什么样的结果呢？<br />
  <font color = 'aaaaaa'>注意：下面说的执行init文件 == init文件中所有的导入操作都会被执行，但这并不意味导入的内容能被当前文件所调用。（这句话主要是针对下面代码块中从包中导入模块这一情况）</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//(<span class="number">1</span>) 从包名导入：包里必须有init文件，且init文件会被执行</span><br><span class="line"><span class="keyword">import</span> 包名                 // 当前文件能调用init文件所导入的全部内容。调用方式为包名.模块名</span><br><span class="line"><span class="keyword">from</span> 包名 <span class="keyword">import</span> *          // 若init文件有all列表，则当前文件只能调用all列表的内容；若无则能调用init文件所导入的全部内容。调用方式为模块名</span><br><span class="line"></span><br><span class="line">//(<span class="number">2</span>) 从包中导入模块：包里如果有init文件则先执行该文件，没有也没关系；然后导入该模块</span><br><span class="line"><span class="keyword">import</span> 包名.模块名           // 当前文件能调用init文件所导入的全部内容和该模块。调用方式为包名.模块名</span><br><span class="line"><span class="keyword">from</span> 包名 <span class="keyword">import</span> 模块名      // 当前文件只能调用该模块，无法调用init文件所导入的内容。调用方式为模块名</span><br><span class="line"></span><br><span class="line">//(<span class="number">3</span>) 单独导入模块：当前文件和该模块需在同一目录下</span><br><span class="line"><span class="keyword">import</span> 模块名                // 当前文件能调用该模块的所有对象(类、函数、变量等)，调用方式为模块名.对象名</span><br><span class="line"></span><br><span class="line">//(<span class="number">4</span>) 从模块中导入对象(类、函数、变量等)</span><br><span class="line"><span class="keyword">from</span> 模块名 <span class="keyword">import</span> 对象名     // 当前文件只能调用该模块下的指定对象，调用方式为对象名</span><br><span class="line"><span class="keyword">from</span> 模块名 <span class="keyword">import</span>  *        // 当前文件只能调用该模块下all列表中的对象，若无all列表则能调用所有的对象。调用方式为对象名</span><br><span class="line"></span><br><span class="line">//(<span class="number">5</span>) 包的相对导入：相对导入只能用<span class="keyword">from</span> <span class="keyword">import</span> 的方式</span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> 包名           // .表示当前文件所在目录的路径</span><br><span class="line"><span class="keyword">from</span> .. <span class="keyword">import</span> 包名          // ..表示当前文件所在目录的上级目录的路径</span><br><span class="line"></span><br><span class="line">//(<span class="number">6</span>) 相对导入应用：init文件中的导入</span><br><span class="line"><span class="keyword">from</span> 包名 <span class="keyword">import</span> 模块名       // 正确</span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> 模块名         // 正确，使用相对导入，其中.表示当前文件所在的目录</span><br><span class="line"><span class="keyword">import</span> 模块名                // 错误</span><br></pre></td></tr></table></figure>
<h4 id="3-import被解释执行"><a class="markdownIt-Anchor" href="#3-import被解释执行"></a> 3. import被解释执行</h4>
<h5 id="31-importlib包介绍"><a class="markdownIt-Anchor" href="#31-importlib包介绍"></a> 3.1 importlib包介绍</h5>
<p>  </p>
<h5 id="32-import的搜索路径"><a class="markdownIt-Anchor" href="#32-import的搜索路径"></a> 3.2 import的搜索路径</h5>
<p>  搜索路径被存储在sys模块中的path变量<br />
  在导入搜索期间首先会被检查的地方是 sys.modules。 这个映射起到缓存之前导入的所有模块的作用（包括其中间路径）。<br />
  当指定名称的模块在 sys.modules 中找不到时，Python 会接着搜索 sys.meta_path，其中包含元路径查找器对象列表。</p>
<h5 id="33-import的注意事项"><a class="markdownIt-Anchor" href="#33-import的注意事项"></a> 3.3 import的注意事项</h5>
<ul>
<li>每个包或模块只会被导入一次。</li>
<li>要避免循环导入。</li>
</ul>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料：</h4>
<ol>
<li>Python语言参考——导入系统：<a href="https://docs.python.org/zh-cn/3/reference/import.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/reference/import.html</a></li>
<li>菜鸟教程——Python3模块：<a href="https://www.runoob.com/python3/python3-module.html" target="_blank" rel="noopener">https://www.runoob.com/python3/python3-module.html</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://holdfire.github.io/2019/10/30/cpp-memory-allocation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="holdfire">
      <meta itemprop="description" content="一只梦想遨游天际的菜鸟！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学无止境">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/30/cpp-memory-allocation/" class="post-title-link" itemprop="url">C++学习——内存分配</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-30 03:55:20" itemprop="dateCreated datePublished" datetime="2019-10-30T03:55:20+08:00">2019-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-02 03:27:24" itemprop="dateModified" datetime="2019-11-02T03:27:24+08:00">2019-11-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" itemprop="url" rel="index">
                    <span itemprop="name">编程语言</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-栈区"><a class="markdownIt-Anchor" href="#1-栈区"></a> 1. 栈区</h4>
<p>由编译器自动分配和释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p>
<h4 id="2-堆区"><a class="markdownIt-Anchor" href="#2-堆区"></a> 2. 堆区</h4>
<p>一般有程序员分配和释放，用new运算符申请；若程序员不释放，程序结束时可由OS回收。</p>
<h4 id="3-全局区静态区"><a class="markdownIt-Anchor" href="#3-全局区静态区"></a> 3. 全局区（静态区）</h4>
<p>全局变量（main函数前声明的变量）和静态变量的存储放在一起。其中，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在一块区域。</p>
<h4 id="4-文字常量区"><a class="markdownIt-Anchor" href="#4-文字常量区"></a> 4. 文字常量区</h4>
<p>常量字符串存储的地方。<br />
为了节省内存，C/C++把常量字符串放到一个单独的内存区域。当几个指针赋值给相同的常量字符串时，它们实际上会指向相同的内存地址。但用常量内存初始化数组时，情况却有所不同。如下所示：str1和str2的地址不同，str3和str4的地址相同。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> str1[] = <span class="string">"Hello world"</span></span><br><span class="line"><span class="keyword">char</span> str2[] = <span class="string">"Hello world"</span></span><br><span class="line"><span class="keyword">char</span>* str3 = <span class="string">"Hello world"</span></span><br><span class="line"><span class="keyword">char</span>* str3 = <span class="string">"Hello world"</span></span><br></pre></td></tr></table></figure>
<h4 id="5-程序代码区"><a class="markdownIt-Anchor" href="#5-程序代码区"></a> 5. 程序代码区</h4>
<p>存放函数体的二进制码。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="holdfire"
    src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">holdfire</p>
  <div class="site-description" itemprop="description">一只梦想遨游天际的菜鸟！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">holdfire</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  
















  

  

</body>
</html>
